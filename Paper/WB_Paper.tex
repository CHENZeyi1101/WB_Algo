\documentclass[11pt,a4paper,reqno]{amsart}

\pdfoutput=1

\usepackage[utf8]{inputenc}
\usepackage{epsfig,amsmath,latexsym,amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage[numbers,sort&compress,longnamesfirst]{natbib}
\usepackage{float}
\usepackage{graphicx}
\usepackage{url}
\usepackage[lmargin=2cm, rmargin=2cm, tmargin=2.5cm, bmargin=2.5cm]{geometry}
%\usepackage[margin=1.1in]{geometry}
\usepackage{color}
\usepackage[citecolor=blue,urlcolor=blue,filecolor=blue]{hyperref}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{lscape}
\usepackage{dsfont}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algorithmic}
\usepackage{placeins}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{times}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{listings}
% \usepackage{setspace} % For line spacing
\usepackage{tikz}
\usetikzlibrary{calc}

\newcommand{\overbow}[1]{
   \tikz [baseline = (N.base), every node/.style={}] {
      \node [inner sep = 0pt] (N) {$#1$};
      \draw [line width = 0.4pt] plot [smooth, tension=1.3] coordinates {
         ($(N.north west) + (0.1ex,0)$)
         ($(N.north)      + (0,0.5ex)$)
         ($(N.north east) + (0,0)$)
      };
   }
}

% Load only the widecheck symbol from mathabx
\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{
  <-> mathx10
}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\widecheck}{0}{mathx}{"71}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{
      <5> <6> <7> <8> <9> <10>
      <10.95> <12> <14.4> <17.28> <20.74> <24.88>
      mathx10
      }{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathSymbol{\bigtimes}{1}{mathx}{"91}

\usepackage[T3,T1]{fontenc}
\DeclareSymbolFont{tipa}{T3}{cmr}{m}{n}
\DeclareMathAccent{\invbreve}{\mathalpha}{tipa}{16}

\newcommand{\overbar}[1]{\mkern 6.5mu\overline{\mkern-5.5mu#1\mkern-2.5mu}\mkern 2.5mu}
\newcommand{\overhat}[1]{\mkern 6.5mu\widehat{\mkern-5.5mu#1\mkern-2.5mu}\mkern 2.5mu}
\newcommand{\overtilde}[1]{\mkern 6.5mu\widetilde{\mkern-5.5mu#1\mkern-2.5mu}\mkern 2.5mu}


\input{mathsymbols}

\numberwithin{equation}{section}  
%\theoremstyle{thm}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}[definition]{Example}
\newtheorem{remark}[definition]{Remark}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{algo}[definition]{Algorithm}
\newtheorem{assumption}[definition]{Assumption}
\newtheorem{setting}[definition]{Setting}
\newtheorem{std_assumption}[definition]{Standing Assumption}
\newtheorem{add_assumption}[definition]{Additional Assumption}
%\newtheorem{procedure}[definition]{Procedure}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
	colorlinks,
	linkcolor={red!70!black},
	citecolor={blue!70!black},
	urlcolor={blue!100!black}
}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\TODO}[1]{{\color{red} #1}}
\newcommand{\TOASK}[1]{{\color{blue} #1}}

%\renewcommand{\baselinestretch}{1.5}

\renewcommand{\theequation}{\thesubsection.\arabic{equation}}
\numberwithin{equation}{section}

\renewcommand{\thefigure}{\thesubsection.\arabic{figure}}
\numberwithin{figure}{section}


\renewcommand{\thetable}{\thesubsection.\arabic{table}}
\numberwithin{table}{section}

\frenchspacing
\DontPrintSemicolon

\newenvironment{conceptual}[1][htb]
  {\renewcommand{\algorithmcfname}{Conceptual Algorithm}% Update algorithm name
   \begin{algorithm}[#1]%
  }{\end{algorithm}}

\begin{document}
%==== FRONT PART====
\title[Provably convergent algorithm for free-support Wasserstein barycenter]{Provably convergent algorithm for free-support Wasserstein barycenter of continuous non-parametric measures}



\author[Z. Chen]{Zeyi Chen}
\author[A. Neufeld]{Ariel Neufeld}
\author[Q. Xiang]{Qikun Xiang}

\address{Division of Mathematical Sciences, Nanyang Technological University, 21 Nanyang Link, 637371 Singapore}
\email{chen1417@e.ntu.edu.sg}

\address{Division of Mathematical Sciences, Nanyang Technological University, 21 Nanyang Link, 637371 Singapore}
\email{ariel.neufeld@ntu.edu.sg}

\address{Division of Mathematical Sciences, Nanyang Technological University, 21 Nanyang Link, 637371 Singapore}
\email{qikun.xiang@ntu.edu.sg}

\date{\today}

\begin{abstract}
	We propose a provably convergent algorithm for approximating the Wasserstein barycenter of continuous non-parametric probability measures, and consider its application in probabilistic forecasts aggregation. 
	Our algorithm is inspired by the fixed-point iterative scheme of {\'A}lvarez-Esteban, Del Barrio, Cuesta-Albertos, and Matr{\'a}n (2016) whose convergence relies on obtaining optimal transport (OT) maps exactly which is computationally intractable for general non-parametric measures. 
	To circumvent this difficulty, we develop tailored approximation techniques including consistent OT map estimators. 
	Replacing the exact OT maps in the fixed-point iterative scheme with our estimated counterparts then gives rise to a computationally tractable stochastic fixed-point algorithm which is provably convergent to the true Wasserstein barycenter. 
	Our algorithm remarkably does not restrict the support of the barycenter to be fixed and can be implemented in a distributed computing environment, which makes it suitable for large-scale information aggregation problems. 
	In our numerical experiments, we apply the algorithm to aggregate the probabilistic forecasts on market sales and evaluate the aggregated forecast. 
	Our results showcase that our algorithm is capable of developing high quality forecasts out of individual forecasts of lower quality, and thus harnesses the ``wisdom of crowds''.

    \textbf{Keywords:} {information aggregation, Wasserstein barycenter, optimal transport, shape-constrained regression}
\end{abstract}
\maketitle
% \tableofcontents

%\include{Front/Acronyms}
%\include{Front/Symbols}

%\addcontentsline{toc}{section}{Lists of Tables}
%\newpage

%==== MAIN PART ====
\section{Introduction}
\label{sec: intro}

Aggregating information from multiple heterogeneous data sources is commonly encountered in many application scenarios. Typical instances include coordinating amongst sensor networks in signal processing \citep{estrin1999next,intanagonwiwat2002impact}, forming group consensus from expert judgements or forecasts in decision analysis \citep{winkler1968consensus, clemen1999combining,oHagan2006uncertain}, combining subsample posteriors for large datasets in Bayesian inferences and learning \citep{Minsker2017,bardenet2017markov}, averaging hypotheses from base algorithms via ensemble models in supervised machine learning \citep{wolpert1992stacked,breiman1996bagging, zhou2012ensemble}, etc. 
A common technique in information aggregation utilizes a type of barycenter of data sources represented in a prescribed metric space.
\TODO{[To be discussed: add more examples of barycenter?]} 
In this paper, we are interested in the particular case of information aggregation where information from $K>2$ data sources represented by probability measures $\nu_1,\ldots,\nu_K$ on $\R^d$ are aggregated via their 2-Wasserstein barycenter ($\CW_2$-Barycenter) {\citep{agueh2011barycenters}} defined as follows.

\begin{definition}[$\CW_2$-distance and $\CW_2$-barycenter {\citep{agueh2011barycenters}}]
	\label{def: barycenter}
	The $2$-Wasserstein distance, or $\CW_2$-distance, between two probability measures $\mu, \nu \in \CP_2(\R^d)$ with finite second moments is defined via the following optimal transport problem (see, e.g., \citep{villani2003topics, villani2009optimal,santambrogio2015optimal}) with squared-distance cost (here $\Pi(\mu,\nu)$ represents the set of couplings of $\mu$ and $\nu$; see Definition~\ref{def: coupling} later): 
	\begin{align}
		\CW_2(\mu, \nu) := \biggl( \inf_{\pi \in \Pi(\mu, \nu)} \int_{\R^d \times \R^d} \|\BIx - \BIy\|^2 \DIFFM{\pi}{\DIFF \BIx, \DIFF \BIy}\biggr)^{\frac{1}{2}}.
		\label{eqn: W2 distance}
	\end{align}
	For $\nu_1, \dots, \nu_K \in \CP_2(\R^d)$, weights $w_1, \dots, w_K > 0$ satisfying $\sum_{k = 1}^{K} w_k = 1$, and for any $\mu \in \CP_2(\R^d)$, let $V(\mu)$ denote the convex combination of the squared $\CW_2$-distances between $\mu$ and $\nu_1,\ldots,\nu_K$ given by
	\begin{align}
		\label{eqn: V-definition}
		V(\mu) := \sum_{k = 1}^{K} w_k \CW_2(\mu, \nu_k)^2.
	\end{align}
	Then, $\bar{\mu} \in \CP_2(\R^d)$ is called a $\CW_2$-barycenter of $\nu_1, \dots, \nu_K$ with weights $w_1, \dots, w_K$ if
	\begin{align*}
		\bar{\mu} \in \argmin_{\mu \in \CP_2(\R^d)} V(\mu).
	\end{align*}
\end{definition}
Essentially, the $\CW_2$-distance between two probability measures $\mu, \nu \in \CP_2(\R^d)$ is defined as the minimal transport cost of moving probability mass from $\mu$ to $\nu$ under the squared-distance cost function, which induces a metric on the space of probability measures with finite second moments that metrizes the weak convergence; see, e.g., \citep[Theorem~6.9]{villani2009optimal}.
The seminal work of \citet{agueh2011barycenters} established the existence and uniqueness results of $\CW_2$-barycenter together with its characterization under mild assumptions. 
Due to its appealing geometric and statistical properties, there soon emerged a series of subsequent studies developing the computational aspects of the $\CW_2$-barycenter and its associated variants; see Section~\ref{ssec: literature} for a review. 
Moreover, the $\CW_2$-barycenter has been serving as a powerful tool in widespread applications in terms of distribution aggregation and representation tasks, including but not limited to computer graphics \citep{solomon2015convolutional,rabin2012wasserstein}, statistical machine learning \citep{peyre2019computational,ye2017fast,dognin2019wasserstein}, natural language processing \citep{muzellec2018generalizing,singh2020context}, theoretical economics \citep{petracou2022decision,neufeld2023feasible}, Bayesian statistics \citep{srivastava2015wasp, srivastava2018scalable,backhoff2022bayesian}, network analysis \citep{spelta2024wasserstein}, financial risk management \citep{arias2020risk,papayiannis2022static}, etc. 
In the sequel, we consider $\CW_2$-barycenters with identical weights $w_1 =  \cdots = w_K = \frac{1}{K}$ for simplicity, but we remark that all our results naturally generalize to the case with non-identical weights.

Our work was inspired by the work of \citet{alvarez2016fixed}, which demonstrated that the $\CW_2$-barycenter of absolutely continuous probability measures $\nu_1, \dots, \nu_K\in\CP_{2,\mathrm{ac}}(\R^d)$ can be computed via a fixed-point of the operator $G: \CP_{2, \mathrm{ac}}(\R^d) \to \CP_{2, \mathrm{ac}}(\R^d)$ defined by 
\begin{align}
	\label{eqn: G-operator}
	G(\mu) := \Big[\textstyle \frac{1}{K} \sum_{k = 1}^K T^\mu_{\nu_k}\Big] \sharp \mu \qquad \forall \mu \in \CP_{2, \mathrm{ac}}(\R^d),
\end{align}
where $T^{\mu}_{\nu_k}$ corresponds to Monge's optimal transport (OT) map from $\mu$ to $\nu_k$ (see Brenier's theorem discussed later in Theorem~\ref{thm: Brenier}), i.e.,
\begin{align*}
	\begin{split}
		T^{\mu}_{\nu_k}\in\argmin_{T}\bigg\{\int_{\R^d} \big\|\BIx - T(\BIx)\big\|^2 \DIFFM{\mu}{\DIFF \BIx}:  T: \R^d \rightarrow \R^d \text{ is Borel measurable and }T \sharp \mu = \nu_k \bigg\}.
	\end{split}
\end{align*}
We summarize the nice properties of the $G$-operator developed by \citet{alvarez2016fixed} in the following theorem.
\begin{theorem}[Properties of the $G$-operator {\cite*[Corollary~3.5 and Theorem~3.6]{alvarez2016fixed}}]
	\label{thm: G-property}
	Let $\nu_1,\ldots,\nu_K\in\CP_{2,\mathrm{ac}}(\R^d)$.
	The $G$-operator defined in \eqref{eqn: G-operator} satisfies the following properties.
	\begin{enumerate}[label=(\roman*)]
		\item\label{thms: G-property-continuity}$G: \CP_{2, \mathrm{ac}}(\R^d) \to \CP_{2, \mathrm{ac}}(\R^d)$ is continuous with respect to the $\CW_2$-metric.
		\item\label{thms: G-property-fixedpoint}The unique $\CW_2$-barycenter $\bar{\mu} \in \CP_{2, \mathrm{ac}}(\R^d)$ (see Theorem~\ref{thm: unique barycenter} later) of $\nu_1, \dots, \nu_K$ is a fixed-point of $G$, i.e., $\bar{\mu} = G(\bar{\mu})$.
		\item\label{thms: G-property-accumulation}For any $\mu_0 \in \CP_{2, \mathrm{ac}}(\R^d)$, the sequence $(\mu_t)_{t \in \N}$ generated by the iteration 
		\begin{align}
			\label{eqn: G-iteration}
			\mu_{t + 1} := G(\mu_t) \qquad \forall t\in\N_0
		\end{align}
		is tight. 
		Moreover, every accumulation point of the sequence $(\mu_t)_{t \in \N_0}$ with respect to the $\CW_2$-metric is a fixed-point of $G$. 
	\end{enumerate}
\end{theorem}

Theorem~\ref{thm: G-property}\ref{thms: G-property-accumulation} thus leads to a simple iterative scheme for $\CW_2$-barycenter, where one begins with an arbitrary $\mu_0\in\CP_{2,\mathrm{ac}}(\R^d)$ and iterates (\ref{eqn: G-iteration}) to generate $(\mu_t)_{t\in\N_0}$. 
When $G$ has a unique fixed-point, Theorem~\ref{thm: G-property}\ref{thms: G-property-fixedpoint} and Theorem~\ref{thm: G-property}\ref{thms: G-property-accumulation} then guarantee that $(\mu_t)_{t\in\N_0}$ converges in $\CW_2$ to the $\CW_2$-barycenter of $\nu_1,\ldots,\nu_K$. 
However, when $\nu_1,\ldots,\nu_K$ are general non-parametric probability measures, the iteration (\ref{eqn: G-iteration}) is a theoretical but computationally intractable ``oracle'' since it suffers from the difficulty in computing the OT map $T^\mu_{\nu_k}$ exactly. 
Therefore, numerical implementations of this scheme are either limited to particular parametric measures (see, e.g., \citep[Section~4]{alvarez2016fixed}), or carried out via neural network approximations without convergence guarantees (see, e.g., \citep{korotin2022wasserstein}). 
This drawback has motivated our development of an estimator-based stochastic extension of this deterministic fixed-point iterative scheme with rigorous convergence guarantee. 
The idea of our stochastic fixed-point iterative scheme is sketched in Conceptual Algorithm~\ref{algo: conceptual}, where we approximate each true OT map $T^{\widehat{\mu}_{t}}_{\nu_k}$ by an OT map estimator $\widehat{T}_{t+1,k}$ (Line~\ref{alglin: conceptual-estimator}) and approximate the $G$-operator when updating from $\widehat{\mu}_t$ to $\widehat{\mu}_{t+1}$ (Line~\ref{alglin: conceptual-update}).
In particular, letting $\widehat{T}_{t+1,k}= T^{\widehat{\mu}_{t}}_{\nu_k}$ in Line~\ref{alglin: conceptual-estimator} and letting $\widehat{\mu}_{t+1} = \big[\frac{1}{K} \sum_{k = 1}^K \widehat{T}_{t+1,k}\big] \sharp \widehat{\mu}_{t}$ in Line~\ref{alglin: conceptual-update} recovers the deterministic fixed-point iterative scheme. 
Our objective is to develop a concrete setting as well as a computationally tractable implementation of Conceptual Algorithm~\ref{algo: conceptual} such that the resultant stochastic sequence of probability measures $(\widehat{\mu}_t)_{t\in\N_0}$ will converge to the $\CW_2$-barycenter of $\nu_1,\ldots,\nu_K$ in an appropriate sense. 

\TODO{[To discuss: shall we highlight the main difficulties?]}

\begin{conceptual}[t]
	\KwIn{$K$ input measures $\nu_1, \ldots, \nu_K \in \CP_{2, \mathrm{ac}}(\R^d)$, initial measure $\mu_0 \in \CP_{2, \mathrm{ac}}(\R^d)$.}
	\KwOut{$(\widehat{\mu}_t)_{t \in \N_0}$.}
	\nl Initialize $\widehat{\mu}_0 \leftarrow \mu_0$.\\
	\nl \For{$t = 0, 1, 2, \dots$}{
		\nl \For{$k=1,\ldots,K$}{
			\nl\label{alglin: conceptual-sample1}Randomly generate $N_{t,k}$ i.i.d.\@ samples $\{\BIX_{t+1,k,i}\}_{i=1:N_{t,k}}$ from $\widehat{\mu}_{t}$. \\
			\nl\label{alglin: conceptual-sample2}Randomly generate $N_{t,k}$ i.i.d.\@ samples $\{\BIY_{t+1,k,i}\}_{i=1:N_{t,k}}$ from $\nu_k$. \\
			\nl\label{alglin: conceptual-estimator}Approximate $T^{\widehat{\mu}_{t}}_{\nu_k}$ with an estimator $\widehat{T}_{t+1,k}\approx T^{\widehat{\mu}_{t}}_{\nu_k}$ using the samples $\{\BIX_{t+1,k,i}\}_{i=1:N_{t,k}}$ and $\{\BIY_{t+1,k,i}\}_{i=1:N_{t,k}}$. \\
		}
		\nl\label{alglin: conceptual-update}Choose $\widehat{\mu}_{t+1} \in \CP_{2, \mathrm{ac}}(\R^d)$ such that $\widehat{\mu}_{t+1} \approx \big[\frac{1}{K} \sum_{k = 1}^K \widehat{T}_{t+1,k}\big] \sharp \widehat{\mu}_{t}$.\\
		}
	\nl \Return $\big(\widehat{\mu}_t\big)_{t \in \N_0}$.\\
	\caption{\bf{Stochastic fixed-point iterative scheme}}
	\label{algo: conceptual}
\end{conceptual}

Specifically, our contributions can be summarized as follows: 
\begin{enumerate}[label = (\roman*),beginpenalty=10000]
	\item We provide a computationally tractable stochastic fixed-point algorithm (i.e., Algorithm~\ref{algo: iteration}) for approximately computing the $\CW_2$-barycenter of $\nu_1,\ldots,\nu_K$. 
	Unlike most existing approximation approaches, we neither restrict the support of the approximate $\CW_2$-barycenter to be a finite collection of points, nor restrict the input measures $\nu_1,\ldots,\nu_K$ to be discrete or to specific parametric families of measures. 
	Instead, our algorithm belongs to the class of ``free-support" approaches and adapts to general continuous non-parametric probability measures as inputs.

	\item We perform rigorous convergence analysis of our algorithm to show that it converges to the true $\CW_2$-barycenter of $\nu_1,\ldots,\nu_K$ in an almost sure sense (see Setting~\ref{sett: convergence} and Theorem~\ref{thm: fixedpoint-convergence}). 
	To the best of our knowledge, our algorithm is the first computationally tractable extension of the fixed-point iterative scheme by \citet{alvarez2016fixed} with convergence guarantee. 

	\TODO{
	\item {[To discuss: are our estimators considered as contributions? One novelty is that we incorporate smoothing techniques into the classical shape-constrained convex least squares estimator to make them infinitely differentiable.]}
	
	\item We demonstrate via numerical experiment that our algorithm is effective and can be implemented in a distributed and paralleled computing environment, which has the potential for scalable and efficient applications in distributed systems under diverse application scenarios; see, e.g.,\@ \citep{bertsekas2015parallel} for a comprehensive reference on the related topics.}
\end{enumerate}


\subsection{Literature review}
\label{ssec: literature}
In this subsection, we review two main streams of literature related to our study, namely the numerical methods for approximating Wasserstein barycenters and the statistical estimation approaches of the optimal transport map.

\TODO{[To be done: detailed comparison with a few existing approaches]}

\TODO{
\subsubsection*{Numerical methods for approximating $\CW_2$-barycenters}
\label{sssec: numerics_WB}
Recent years have witnessed an extensive and rapid growth in the literature on numerically computing $\CW_2$-barycenters since the seminal work by \citet{agueh2011barycenters}. 
Exact or regularized methods applicable to specific parametric families of probability measures have been developed, such as Gaussian \citep{agueh2011barycenters, chewi2020gradient}, the location-scatter family \citep{alvarez2016fixed}, or $\varphi$-exponential distributions \citep{kum2022gpm}.
For non-parametric measures, many studies have developed efficient algorithms for the case when the input measures are discrete with finite supports on $\R^d$; see, e.g., \citep{rabin2012wasserstein, cuturi2014fast,anderes2016discrete,izzo2021dimensionality,guo2020fast,ge2019interior,heinemann2022randomized,lin2020fixed,tiapkin2022stochastic,benamou2015iterative}, etc.
In particular, given discrete measures, \citet[Proposition~1]{borgwardt2022lp} showed the existence of a sparsely-supported discrete $\CW_2$-barycenter, and \citet{altschuler2021wasserstein} and \citet{altschuler2022NP} proved that the $\CW_2$-barycenter can be computed in polynomial time for any fixed $d$ yet has exponential dependence on $d$. 

Regarding possibly continuous input measures, a popular strategy is to discretize the support of the underlying $\CW_2$-barycenter over a fixed number of atoms hence it suffices to optimize the histogram weights over a finite-dimensional simplex, which is computationally favorable; see a variety of relevant algorithms in, e.g.,\citep{staib2017parallel,claici2018stochastic,dvurechenskii2018decentralize,takezawa22tree}.
Despite their advantages in computational speed, such ``fixed-support'' algorithms incorporates inductive biases since the true support of the barycenter is unknown a priori. 
In contrast, ``free-support'' algorithms impose no restrictions on the support of the underlying Wasserstein barycenter, and approximative approaches include alternating optimization with Newton's method \citep{cuturi2014fast}, stochastic gradient descent under entropic or quadratic regularization \citep{li2020continuous}, approximation schemes via variational distributions \citep{chi2023variational}, Sinkhorn divergence based Frank--Wolfe algorithm \citep{luise2019sinkhorn}, etc. 
Recently, there exhibits a growing interest in the numerical methods for approximating continuous $\CW_2$-barycenters leveraging neural network parametrization or generative neural networks, while the non-convex training objectives pose challenges to further theoretical analyses; see, e.g., \citep{cohen2020estimating,korotin2022wasserstein,korotin2021continuous,chen2021scalable}. 
To provide theoretic insights, some studies detour to investigate algorithms in tackling the more general MMOT problem and view numerically solving the $\CW_2$-barycenter a specific application of it; see, e.g., \citep{von2022approximative,altschuler2023polynomial,neufeld2022numerical,neufeld2023feasible}, etc.

\subsubsection*{Estimations of the optimal transport map}
In this thesis, we are specially interested in estimators of the optimal transport (OT) map as they constitute a crucial ingredient of our proposed stochastic fixed-point algorithm.
The problem of finding an optimal measure-preserving map without mass splits dates back to Monge's formulation in \citep{monge1781memoire}, which could be infeasible.
Studies by \citet{knott1984optimal} and \citet{brenier1991polar} showed that, for arbitrary measures $\mu, \nu \in \CP_2(\CX)$ where $\CX$ is a Polish space, an OT map always exists and is uniquely determined $\mu$-almost everywhere as the gradient of a convex function referred as the Brenier potential (up to a constant shift), whenever $\mu$ is absolutely continuous with respect to the Lebesgue measure (see details in Theorem~\ref{thm: Brenier}). These classical results provide a useful perspective for computing the Wasserstein distance between measures by solving the underlying OT map, which turns out to be the object of interest in many statistical and machine learning applications on its own right; see a comprehensive review by \citet{peyre2019computational}. However, computing the true OT map has proved to be exceptionally hard and suffer from the curse of dimensionality; see, e.g., \citep{tacskesen2023discrete,taskesen2023semi} for rigorous complexity analyses. As such, many studies focus on designing statistically well-behaved estimators to the OT map out of independent samples from respective probability measures. 

In literature, various efficient heuristic and learning-based methods were discussed for the computational sake; see, e.g., \citep{perrot2016mapping,jagarlapudi2020statistical,makkuva2020optimal}. Recently, diverse types of OT map estimators with rigorous statistical guarantees have been proposed. Under pre-specified regularity assumptions, \citet{hutter2021minimax} disclosed a minimax $\CL^2(\mu)$-convergence rate (of increasing data) for any measurable function of samples as a lower bound (see Theorem~6 therein), and proposed a near-optimal OT map estimator via the truncated wavelet approximation. On the other hand, \citet{gunsilius2022convergence} obtained from kernel density estimations an upper bound on the $\CL^2(\mu)$-risk of plug-in estimators extended over $\R^d$. Subsequently, \citet{pooladian2021entropic} and \citet{deb2021rates} derived distinguished minimax optimal map candidates via barycentric projection techniques \Citep[Definition~5.4.2]{ambrosio2005gradient} in regularized and non-regularized settings. 
\citet{manole2021plugin} sharpened the upper bound risk in \citep{gunsilius2022convergence} and provided in addition so-called ``plug-in'' estimators, which are built upon the optimal transport plan when the source and target measures are replaced by their empirical counterparts yet extended over $\R^d$. Moreover, by employing kernel sum-of-squares \citep[Chapter~3]{blekherman2012semidefinite} as building blocks, \citet{vacher2021dimension} and \citet{muzellec2021near} proposed estimators with comparable $\CL^2(\mu)$-convergence rate plus dimension-free computational complexity up to a constant term potentially exponential on $d$. Regardless of the convergence rate issues, another remarkable stream of works developed a class of plug-in estimators built from smooth and strongly convex regression and interpolation (see recent advances in, e.g., \citep{taylor2017convex,taylor2017smooth,mazumder2019computational,seijo2011nonparametric,lim2012consistency}, and references therein) to approximate OT maps, in light of the underlying geometric properties of Brenier potential as implied by the prominent Caffarelli's regularity theory \citep*{caffarelli1992regularity}. For instance, \citet{paty2020regularity} formulated a quadratically constrained quadratic program (QCQP) for approximating Brenier potential leveraging the convex interpolability framework developed by \citet{taylor2017convex}; \citet{curmei2023shape} developed a hierarchy of semi-definite programs against the shape constraints using sum-of-square polynomials; \citet{gonzalez2022gan} deployed state-of-the-art Lipschitz-constrained generative adversarial networks (GAN) for the regression task. 

}


\subsection{Organization}
\label{ssec: organization}
This paper is organized as follows. 
Section~\ref{sec: preliminaries} introduces the notations used in this paper and crucial preliminary results on which our contributions are based. 
Section~\ref{sec: fixedpoint-algo} presents our stochastic fixed-point algorithm for approximating the $\CW_2$-barycenter.
We also perform detailed analysis of its convergence. 
In Section~\ref{sec: concrete-estimators}, we develop two concrete plug-in OT map estimators that can guarantee the convergence of our stochastic fixed-point algorithm. 
\TODO{Finally, we compare our proposed algorithm and estimators with other state-of-the-art algorithms in $\CW_2$-barycenter approximations through numerical experiments in Section~\ref{sec: numerics}, thus verify their efficacy.}


\section{Notations and Preliminaries}
\label{sec: preliminaries}

\subsection{Notations}
\label{ssec: notations}
In the following, we introduce the terminologies and notations that are used throughout this paper. 
All vectors are assumed to be column vectors and are denoted by boldface symbols. 
In particular, for $k\in\N$, $\veczero_k$ denotes the vector in $\R^k$ with all entries equal to zero. 
We also use $\veczero$ when the dimension can be inferred from the context. 
We denote by $\langle \cdot, \cdot \rangle$ the Euclidean dot product, i.e., $\langle \BIx, \BIy \rangle := \BIx^\TRANSP \BIy$ and we denote by $\|\cdot\|$ the Euclidean norm, i.e., $\|\BIx\|:=(\langle\BIx,\BIx\rangle)^{\frac{1}{2}}$. 
Open and closed balls centered at $\BIx$ with radius $r$ are denoted by $B(\BIx,r)$ and $\bar{B}(\BIx,r)$, respectively.
For any set $\CX \subseteq \R^d$, we let $\inter(\CX)$, $\clos(\CX)$, and $\boundary(\CX)$ denote its interior, closure, and boundary, respectively. 
Moreover, for two matrices $\BA$ and $\BB$, $\BA \succeq \BB$ indicates that $\BA - \BB$ is positive semi-definite.
Furthermore, for $k\in\N$, $\BI_k$ denotes the $k$-by-$k$ identity matrix. 

For a closed subset $\CX$ of a Euclidean space, let $\CB(\CX)$ denote the Borel subsets of $\CX$, and let $\CP(\CX)$ denote the set of Borel probability measures on $\CX$, while $\CP_2(\CX)\subseteq\CP(\CX)$ consists of the ones with finite second moments. 
The associated set $\CP_{2, \mathrm{ac}}(\CX)$ contains probability measures in $\CP_2(\CX)$ which are absolutely continuous with respect to the Lebesgue measure. 
For any $\mu\in\CP(\CX)$ and any $\CY\in\CB(\CX)$ with $\mu(\CY)>0$, let $\mu|_{\CY}$ denote the probability measure formed by truncating $\mu$ to $\CY$, i.e., $\mu|_{\CY}(A):=\frac{\mu(\CY\cap A)}{\mu(\CY)}$ for all $A\in\CB(\CX)$.
Moreover, for two closed subsets $\CX,\CY$ of Euclidean spaces and a Borel measurable function $T:\CX\to\CY$, the pushforward of a probability measure $\mu\in\CP(\CX)$ by $T$ is denoted by $T\sharp\mu$, i.e., $T\sharp\mu:=\mu\circ T^{-1}$.

Let us introduce the notations for the following families of functions. 
For an open bounded set $\CX\subset\R^d$ and for $k\in\N_0$, $\alpha\in(0,1]$, 
let $\CC^k(\clos(\CX))$ denote the set of $\R$-valued continuous functions on $\clos(\CX)$ that are $k$-times continuously differentiable on $\CX$, 
and let $\CC^{k,\alpha}(\clos(\CX))$ denote the set of $\R$-valued continuous functions on $\clos(\CX)$ that are $k$-times continuously differentiable on $\CX$ whose $k$-th order partial derivatives are $\alpha$-H{\"o}lder continuous.
In particular, $\CC^{k,\alpha}(\clos(\CX))$ is a Banach space with respect to the following norm (see, e.g., \citet[Theorem~5.1.1]{evans2022partial}):
\begin{align*}
	\|\varphi\|_{\CC^{k,\alpha}(\clos(\CX))}:=\max_{|\Bbeta|\le k}\sup_{\BIx\in\CX}\big|\partial^{\Bbeta}\varphi(\BIx)\big|+\max_{|\Bbeta|=k}\sup_{\BIx,\BIy\in\CX}\frac{\big|\partial^{\Bbeta}\varphi(\BIx)-\partial^{\Bbeta}\varphi(\BIy)\big|}{\|\BIx-\BIy\|^{\alpha}} \qquad\forall \varphi\in \CC^{k,\alpha}(\clos(\CX)),
\end{align*}
where $\Bbeta=(\beta_1,\ldots,\beta_d)\in\N_0^d$ is a multi-index, $|\Bbeta|:=\beta_1+\cdots+\beta_d$, and $\partial^{\Bbeta}\varphi:=\frac{\partial^{|\Bbeta|}\varphi}{\partial x_1^{\beta_1}\cdots\partial x_d^{\beta_d}}$ denotes the partial derivative of $\varphi$ with respect to the multi-index $\Bbeta$. 
We refer to $\CC^{k,\alpha}(\clos(\CX))$ as the set of $(k,\alpha)$-H{\"o}lder functions on $\clos(\CX)$. 
Moreover, let $\CC^{k,\alpha}_{\mathrm{loc}}(\R^d)$ denote the set of $\R$-valued functions on $\R^d$ that are $(k,\alpha)$-H{\"o}lder when restricted to $\clos(\CX)$ for any bounded open set $\CX\subset\R^d$.
We refer to $\CC^{k,\alpha}_{\mathrm{loc}}(\R^d)$ as the set of locally $(k,\alpha)$-H{\"o}lder functions on $\R^d$. 
In addition, let $\CC^{\infty}(\R^d)$ denote the set of infinitely differentiable $\R$-valued functions on $\R^d$.
Lastly, we denote by $\CC_{\mathrm{lin}}(\R^d,\R^d)$ the set of continuous functions from $\R^d$ to $\R^d$ that have at most linear growth, i.e., $T\in\CC_{\mathrm{lin}}(\R^d,\R^d)$ if and only if $\sup_{\BIx\in\R^d}\frac{\|T(\BIx)\|}{1+\|\BIx\|}<\infty$. 
Note that $\CC_{\mathrm{lin}}(\R^d,\R^d)$ is a Banach space with respect to the norm $\|T\|_{\CC_{\mathrm{lin}}(\R^d,\R^d)}:=\sup_{\BIx\in\R^d}\frac{\|T(\BIx)\|}{1+\|\BIx\|}$ $\forall T\in \CC_{\mathrm{lin}}(\R^d,\R^d)$. 

 
\subsection{Smooth and strongly convex functions}
\label{ssec: smooth-strongly-convex}
We give an overview of the notion of smooth and strongly convex functions that is frequently used in our discussions.

\begin{definition}[Smooth and strongly convex functions]
	\label{def: smooth-strongly-convex}
	For $0\le \underline{l}\le \overline{l}\le \infty$, a proper, lower semi-continuous (l.s.c.\@) and convex function $\varphi: \R^d \rightarrow \R \cup \{\infty\}$ is called $\overline{l}$-smooth if
	\begin{align*}
		\varphi(\BIy) \leq \varphi(\BIx) + \langle\BIg, \BIy - \BIx\rangle + \frac{\overline{l}}{2}\|\BIx - \BIy\|^2 \qquad\forall \BIx,\BIy\in\R^d, \; \forall \BIg\in\partial \varphi(\BIx),
	\end{align*}
	and is called $\underline{l}$-strongly convex if
	\begin{align*}
		\varphi(\BIy) \geq \varphi(\BIx) + \langle\BIg, \BIy - \BIx\rangle + \frac{\underline{l}}{2}\|\BIx - \BIy\|^2 \qquad \forall \BIx,\BIy\in\R^d, \; \forall \BIg\in\partial \varphi(\BIx).
	\end{align*}
\end{definition}
We denote by $\FC_{\underline{l}, \overline{l}}(\R^d)$ the collection of proper l.s.c.\@ convex functions on $\R^d$ which are $\overline{l}$-smooth and $\underline{l}$-strongly convex. In particular, $\FC_{0, \infty}(\R^d)$ contains all proper l.s.c.\@ convex functions on $\R^d$. 
Moreover, we denote 
$\FC^{\infty}_{\underline{l},\overline{l}}(\R^d):=\CC^{\infty}(\R^d)\cap\FC_{\underline{l},\overline{l}}(\R^d)$, 
$\FC^{k}_{\underline{l},\overline{l}}(\R^d):=\CC^{k}(\R^d)\cap\FC_{\underline{l},\overline{l}}(\R^d)$, 
$\FC^{k,\alpha}_{\underline{l},\overline{l}}(\R^d):=\CC^{k,\alpha}_{\mathrm{loc}}(\R^d)\cap\FC_{\underline{l},\overline{l}}(\R^d)$ for $k\in\N_0$, $\alpha\in(0,1]$.
It follows from classical results (see, e.g., \citep[Lemma~1.2.3 \& Theorem~2.1.5]{nesterov2004introconvex}) that for $\overline{l}<\infty$, every $\varphi\in\FC_{0,\overline{l}}(\R^d)$ is continuously differentiable and $\nabla\varphi$ is $\overline{l}$-Lipschitz continuous. 


\subsection{Optimal transport and Wasserstein distance}
\label{ssec:setting-OTWass}
Many of our discussions in this paper are established upon results from optimal transport theory, especially properties around the Wasserstein distance between probability measures; see, e.g.,\@ the seminal work of \citet{villani2003topics, villani2009optimal} and \citet{santambrogio2015optimal}.
We start by introducing the notion of couplings, which is detailed as follows. 
\begin{definition}[Coupling]
	\label{def: coupling}
	Given $m \in \N$ probability measures $\nu_1 \in \CP(\CX_1), \dots, \nu_m \in \CP(\CX_m)$ on closed subsets $\CX_1, \dots, \CX_m$ of $\R^d$, the set of couplings of $\nu_1, \dots, \nu_m$ is denoted by $\Pi(\nu_1, \dots, \nu_m)$, which is defined as
	\begin{align*}
		\Pi(\nu_1, \dots, \nu_m) := \big\{ \pi \in \CP(\CX_1 \times \dots \times \CX_m): \text{ the marginal of }\pi \text{ on } \CX_i \text{ is } \nu_i \text{ for }i = 1, \dots, m \big\}.
	\end{align*}
\end{definition}

The minimization problem embedded in the formulation (\ref{eqn: W2 distance}) is known as Kantorovich's optimal transport problem \citep{kantorovich1948problem} with respect to the squared-distance cost, and the infimum is well known to be attained by an optimal coupling; see, e.g., \citep[Theorem~4.1]{villani2009optimal}.
In the rest of this paper, the optimality of a coupling is always considered with respect to the squared-distance cost.


The existence of a $\CW_2$-barycenter is shown by \citet[Proposition~2.3]{agueh2011barycenters}. In general, there may exist more than one $\CW_2$-barycenters of $\nu_1, \nu_2, \dots, \nu_K$. A sufficient condition to guarantee the uniqueness of the $\CW_2$-barycenter is given as follows.

\begin{theorem}[{\citep[Proposition~3.5 \& Theorem~5.1]{agueh2011barycenters}}]
	\label{thm: unique barycenter}
	Among $\nu_1, \dots, \nu_K \in \CP_2(\R^d)$, if there exists at least one index $k \in \{1, \dots, K\}$ such that $\nu_k \in \CP_{2, \mathrm{ac}}(\R^d)$, then the $\CW_2$-barycenter $\bar{\mu}$ in Definition~\ref{def: barycenter} is unique. Moreover, if there exists at least one index $k \in \{1, \dots, K\}$ such that $\nu_k$ has bounded density, then the unique $\bar{\mu} \in \CP_{2, \mathrm{ac}}(\R^d)$. 
\end{theorem}


Next, let us present Brenier's theorem which characterizes optimal couplings with gradient of convex functions when the source measure $\mu$ is absolutely continuous with respect to the Lebesgue measure.

\begin{theorem}[Brenier's theorem {\citep{brenier1991polar} \& \citep[Theorem~2.12]{villani2003topics}}]
	\label{thm: Brenier}
	Let $\mu\in\CP_{2,\mathrm{ac}}(\R^d)$, $\nu \in \CP_2(\R^d)$. 
	Then, there is a unique optimal coupling $\pi^\star \in \Pi(\mu, \nu)$ that minimizes (\ref{eqn: W2 distance}). 
	Moreover, $\pi\in \Pi(\mu, \nu)$ minimizes (\ref{eqn: W2 distance}) if and only if there exists a lower semi-continuous (l.s.c.\@) and convex function $\varphi^{\mu}_{\nu}:\R^d\to\R\cup\{\infty\}$ such that $\pi = \big[I_d, T^{\mu}_{\nu}\big] \sharp\mu$ where $I_d:\R^d\to\R^d$ denotes the identity map on $\R^d$ and $T^{\mu}_{\nu}=\nabla\varphi^{\mu}_{\nu}$ is the gradient of $\varphi^{\mu}_{\nu}$ (that is uniquely determined $\mu$-almost everywhere). 
	In this case, $T^{\mu}_{\nu}$ is also the $\mu$-almost everywhere unique optimal solution of Monge's optimal transport problem:
	\begin{align*}
		\begin{split}
			\inf\bigg\{\int_{\R^d} \big\|\BIx - T(\BIx)\big\|^2 \DIFFM{\mu}{\DIFF \BIx}:  T: \R^d \rightarrow \R^d \text{ is Borel measurable and } T \sharp \mu = \nu  \bigg\}.
		\end{split}
	\end{align*}
\end{theorem}

We refer to $\varphi^{\mu}_{\nu}:\R^d\to\R\cup\{\infty\}$ and $T^{\mu}_{\nu}:\R^d\to\R^d$ in Theorem~\ref{thm: Brenier} as the optimal Brenier potential from $\mu$ to $\nu$ and the optimal transport (OT) map from $\mu$ to $\nu$, respectively. 
Note that the optimal Brenier potential $\varphi^{\mu}_{\nu}$ is $\mu$-almost everywhere uniquely determined up to the addition of an arbitrary constant. 


The convergence of our proposed algorithm requires regularity properties of the OT map $T^{\mu}_{\nu}$. 
Regarding this matter, a series of studies by \citet{caffarelli1990localization, caffarelli1991some, caffarelli1992regularity, caffarelli1996boundary} developed foundations of the regularity theory of OT maps under suitable geometric assumptions on supports and densities of the measures. 
Here, we report a part of these results in \citep{villani2009optimal}.

\begin{theorem}[Caffarelli's global regularity theory; see, e.g.\@, {\citep[Theorem~12.50]{villani2009optimal}}]
	\label{thm: caffarelli}
	Let $\CX_\mu$ and $\CX_\nu$ be two connected bounded open sets in $\R^d$ that both have $\CC^2$-boundaries and are both uniformly convex\footnote{A set $\CX \subset \R^d$ is said to have $\CC^p$ boundary with $p \in [0, +\infty)$ if $\boundary(\CX)$ is locally the graph of a $\CC^p$ function, and is said to be uniformly convex if for every $\epsilon > 0$, there exists $\delta > 0$ such that, for any $\BIx, \BIy \in \CX$ with $\|\BIx - \BIy\| < \epsilon$, the distance from the mid-point $(\BIx + \BIy)/2$ to $\boundary(\CX)$ is at least $\delta$.}.
	Let $\mu\in\CP_{2,\mathrm{ac}}(\R^d)$ be concentrated on $\CX_{\mu}$ and let $\nu\in\CP_{2,\mathrm{ac}}(\R^d)$ be concentrated on $\CX_{\nu}$, i.e., \sloppy{$\mu(\R^d\backslash \CX_{\mu}) =\nu(\R^d\backslash\CX_{\nu}){=0}$}.
	Suppose that for $k\in\N_0$, $\alpha\in(0,1]$, $f_{\mu}\in\CC^{k,\alpha}\big(\clos(\CX_{\mu})\big)$ and $f_{\nu}\in\CC^{k,\alpha}\big(\clos(\CX_{\nu})\big)$ are the density functions of $\mu$ and $\nu$ with respect to the Lebesgue measure, respectively. 
	Moreover, suppose that there exists $\gamma>1$ such that $\gamma^{-1}\le f_{\mu}(\BIx)\le \gamma$ for all $\BIx\in\clos(\CX_{\mu})$ and that $\gamma^{-1}\le f_{\nu}(\BIx)\le\gamma$ for all $\BIx\in\clos(\CX_{\nu})$. 
	Then, the optimal Brenier potential $\varphi^{\mu}_{\nu}$ satisfies $\varphi^{\mu}_{\nu}\in\CC^{k+2,\alpha}\big(\clos(\CX_{\mu})\big)$.
\end{theorem}


\section{Stochastic Fixed-Point Algorithm for $\CW_2$-Barycenter}
\label{sec: fixedpoint-algo}
In this section, we will present our computationally tractable stochastic fixed-point algorithm for $\CW_2$-Barycenter and show its convergence. 
Section~\ref{ssec: fixedpoint-settings} introduces the specifications of the approximation steps in Line~\ref{alglin: conceptual-estimator} and Line~\ref{alglin: conceptual-update} of Conceptual Algorithm~\ref{algo: conceptual} as well as additional assumptions. 
In Section~\ref{ssec: fixedpoint-convergence}, we develop sufficient conditions for the convergence of our stochastic fixed-point algorithm. 


\subsection{Settings}
\label{ssec: fixedpoint-settings}
Conceptual Algorithm~\ref{algo: conceptual} illustrates the conceptual procedure of our stochastic fixed-point iterative scheme. 
Before presenting its computationally tractable implementation as a concrete algorithm, let us introduce the following additional notions in Definition~\ref{def: admissible-measures}, Assumption~\ref{asp: family-truncation}, and Assumption~\ref{asp: plugin-estimator}. 

\begin{definition}[Admissible support sets and admissible probability measures]
	\label{def: admissible-measures}
	Let $\CS$ denote the collection of subsets of $\R^d$ defined as follows:
	\begin{align*}
		\CS:=\big\{\clos(\CY):\CY\subset\R^d \text{ is open, bounded, uniformly convex, and has a }\CC^2 \text{-boundary}\big\}.
	\end{align*}
	We will refer to $\CS$ as the admissible support sets.
	Let $\CM$ denote the collection of probability measures on $\R^d$ defined as follows:
	\begin{align*}
		\CM:=\left\{\mu\in\CP_{2,\mathrm{ac}}(\R^d) : \begin{tabular}{l}
			$\support(\mu)\in\CS,\; \exists\alpha\in(0,1],\;\exists \gamma>1,\;\exists f_{\mu}\in\CC^{0,\alpha}(\support(\mu)),$ \\
			$\gamma^{-1}\le f_{\mu}(\BIx)\le \gamma\;\forall \BIx\in\support(\mu),\;f_{\mu}$ is the density function of $\mu$
			\end{tabular}
		\right\}.
	\end{align*}
	We will refer to $\CM$ as the admissible compactly supported probability measures.
	Moreover, let $\CM_{\mathrm{full}}$ denote the collection of probability measures on $\R^d$ defined as follows:
	\begin{align*}
		\CM_{\mathrm{full}}:=\left\{\rho\in\CP_{2,\mathrm{ac}}(\R^d): \begin{tabular}{l}
			$\support(\rho)=\R^d,\; \exists\alpha\in(0,1],\;\exists f_{\rho}\in\CC_{\mathrm{loc}}^{0,\alpha}(\R^d),$ \\
			$f_{\rho}(\BIx)> 0\;\forall \BIx\in\R^d,\;f_{\rho}$ is the density function of $\rho$
			\end{tabular}
		\right\}. 
	\end{align*}
	We will refer to $\CM_{\mathrm{full}}$ as the admissible fully supported probability measures. 
\end{definition}

The conditions in the definitions of the admissible support sets and the admissible compactly supported probability measures are motivated by the conditions in Caffarelli's global regularity theory (Theorem~\ref{thm: caffarelli}).
As a result, we can derive the following curvature properties of the optimal Brenier potential  $\varphi^{\mu}_{\nu}$ from $\mu\in\CM$ to $\nu\in\CM$, which will serve as a crucial premise when controlling the estimation errors of OT map estimators; see the results in Section~\ref{sec: concrete-estimators}.

\begin{lemma}[Curvature properties of $\varphi^{\mu}_{\nu}$ {\citep[Lemma~2]{manole2021plugin} \& \citep[Corollary 3.2]{gigli2011on}}]
	\label{lem: curvature}
	Let $\mu,\nu\in\CM$ be arbitrary. 
	Then, $\varphi^{\mu}_{\nu}\in\CC^2(\support(\mu))$ and there exist $0<\lambda_{\mathrm{LB}}\le \lambda_{\mathrm{UB}}<\infty$ such that $\lambda_{\mathrm{LB}}\BI_d\preceq \nabla^2 \varphi^{\mu}_{\nu}(\BIx)\preceq \lambda_{\mathrm{UB}}\BI_d$ for all $\BIx\in\support(\mu)$, where $\varphi^{\mu}_{\nu}:\R^d\to\R$ is the  optimal Brenier potential from $\mu$ to $\nu$ (that is unique {$\mu$-almost everywhere} up to the addition of an arbitrary constant). 
	Moreover, there exists $\widetilde{\varphi}^{\mu}_{\nu}\in\FC_{\lambda_{\mathrm{LB}},\lambda_{\mathrm{UB}}}(\R^d)$ that is equal to $\varphi^{\mu}_{\nu}$ $\mu$-almost everywhere, and therefore one can let $\varphi^{\mu}_{\nu}\in\FC_{\lambda_{\mathrm{LB}},\lambda_{\mathrm{UB}}}(\R^d)$ without loss of generality. 
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem: curvature}]
	Let $f_{\mu}$ and $f_{\nu}$ denote the density functions of $\mu$ and $\nu$ which satisfy the conditions in Definition~\ref{def: admissible-measures}.
	The properties of $\mu$ and $\nu$ in Definition~\ref{def: admissible-measures} and Caffarelli's global regularity theory (Theorem~\ref{thm: caffarelli}) imply that the optimal Brenier potential $\varphi^{\mu}_{\nu}$ satisfies $\varphi^{\mu}_{\nu}\in\CC^{2,\alpha}(\support(\mu))$ for some $\alpha\in(0,1]$.
	Thus, there exists $\lambda_{\mathrm{UB}}<\infty$ such that $\nabla^2 \varphi^{\mu}_{\nu}(\BIx)\preceq \lambda_{\mathrm{UB}}\BI_d$ for all $\BIx\in\support(\mu)$. 
	Moreover, $\varphi^{\mu}_{\nu}$ needs to satisfy the following Monge--Amp{\`e}re type equation as implied by the change of variable formula for pushforward (see, e.g., \citep[Lemma 5.5.3]{ambrosio2005gradient}):
	\begin{align*}
		\det\big(\nabla^2\varphi^{\mu}_{\nu}(\BIx)\big)=\frac{f_{\mu}(\BIx)}{f_{\nu}\big(\nabla\varphi^{\mu}_{\nu}(\BIx)\big)} \qquad\forall \BIx\in\support(\mu).
	\end{align*}
	Since both $f_{\mu}$ and $f_{\nu}$ are bounded from above and bounded away from~0, it follows that $\det\big(\nabla^2\varphi^{\mu}_{\nu}(\BIx)\big)$ is bounded away from~0 on $\support(\mu)$.
	Consequently, there exists $\lambda_{\mathrm{LB}}>0$ such that $\nabla^2 \varphi^{\mu}_{\nu}(\BIx) \succeq \lambda_{\mathrm{LB}}\BI_d$ for all $\BIx\in\support(\mu)$.
	Lastly, due to the closedness and convexity of $\support(\mu)$, $\varphi^{\mu}_{\nu}$ can be extended to a $\lambda_{\mathrm{UB}}$-smooth, $\lambda_{\mathrm{LB}}$-strongly convex function $\widetilde{\varphi}^{\mu}_{\nu}\in\FC_{\lambda_{\mathrm{LB}},\lambda_{\mathrm{UB}}}(\R^d)$ such that $\widetilde{\varphi}^{\mu}_{\nu}(\BIx)=\varphi^{\mu}_{\nu}(\BIx)$ and $\nabla\widetilde{\varphi}^{\mu}_{\nu}(\BIx)=\nabla\varphi^{\mu}_{\nu}(\BIx)$ for all $\BIx\in\support(\mu)$; see, e.g., \citep[Corollary~2.60]{taylor2017convex}.
	The proof is now complete.
\end{proof}

Our algorithm uses a family of increasing sets for truncating probability measures in $\CM_{\mathrm{full}}$ to probability measures in $\CM$, which satisfies the assumption below. 
\begin{assumption}[Family of increasing sets]
	\label{asp: family-truncation}
	$(\CX_r)_{r\in\N}$ is an infinite collection of subsets of $\R^d$ that satisfies the following conditions:
	\begin{enumerate}[label=(\roman*), beginpenalty=10000]
		\item for all $r\in\N$, $\CX_r\in\CS$ where $\CS$ is the collection of admissible support sets in Definition~\ref{def: admissible-measures};
		
		\item $\veczero_d\in\CX_1$;
		
		\item for all $r\in\N$, $\CX_{r+1}\supseteq\CX_r$;
		
		\item $\bigcup_{r\in\N}\CX_r=\R^d$.
	\end{enumerate}
\end{assumption}
A concrete example of such a family of increasing sets is $\big(\bar{B}(\veczero_d,r)\big)_{r\in\N}$.
Similarly, a family of nested ellipsoids in $\R^d$ containing the origin also satisfies Assumption~\ref{asp: family-truncation}.

With respect to any pair of admissible compactly supported probability measures $\mu,\nu\in\CM$, we consider plug-in estimators of the true OT map $T^{\mu}_{\nu}$ from $\mu$ to $\nu$ which satisfy the conditions in the assumption below.
\begin{assumption}[Plug-in OT map estimator]
	\label{asp: plugin-estimator}
	Let $(\Omega,\CF,\PROB)$ be a probability space.
	For $\mu,\nu\in\CM$ with $\veczero_d\in\support(\mu)$, and for $m,n\in\N$, let $\BIX_1,\ldots,\BIX_m,\BIY_1,\ldots,\BIY_n:\Omega\to\R^d$ be independent random variables such that $\mathrm{law}(\BIX_i)=\mu$ for $i=1,\ldots,m$ and $\mathrm{law}(\BIY_j)=\nu$ for $j=1,\ldots,n$. 
	For any $\theta\in\N$, let \sloppy{$\widehat{T}^{\mu,m}_{\nu,n}(\BIX_1,\ldots,\BIX_m,\BIY_1,\ldots,\BIY_n; \theta)$} be a $\CC_{\mathrm{lin}}(\R^d,\R^d)$-valued random variable that estimates the OT map $T^{\mu}_{\nu}$ from $\mu$ to $\nu$ based on $m$ independent samples from $\mu$ and $n$ independent samples from $\nu$, where $\theta$ denotes a parameter that, for example, represents the extend of smoothing/regularization.
	We assume that $\theta$ takes value in $\N$ for simplicity; see Section~\ref{sec: concrete-estimators} for details about this parameter in concrete plug-in OT map estimators. 
	For the sake of notational simplicity, we make the dependence of this estimated OT map on the samples implicit and use $\widehat{T}^{\mu,m}_{\nu,n}(\BIx;\theta)$ to denote $\widehat{T}^{\mu,m}_{\nu,n}(\BIX_1,\ldots,\BIX_m,\BIY_1,\ldots,\BIY_n;\theta)$ evaluated at $\BIx\in\R^d$.

	We assume that $\widehat{T}^{\mu,m}_{\nu,n}(\,\cdot\,;\theta)$ satisfies the following conditions.
	\begin{enumerate}[label=(\roman*), beginpenalty=10000]
		\item\label{asps: plugin-estimator-shape}\textbf{Shape condition:}
		there exist constants $\alpha\equiv \alpha(\mu,\nu)\in(0,1]$, $\underline{\lambda}\equiv \underline{\lambda}(\mu,\nu)$, $\overline{\lambda} \equiv \overline{\lambda}(\mu,\nu)$ with $0<\underline{\lambda}\le \overline{\lambda}<\infty$, all of which depend on $\mu$ and $\nu$, such that
		for all $m,n\in\N$ and all $\theta\in\N$, it holds $\PROB$-almost surely that $\widehat{T}^{\mu,m}_{\nu,n}(\,\cdot\,;\theta)=\nabla \widehat{\varphi}^{\mu,m}_{\nu,n}(\,\cdot\,;\theta)$ for $\widehat{\varphi}^{\mu,m}_{\nu,n}(\,\cdot\,;\theta)\in\FC_{\underline{\lambda},\overline{\lambda}}^{2,\alpha}(\R^d)$.
		
		\item\label{asps: plugin-estimator-growth}\textbf{Growth condition:}
		there exist constants $u_1(\nu),u_2(\nu)\in\R_+$ that only depend on $\nu$ such that,
		for all $m,n\in\N$ and all $\theta\in\N$, it holds that $\EXP\Big[\big\|\widehat{T}^{\mu,m}_{\nu,n}(\BIx;\theta)\big\|^2\Big]\le u_1(\nu)+u_2(\nu)\|\BIx\|^2$ $\forall \BIx\in\R^d$. 
		
		\item\label{asps: plugin-estimator-consistency}\textbf{Consistency condition:}
		for any $\epsilon>0$, there exist $\overline{n}(\mu,\nu,\epsilon)\in\N$ that depends on $\mu,\nu,\epsilon$ and $\overline{\theta}(\mu,\nu, m, n,\epsilon)\in\N$ that depends on $\mu,\nu,\epsilon$ as well as the sample sizes $m,n$ such that
		\begin{align*}
			\hspace{30pt}\EXP\Big[\big\|\widehat{T}^{\mu,m}_{\nu,n}(\,\cdot\,;\theta)-T^{\mu}_{\nu}\big\|^2_{\CL^2(\mu)}\Big]\le \epsilon \qquad \forall m\ge \overline{n}(\mu,\nu,\epsilon),\;\forall n\ge \overline{n}(\mu,\nu,\epsilon),\; \forall \theta\ge\overline{\theta}(\mu,\nu, m, n,\epsilon),
		\end{align*}
		where
		\begin{align*}
			\big\|\widehat{T}^{\mu,m}_{\nu,n}(\,\cdot\,;\theta)-T^{\mu}_{\nu}\big\|_{\CL^2(\mu)} := \left(\int_{\R^d}\big\|\widehat{T}^{\mu,m}_{\nu,n}(\BIx;\theta)-T^{\mu}_{\nu}(\BIx)\big\|^2\DIFFM{\mu}{\DIFF\BIx}\right)^{\frac{1}{2}}.
		\end{align*}
	\end{enumerate}
\end{assumption}
Concrete examples of plug-in OT map estimators which satisfy Assumption~\ref{asp: plugin-estimator} will be introduced later in Section~\ref{sec: concrete-estimators}.
Note that the consistency condition in Assumption~\ref{asp: plugin-estimator}\ref{asps: plugin-estimator-consistency} is possible due to the curvature properties of $T^{\mu}_{\nu}=\nabla\varphi^{\mu}_{\nu}$ in Lemma~\ref{lem: curvature}.

With these notions, Algorithm~\ref{algo: iteration} describes a computationally tractable algorithm which completes Conceptual Algorithm~\ref{algo: conceptual}. 
The setting for Algorithm~\ref{algo: iteration} is presented below.

\begin{algorithm}[t]
	\KwIn{$K$ input measures $\nu_1, \ldots, \nu_K \in \CM$, $\rho_0 \in \CM_{\mathrm{full}}$, family $(\CX_r)_{r \in \N}$ of increasing sets, plug-in OT map estimator $\widehat{T}^{\mu,m}_{\nu,n}(\,\cdot\,;\theta)$.}
	\KwOut{$(\widehat{\mu}_t)_{t \in \N_0}$.}
	\nl\label{alglin: iteration-initialize}Initialize $\widehat{\rho}_0 \leftarrow \rho_0$.\\
	\nl\label{alglin: iteration-iteration}\For{$t = 0, 1, 2, \ldots$}{
		\nl\label{alglin: iteration-radius}Choose $R_t\in\N$. \\
		\nl\label{alglin: iteration-truncation}$\widehat{\mu}_t\leftarrow \widehat{\rho}_t|_{\CX_{R_t}}$. \\
		\nl \For{$k=1,\ldots,K$}{
			\nl\label{alglin: iteration-samplesize}Choose $N_{t,k}\in\N$ and $\ITheta_{t,k}\in\N$. \\
			\nl\label{alglin: iteration-sample1}Randomly generate $N_{t,k}$ i.i.d. samples $\{\BIX_{t+1,k,i}\}_{i=1:N_{t,k}}$ from $\widehat{\mu}_t$. \\
			\nl\label{alglin: iteration-sample2}Randomly generate $N_{t,k}$ i.i.d. samples $\{\BIY_{t+1,k,i}\}_{i=1:N_{t,k}}$ from $\nu_k$. \\
			\nl\label{alglin: iteration-estimator}$\widehat{T}_{t+1,k}\leftarrow \widehat{T}^{\mu,m}_{\nu,n}(\,\cdot\,;\theta)\big|_{\mu=\widehat{\mu}_t,\,\nu=\nu_k,\,m=n=N_{t,k},\,\theta=\ITheta_{t,k}}$.\\
		}
		\nl\label{alglin: iteration-pushforward}$\widehat{\rho}_{t+1}\leftarrow \Big[\frac{1}{K}\sum_{k=1}^K\widehat{T}_{t+1,k}\Big]\sharp\widehat{\rho}_t$. \\
	}
	\nl \Return $(\widehat{\mu}_t)_{t \in \N_0}$.\\
	\caption{\textbf{Computationally tractable stochastic fixed-point iterative scheme}}
	\label{algo: iteration}
\end{algorithm}


\begin{setting}[Inputs of Algorithm~\ref{algo: iteration}]
\label{sett: algo-iteration}
In Algorithm~\ref{algo: iteration}, we assume that the inputs $\nu_1,\ldots,\nu_K$ are admissible compactly supported probability measures in $\CM$.
The input $\rho_0$ is an arbitrary admissible fully supported probability measure in $\CM_{\mathrm{full}}$. 
Moreover, we assume that $(\CX_r)_{r \in \N}$ is a family of increasing sets satisfying the conditions in Assumption~\ref{asp: family-truncation}, and $\widehat{T}^{\mu,m}_{\nu,n}(\,\cdot\,;\theta)$ is a plug-in OT map estimator satisfying the conditions in Assumption~\ref{asp: plugin-estimator}.
\end{setting}

Definition~\ref{def: admissible-measures} and the shape condition in Assumption~\ref{asp: plugin-estimator}\ref{asps: plugin-estimator-shape} imply the following property of $(\widehat{\mu}_t)_{t\in\N_0}$ which shows that the procedure in Algorithm~\ref{algo: iteration} is well-defined.


\begin{proposition}[Well-definedness of Algorithm~\ref{algo: iteration}]
	\label{prop: iteration-welldefined}
	Let the inputs of Algorithm~\ref{algo: iteration} satisfy Setting~\ref{sett: algo-iteration} and let $(\Omega,\CF,\PROB)$ be the probability space on which the random samples on Line~\ref{alglin: iteration-sample1} and Line~\ref{alglin: iteration-sample2} of Algorithm~\ref{algo: iteration} are defined.
	Then, it holds $\PROB$-almost surely that $\widehat{\rho}_t\in\CM_{\mathrm{full}}$ and $\widehat{\mu}_t\in\CM$ for all $t\in\N_0$, and thus $\widehat{T}_{t+1,k}$ on Line~\ref{alglin: iteration-estimator} of Algorithm~\ref{algo: iteration} is $\PROB$-almost surely well-defined. 
\end{proposition}

\begin{proof}[Proof of Proposition~\ref{prop: iteration-welldefined}]
	For $t=0$, $\widehat{\rho}_0=\rho_0\in\CM_{\mathrm{full}}$ holds by assumption.
	Since $\support(\widehat{\rho}_0)=\R^d$, it holds by Line~\ref{alglin: iteration-truncation} that $\support(\widehat{\mu}_0)=\support\big(\widehat{\rho}_0|_{\CX_{R_0}}\big)=\CX_{R_0}\in\CS$. 
	Moreover, $f_{\widehat{\mu}_0}:=\frac{f_{\widehat{\rho}_0}\INDI_{\CX_{R_0}}}{\widehat{\rho}_0(\CX_{R_0})}\in\CC^{0,\alpha}(\CX_{R_0})$ is the density function of $\widehat{\mu}_0$, where $f_{\widehat{\rho}_0}\in\CC_{\mathrm{loc}}^{0,\alpha}(\R^d)$ is the density function of $\widehat{\rho}_0$. 
	Since $f_{\widehat{\rho}_0}(\BIx)>0$ $\forall\BIx\in\R^d$, it holds by the compactness of $\CX_{R_0}$ that $0<\inf_{\BIx\in\CX_{R_0}}f_{\widehat{\mu}_0}(\BIx)\le \sup_{\BIx\in\CX_{R_0}}f_{\widehat{\mu}_0}(\BIx) < \infty$ and thus $\widehat{\mu}_0\in\CM$.
	Consequently, for $k=1,\ldots,K$, $\widehat{T}_{1,k}= \widehat{T}^{\mu,m}_{\nu,n}(\,\cdot\,;\theta)\big|_{\mu=\widehat{\mu}_0,\,\nu=\nu_k,\,m=n=N_{0,k},\,\theta=\ITheta_{0,k}}$ is well-defined. 

	Next, let us assume that $\widehat{\rho}_{t}\in\CM_{\mathrm{full}}$ and $\widehat{\mu}_{t}\in\CM$ $\PROB$-almost surely for some $t\in\N_0$. 
	For $k=1,\ldots,K$, since $\widehat{T}_{t+1,k}$ is $\PROB$-almost surely well-defined and satisfies the shape condition in Assumption~\ref{asp: plugin-estimator}\ref{asps: plugin-estimator-shape}, it holds $\PROB$-almost surely that there exists $\widehat{\varphi}_{t+1,k}\in\FC^{2,\alpha_k}_{\underline{\lambda}_k,\overline{\lambda}_k}(\R^d)$ such that $\widehat{T}_{t+1,k}=\nabla \widehat{\varphi}_{t+1,k}$, $\alpha_k\in(0,1]$, $0<\underline{\lambda}_k\le\overline{\lambda}_k<\infty$.
	Subsequently, let us denote $\bar{T}_{t+1}:=\frac{1}{K}\sum_{k=1}^K\widehat{T}_{t+1,k}$ and $\bar{\varphi}_{t+1}:=\frac{1}{K}\sum_{k=1}^K\widehat{\varphi}_{t+1,k}$.
	It thus follows that $\bar{T}_{t+1}=\nabla\bar{\varphi}_{t+1}$ and $\bar{\varphi}_{t+1}\in\FC^{2,\alpha}_{\underline{\lambda},\overline{\lambda}}(\R^d)$ $\PROB$-almost surely for $\alpha:=\min_{1\le k\le K}\{\alpha_k\}\in(0,1]$, $\underline{\lambda}:=\min_{1\le k\le K}\{\underline{\lambda}_k\}>0$, $\overline{\lambda}:=\max_{1\le k\le K}\{\overline{\lambda}_k\}<\infty$. 
	By the well-known duality between smooth convex functions and strongly convex functions (see, e.g., \citep[Proposition~12.60]{rockafellar1998variational}), it holds that the convex conjugate $\bar{\varphi}_{t+1}^*$ of $\bar{\varphi}_{t+1}$ is $\underline{\lambda}^{-1}$-smooth, $\overline{\lambda}^{-1}$-strongly convex and $\nabla\bar{\varphi}_{t+1}^*=\bar{T}_{t+1}^{-1}$.
	This shows that $\bar{T}_{t+1}:\R^d\to\R^d$ is a homeomorphism and $\bar{T}_{t+1}^{-1}$ is $\underline{\lambda}^{-1}$-Lipschitz continuous on $\R^d$.
	Moreover, we have by the second-order characterization of smooth and strongly convex functions (see, e.g., \citep[Theorem~2.1.6]{nesterov2004introconvex}) that $\underline{\lambda}\BI_d\preceq \nabla^2\bar{\varphi}_{t+1}(\BIx)\preceq \overline{\lambda}\BI_d$ for all $\BIx\in\R^d$. 
	Now, since Line~\ref{alglin: iteration-pushforward} sets $\widehat{\rho}_{t+1}:=\bar{T}_{t+1}\sharp\widehat{\rho}_t$, the change of variable formula for pushforward (see, e.g., \citep[Lemma 5.5.3]{ambrosio2005gradient}) yields
	\begin{align}
		\label{eqn: iteration-changeofvariable}
		f_{\widehat{\rho}_{t+1}}(\BIy)=\frac{f_{\widehat{\rho}_t}\big(\bar{T}^{-1}_{t+1}(\BIy)\big)}{\det\big(\nabla^2\bar{\varphi}_{t+1}\big(\bar{T}^{-1}_{t+1}(\BIy)\big)\big)} \qquad\forall \BIy\in\R^d,
	\end{align}
 	where $f_{\widehat{\rho}_{t+1}}$ and $f_{\widehat{\rho}_t}$ denote the density functions of $\widehat{\rho}_{t+1}$ and $\widehat{\rho}_t$, respectively. 
	Since $f_{\widehat{\rho}_t}(\BIx) > 0$ $\forall \BIx\in\R^d$ by assumption and since $\det\big(\nabla^2\bar{\varphi}_{t+1}(\BIx)\big)\ge \underline{\lambda}^d >0$ $\forall \BIx\in\R^d$, it holds that $f_{\widehat{\rho}_{t+1}}(\BIy)>0$ $\forall\BIy\in\R^d$. 
	Let us now show the local H{\"o}lder continuity of $f_{\widehat{\rho}_{t+1}}$. 
	On the one hand, combining the induction hypothesis that $f_{\widehat{\rho}_t}\in\CC_{\mathrm{loc}}^{0,\alpha'}(\R^d)$ for some $\alpha'\in(0,1]$ and the Lipschitz continuity of $\bar{T}_{t+1}^{-1}$ on $\R^d$ shows that the numerator of (\ref{eqn: iteration-changeofvariable}) satisfies $f_{\widehat{\rho}_t}\circ \bar{T}^{-1}_{t+1}\in\CC^{0,\alpha'}_{\mathrm{loc}}(\R^d)$. 
	On the other hand, since $\bar{\varphi}_{t+1}\in\CC_{\mathrm{loc}}^{2,\alpha}(\R^d)$ and $\det(\cdot)$ is a polynomial in all entries of the input matrix, the denominator of (\ref{eqn: iteration-changeofvariable}) satisfies $\det \circ\, \nabla^2\bar{\varphi}_{t+1} \circ \bar{T}^{-1}_{t+1}\in \CC^{0,\alpha''}_{\mathrm{loc}}(\R^d)$ for some $\alpha''\in(0,1]$. 
	Consequently, since the denominator is also bounded from below by $\underline{\lambda}^d>0$, we get $f_{\widehat{\rho}_{t+1}}\in\CC_{\mathrm{loc}}^{0,\widetilde{\alpha}}(\R^d)$ for some $\widetilde{\alpha}\in(0,1]$. 
	Furthermore, we have by the $\PROB$-almost sure $\overline{\lambda}$-Lipschitz continuity of $\bar{T}_{t+1}$ that
	\begin{align*}
		\int_{\R^d}\|\BIy\|^2\DIFFM{\widehat{\rho}_{t+1}}{\DIFF\BIy}=\int_{\R^d}\big\|\bar{T}_{t+1}(\BIx)\big\|^2\DIFFM{\widehat{\rho}_t}{\DIFF\BIx}\le \int_{\R^d}\big(\big\|\bar{T}_{t+1}(\veczero)\big\|+\overline{\lambda}\|\BIx\|\big)^2\DIFFM{\widehat{\rho}_t}{\DIFF\BIx}<\infty \qquad\PROB\text{-a.s.},
	\end{align*}
	and thus $\widehat{\rho}_{t+1}\in\CP_{2,\mathrm{ac}}(\R^d)$.
	Therefore, we have shown that $\widehat{\rho}_{t+1}\in\CM_{\mathrm{full}}$ $\PROB$-almost surely.
	Lastly, it can be shown via the same argument used for the case $t=0$ that $\widehat{\mu}_{t+1}=\widehat{\rho}_{t+1}|_{\CX_{R_{t+1}}}\in\CM$ $\PROB$-almost surely. 

	We conclude by induction that $\widehat{\rho}_{t}\in\CM_{\mathrm{full}}$ and $\widehat{\mu}_{t}\in\CM$ for all $t\in\N_0$ and that Line~\ref{alglin: iteration-estimator} is well-defined for all iterations $\PROB$-almost surely. 
	The proof is now complete. 
\end{proof}

\begin{remark}
	In Algorithm~\ref{algo: iteration}, rather than directly updating $\widehat{\mu}_{t+1}$ to \sloppy{$\widehat{\mu}_{t+1}\leftarrow\Big[\frac{1}{K}\sum_{k=1}^K\widehat{T}_{t+1,k}\Big]\sharp\widehat{\mu}_t$}, we first apply the pushforward of $\widehat{\rho}_t\in\CM_{\mathrm{full}}$ by $\Big[\frac{1}{K}\sum_{k=1}^K\widehat{T}_{t+1,k}\Big]$ in Line~\ref{alglin: iteration-pushforward} to obtain $\widehat{\rho}_{t+1}\in\CM_{\mathrm{full}}$, and then truncate $\widehat{\rho}_{t+1}$ to $\CX_{R_{t+1}}$ to get $\widehat{\mu}_{t+1}$. 
	The truncation step guarantees that $\widehat{\mu}_{t+1}\in\CM$ so that the consistency condition of the plug-in OT map estimator in Assumption~\ref{asp: plugin-estimator}\ref{asps: plugin-estimator-consistency} can be satisfied (see our results and discussions in Section~\ref{sec: concrete-estimators}). 
	Note that the support of the pushforward $\Big[\frac{1}{K}\sum_{k=1}^K\widehat{T}_{t+1,k}\Big]\sharp\widehat{\mu}_t$ is not necessarily an admissible support set in $\CS$; specifically, the uniform convexity condition may fail. 
\end{remark}

Let us now examine the stochastic processes generated by Algorithm~\ref{algo: iteration}. 
To begin, let us consider a probability space $(\Omega,\CF,\PROB)$ on which the random samples on Line~\ref{alglin: iteration-sample1} and Line~\ref{alglin: iteration-sample2} are defined.
Let $\CF_0:=\{\emptyset,\Omega\}$.
$\widehat{\rho}_0:\Omega\to\CM_{\mathrm{full}}$ initialized in Line~\ref{alglin: iteration-initialize} takes a pre-specified value $\rho_0$ and is thus $\CF_0$-measurable.
For each $t=0,1,2,\ldots$, the index $R_t:\Omega\to\N$ in Line~\ref{alglin: iteration-radius} is an $\CF_t$-measurable random variable. 
After $R_t$ has been chosen, $\widehat{\mu}_t:\Omega\to\CM$ is set to $\widehat{\rho}_t|_{\CX_{R_t}}$ in Line~\ref{alglin: iteration-truncation}, which is also $\CF_t$-measurable. 
Next, for $k=1,\ldots,K$, the sample size $N_{t,k}:\Omega\to\N$ and the parameter $\ITheta_{t,k}:\Omega\to\N$ in Line~\ref{alglin: iteration-samplesize} are both $\CF_t$-measurable random variables. 
Subsequently, after $N_{t,k}$ and $\ITheta_{t,k}$ have been chosen, 
$N_{t,k}$ i.i.d.\@ samples $\BIX_{t+1,k,1},\ldots,\BIX_{t+1,k,N_{t,k}}:\Omega\to\R^d$ from $\widehat{\mu}_t$ and $N_{t,k}$ i.i.d.\@ samples $\BIY_{t+1,k,1},\ldots,\BIY_{t+1,k,N_{t,k}}:\Omega\to\R^d$ from $\nu_k$ are randomly generated in Line~\ref{alglin: iteration-sample1} and Line~\ref{alglin: iteration-sample2}. 
We require $\big\{\BIX_{t+1,k,1},\ldots,\BIX_{t+1,k,N_{t,k}},\BIY_{t+1,k,1},\ldots,\BIY_{t+1,k,N_{t,k}}\big\}_{k=1:K}$ to be jointly independent conditional on $\CF_t$. 
Let $\CF_{t+1}$ be the $\sigma$-algebra generated by all the random samples up to the iteration $t+1$, 
i.e., $\CF_{t+1}:=\sigma\left(\bigcup_{0\le s\le t}\big(\{\BIX_{s+1,k,i}\}_{i=1:N_{s,k},\,k=1:K}\cup \{\BIY_{s+1,k,i}\}_{i=1:N_{s,k},\,k=1:K}\big)\right)$.
For $k=1,\ldots,K$, the plug-in OT map estimator $\widehat{T}_{t+1,k}:\Omega\to\CC_{\mathrm{lin}}(\R^d,\R^d)$ in Line~\ref{alglin: iteration-estimator} is thus $\CF_{t+1}$-measurable.
Since $\widehat{\rho}_{t+1}:\Omega\to\CM_{\mathrm{full}}$ in Line~\ref{alglin: iteration-pushforward} depends on $\widehat{\rho}_t$ and $\big(\widehat{T}_{t+1,k}\big)_{k=1:K}$, it is $\CF_{t+1}$-measurable. 
Iteratively repeating the above construction for $t=0,1,2,\ldots$ leads to a filtered probability space with filtration $(\CF_t)_{t\in\N_0}$.
The resulting sequences $(\widehat{\rho}_t)_{t\in\N_0}$ and $(\widehat{\mu}_t)_{t\in\N_0}$ are thus $(\CF_t)_{t\in\N_0}$-adapted stochastic processes. 
In the next subsection, we will specify the choices of $(R_t)_{t\in\N_0}$, $(N_{t,k})_{k=1:K,\,t\in\N_0}$, and $(\ITheta_{t,k})_{k=1:K,\,t\in\N_0}$ (which are $(\CF_t)_{t\in\N_0}$-adapted stochastic processes) in order to achieve $\PROB$-almost sure convergence of the output process $(\widehat{\mu}_t)_{t\in\N_0}$ of Algorithm~\ref{algo: iteration}. 


\begin{remark}[Measurability]
We would like to remark that the above description of the stochastic processes $(\widehat{\rho}_t)_{t\in\N_0}$ and $(\widehat{\mu}_t)_{t\in\N_0}$ has not yet rigorously justified the measurability of all the constructive operations.
The rigorous justification of measurability will be carried out in Appendix~\ref{apx: measurability} with specific choices of the plug-in OT map estimator $\widehat{T}^{\mu,m}_{\nu,n}(\,\cdot\,;\theta)$.
\end{remark}



\subsection{Convergence analysis}
\label{ssec: fixedpoint-convergence}

The goal of this subsection is to develop sufficient conditions for the convergence of the output process $(\widehat{\mu}_t)_{t\in\N_0}$ in Algorithm~\ref{algo: iteration}.
Let us begin by analyzing the decrements of the process $\big(V(\widehat{\mu}_t)\big)_{t\in\N_0}$.
This will subsequently lead to sufficient conditions on the choices of $(R_t)_{t\in\N_0}$, $(N_{t,k})_{k=1:K,\,t\in\N_0}$, and $(\ITheta_{t,k})_{k=1:K,\,t\in\N_0}$ to guarantee the convergence of $(\widehat{\mu}_t)_{t\in\N_0}$.

\begin{proposition}[Decrement of the process $\big(V(\widehat{\mu}_t)\big)_{t\in\N_0}$]
	\label{prop: decrement}
	Let the inputs of Algorithm~\ref{algo: iteration} satisfy Setting~\ref{sett: algo-iteration}, let \sloppy{$\big(\Omega,\CF,\PROB,(\CF_t)_{t\in\N_0}\big)$} be the filtered probability space constructed by Algorithm~\ref{algo: iteration}, and let $(\widehat{\mu}_t)_{t \in \N_0}$ be the output of Algorithm~\ref{algo: iteration}. 
	Moreover, let $V(\cdot)$ be the function defined in (\ref{eqn: V-definition}) and let $G(\cdot)$ be the operator defined in (\ref{eqn: G-operator}). 
	Then, the sequence $\bigl(V(\widehat{\mu}_t)\bigr)_{t \in \N_0}$ satisfies
	\begin{align}
		\label{eqn: decrement-pathwise}
			\begin{split}
			V(\widehat{\mu}_{t + 1}) -	V(\widehat{\mu}_t) &\leq - \CW_2\bigl(\widehat{\mu}_t, G(\widehat{\mu}_t) \bigr)^2  + \frac{2}{K} \sum_{k = 1}^{K}  \big\|\widehat{T}_{t+1,k} - T^{\widehat{\mu}_t}_{\nu_k} \big\|^2_{\CL^2(\widehat{\mu}_t)} \\
			&\qquad + 2 \CW_2\big(\big[{\textstyle\frac{1}{K}\sum_{k=1}^K}\widehat{T}_{t+1,k}\big]\sharp\widehat{\mu}_{t}, \widehat{\mu}_{t + 1}\big)^2 \hspace{54pt}\qquad\qquad\forall t\in\N_0,\;\PROB\text{-a.s.}
			\end{split}
	\end{align}
	In particular, taking expectations on both sides of (\ref{eqn: decrement-pathwise}) conditional on $\CF_t$ yields
	\begin{align}
		\label{eqn: decrement-condexp}
		\begin{split}
			\EXP\big[V(\widehat{\mu}_{t + 1}) \big| \CF_t \big] -	V(\widehat{\mu}_t) &\leq - \CW_2 \bigl(\widehat{\mu}_t, G(\widehat{\mu}_t) \bigr)^2  + \frac{2}{K} \sum_{k = 1}^{K} \EXP \Big[ \big\|\widehat{T}_{t+1,k} - T^{\widehat{\mu}_t}_{\nu_k} \big\|^2_{\CL^2(\widehat{\mu}_t)} \Big | \CF_t\Big] \\
			&\qquad + 2 \EXP \Big[\CW_2\big(\big[{\textstyle\frac{1}{K}\sum_{k=1}^K}\widehat{T}_{t+1,k}\big]\sharp\widehat{\mu}_{t}, \widehat{\mu}_{t + 1}\big)^2 \Big| \CF_t\Big] \qquad\quad \forall t\in\N_0,\;\PROB\text{-a.s.}
		\end{split}
	\end{align}
\end{proposition}

\begin{proof}[Proof of Proposition~\ref{prop: decrement}]
	Throughout this proof, let us fix an arbitrary $t\in\N_0$, denote $\bar{T}^{\widehat{\mu}_t}:=\frac{1}{K}\sum_{k=1}^KT^{\widehat{\mu}_t}_{\nu_k}$, $\bar{T}_{t+1}:=\frac{1}{K}\sum_{k=1}^K\widehat{T}_{t+1,k}$, and denote $\widetilde{\mu}_{t+1}:=\bar{T}_{t+1}\sharp\widehat{\mu}_t$. 
	As we have shown in the proof of Proposition~\ref{prop: iteration-welldefined}, $\bar{T}_{t+1}:\R^d\to\R^d$ is a homeomorphism $\PROB$-almost surely. 
	Subsequently, the change of variable formula for pushforward (see, e.g., \citep[Lemma 5.5.3]{ambrosio2005gradient}) implies that $\widetilde{\mu}_{t+1}\in\CP_{2,\mathrm{ac}}(\R^d)$ $\PROB$-almost surely.  
	Let $T^{\widetilde{\mu}_{t+1}}_{\widehat{\mu}_{t+1}}:\R^d\to\R^d$ denotes the OT map from $\widetilde{\mu}_{t+1}$ to $\widehat{\mu}_{t+1}$. 
	In the remainder of this proof, all statements hold in $\PROB$-almost sure sense, and we will omit ``$\PROB$-a.s.\@'' for simplicity. 

	Our proof below uses the following identity, which can be verified directly by expanding both sides:
	\begin{align}
		\label{eqn: decrement-proof-identity}
		\begin{split}
			\frac{1}{K} \sum_{k= 1}^{K} \big\| \BIy - \BIz_k \big\|^2 &= \| \BIy - \bar{\BIz}\|^2 + \frac{1}{K}\sum_{k = 1}^{K} \| \bar{\BIz} - \BIz_k \|^2 \\
			& \qquad \qquad \text{where }\bar{\BIz}:=\frac{1}{K}\sum_{k=1}^K\BIz_k \qquad \forall \BIy,\BIz_1,\ldots,\BIz_k\in\R^d.
		\end{split}
	\end{align}
	For any $\BIx\in\R^d$, substituting $\BIy \leftarrow \BIx$ and $\BIz_k \leftarrow T^{\widehat{\mu}_t}_{\nu_k}(\BIx)$ in \eqref{eqn: decrement-proof-identity} gives us
	\begin{align}
		\label{eqn: decrement-proof-decompose-1}
		\frac{1}{K} \sum_{k = 1}^{K}\big\|\BIx - T_{\nu_k}^{\widehat{\mu}_t}(\BIx) \big\|^2 = \big\|\BIx - \bar{T}^{\widehat{\mu}_t}(\BIx)\big\|^2 + \frac{1}{K} \sum_{k = 1}^{K}  \big\|\bar{T}^{\widehat{\mu}_t}(\BIx) - T_{\nu_k}^{\widehat{\mu}_t}(\BIx) \big\|^2.
	\end{align}
	Moreover, substituting $\BIy \leftarrow T_{\widehat{\mu}_{t + 1}}^{\widetilde{\mu}_{t + 1}} \circ \bar{T}_{t+1}(\BIx)$ and $\BIz_k \leftarrow T_{\nu_k}^{\widehat{\mu}_t}(\BIx)$ in \eqref{eqn: decrement-proof-identity}, we obtain
	\begin{align}
		\label{eqn: decrement-proof-decompose-2}
		\begin{split}
			&\frac{1}{K}\sum_{k = 1}^{K} \Big\| T_{\widehat{\mu}_{t + 1}}^{\widetilde{\mu}_{t + 1}} \circ \bar{T}_{t+1}(\BIx) - T_{\nu_k}^{\widehat{\mu}_t}(\BIx) \Big\|^2 \\
			&\qquad= \Big\|T_{\widehat{\mu}_{t + 1}}^{\widetilde{\mu}_{t + 1}} \circ \bar{T}_{t+1}(\BIx) - \overline{T}^{\widehat{\mu}_t}(\BIx)\Big\|^2 + \frac{1}{K}\sum_{k = 1}^{K} \big\|\overline{T}^{\widehat{\mu}_t}(\BIx) - T_{\nu_k}^{\widehat{\mu}_t}(\BIx) \big\|^2.
		\end{split}
	\end{align}
	Combining (\ref{eqn: decrement-proof-decompose-1}) and (\ref{eqn: decrement-proof-decompose-2}) yields
	\begin{align}
	\label{eqn: decrement-proof-decompose-comb}
	\begin{split}
		&\left(\frac{1}{K}\sum_{k = 1}^{K} \Big\| T_{\widehat{\mu}_{t + 1}}^{\widetilde{\mu}_{t + 1}} \circ \bar{T}_{t+1}(\BIx) - T_{\nu_k}^{\widehat{\mu}_t}(\BIx) \Big\|^2\right)- \left(\frac{1}{K} \sum_{k = 1}^{K}\big\|\BIx - T_{\nu_k}^{\widehat{\mu}_t}(\BIx) \big\|^2\right) \\
		& \qquad \qquad = \Big\|T_{\widehat{\mu}_{t + 1}}^{\widetilde{\mu}_{t + 1}} \circ \bar{T}_{t+1}(\BIx) - \overline{T}^{\widehat{\mu}_t}(\BIx)\Big\|^2 - \big\|\BIx - \bar{T}^{\widehat{\mu}_t}(\BIx)\big\|^2\\
		& \qquad \qquad \le 2\Big\|T_{\widehat{\mu}_{t + 1}}^{\widetilde{\mu}_{t + 1}} \circ \bar{T}_{t+1}(\BIx) - \overline{T}_{t+1}(\BIx)\Big\|^2 + 2\big\|\overline{T}_{t+1}(\BIx) - \overline{T}^{\widehat{\mu}_t}(\BIx)\big\|^2 - \big\|\BIx - \bar{T}^{\widehat{\mu}_t}(\BIx)\big\|^2 \\
		& \hspace{404pt} \forall\BIx\in\R^d. 
	\end{split}
	\end{align}

	In the following, let us examine the integral of each term in (\ref{eqn: decrement-proof-decompose-comb}) with respect to $\widehat{\mu}_t$.
	Firstly, for $k=1,\ldots,K$, let $\pi_k:=\Big[T_{\widehat{\mu}_{t + 1}}^{\widetilde{\mu}_{t + 1}} \circ \bar{T}_{t+1}, T_{\nu_k}^{\widehat{\mu}_t}\Big]\sharp\widehat{\mu}_t\in\CP(\R^d\times\R^d)$. 
	Since $\Big(T_{\widehat{\mu}_{t + 1}}^{\widetilde{\mu}_{t + 1}} \circ \bar{T}_{t+1}\Big)\sharp\widehat{\mu}_t=T_{\widehat{\mu}_{t + 1}}^{\widetilde{\mu}_{t + 1}}\sharp\widetilde{\mu}_{t+1}=\widehat{\mu}_{t+1}$ and $T_{\nu_k}^{\widehat{\mu}_t}\sharp\widehat{\mu}_t=\nu_k$, it follows that $\pi_k\in\Pi(\widehat{\mu}_{t+1},\nu_k)$.
	Thus, we get
	\begin{align}
		\label{eqn: decrement-proof-decompose-integral1}
		\begin{split}
			\frac{1}{K}\sum_{k = 1}^{K} \int_{\R^d}\Big\| T_{\widehat{\mu}_{t + 1}}^{\widetilde{\mu}_{t + 1}} \circ \bar{T}_{t+1}(\BIx) - T_{\nu_k}^{\widehat{\mu}_t}(\BIx) \Big\|^2 \DIFFM{\widehat{\mu}_t}{\DIFF\BIx}&=\frac{1}{K}\sum_{k=1}^K \int_{\R^d\times\R^d}\|\BIx_1-\BIx_2\|^2\DIFFM{\pi_k}{\DIFF\BIx_1,\DIFF\BIx_2} \\
			&\ge \frac{1}{K}\sum_{k=1}^K \CW_2(\widehat{\mu}_{t+1},\nu_k)^2 = V(\widehat{\mu}_{t+1}).
		\end{split}
	\end{align}
	Secondly, since $T^{\widehat{\mu}_t}_{\nu_k}$ is the OT map from $\widehat{\mu}_t$ to $\nu_k$ for $k=1,\ldots,K$, we have 
	\begin{align}
		\label{eqn: decrement-proof-decompose-integral2}
		\begin{split}
			\frac{1}{K}\sum_{k = 1}^{K} \int_{\R^d}\big\|\BIx - T_{\nu_k}^{\widehat{\mu}_t}(\BIx) \big\|^2 \DIFFM{\widehat{\mu}_t}{\DIFF\BIx}&= \frac{1}{K}\sum_{k=1}^K \CW_2(\widehat{\mu}_{t},\nu_k)^2 = V(\widehat{\mu}_{t}).
		\end{split}
	\end{align}
	Thirdly, it holds that
	\begin{align}
		\label{eqn: decrement-proof-decompose-integral3}
		\begin{split}
			\int_{\R^d}\Big\|T_{\widehat{\mu}_{t + 1}}^{\widetilde{\mu}_{t + 1}} \circ \bar{T}_{t+1}(\BIx) - \overline{T}_{t+1}(\BIx)\Big\|^2 \DIFFM{\widehat{\mu}_t}{\DIFF\BIx}&= \int_{\R^d}\Big\|T_{\widehat{\mu}_{t + 1}}^{\widetilde{\mu}_{t + 1}}(\BIx) - \BIx\Big\|^2 \DIFFM{\widetilde{\mu}_{t+1}}{\DIFF\BIx}=\CW_2(\widetilde{\mu}_{t+1},\widehat{\mu}_{t+1})^2.
		\end{split}
	\end{align}
	Fourthly, the convexity of $\R^d\ni\BIz\mapsto\|\BIz\|^2\in\R$ together with Jensen's inequality gives
	\begin{align*}
		\big\|\overline{T}_{t+1}(\BIx) - \overline{T}^{\widehat{\mu}_t}(\BIx)\big\|^2&=\left\|\frac{1}{K}\sum_{k=1}^K\big(\widehat{T}_{t+1,k}(\BIx)-T^{\widehat{\mu}_t}_{\nu_k}(\BIx)\big)\right\|^2\le \frac{1}{K}\sum_{k=1}^K\big\|\widehat{T}_{t+1,k}(\BIx)-T^{\widehat{\mu}_t}_{\nu_k}(\BIx)\big\|^2 \quad \forall \BIx\in\R^d,
	\end{align*}
	which results in
	\begin{align}
		\label{eqn: decrement-proof-decompose-integral4}
		\begin{split}
			\int_{\R^d}\big\|\overline{T}_{t+1}(\BIx) - \overline{T}^{\widehat{\mu}_t}(\BIx)\big\|^2 \DIFFM{\widehat{\mu}_t}{\DIFF\BIx}&\le \frac{1}{K}\sum_{k=1}^K \int_{\R^d}\big\|\widehat{T}_{t+1,k}(\BIx)-T^{\widehat{\mu}_t}_{\nu_k}(\BIx)\big\|^2 \DIFFM{\widehat{\mu}_{t}}{\DIFF\BIx}\\
			&=\frac{1}{K}\sum_{k=1}^K \big\|\widehat{T}_{t+1,k}-T^{\widehat{\mu}_t}_{\nu_k}\big\|^2_{\CL^2(\widehat{\mu}_t)}.
		\end{split}
	\end{align}
	Lastly, for $k=1,\ldots,K$, let $\varphi^{\widehat{\mu}_t}_{\nu_k}\in\FC_{\underline{\lambda}_k,\overline{\lambda}_k}(\R^d)$ denote the optimal Brenier potential from $\widehat{\mu}_t$ to $\nu_k$, where $0<\underline{\lambda}_k\le\overline{\lambda}_k<\infty$ (see Lemma~\ref{lem: curvature}). 
	It follows that $\bar{T}^{\widehat{\mu}_t}$ is the gradient of the continuously differentiable convex function $\frac{1}{K}\sum_{k=1}^K\varphi^{\widehat{\mu}_t}_{\nu_k}$. 
	Thus, Brenier's theorem (Theorem~\ref{thm: Brenier}) implies that $\bar{T}^{\widehat{\mu}_t}$ is the OT map from $\widehat{\mu}_t$ to $\bar{T}^{\widehat{\mu}_t}\sharp\widehat{\mu}_t=G(\widehat{\mu}_t)$, resulting in
	\begin{align}
		\label{eqn: decrement-proof-decompose-integral5}
		\begin{split}
			\int_{\R^d}\big\|\BIx - \bar{T}^{\widehat{\mu}_t}(\BIx) \big\|^2 \DIFFM{\widehat{\mu}_t}{\DIFF\BIx}&= \CW_2\big(\widehat{\mu}_t, G(\widehat{\mu}_t)\big)^2.
		\end{split}
	\end{align}
	Now, integrating both sides of (\ref{eqn: decrement-proof-decompose-comb}) with respect to $\widehat{\mu}_t$ and then combining it with (\ref{eqn: decrement-proof-decompose-integral1})--(\ref{eqn: decrement-proof-decompose-integral5}) completes the proof of (\ref{eqn: decrement-pathwise}).
	Finally, taking conditional expectations with respect to $\CF_t$ on both sides of (\ref{eqn: decrement-pathwise}) proves (\ref{eqn: decrement-condexp}). 
	The proof is now complete.
\end{proof}

\begin{remark}
	In \citep[Proposition~3.3]{alvarez2016fixed}, the decrement of the sequence $\big(V(\mu_t)\big)_{t\in\N_0}$ in the deterministic fixed-point iteration $\mu_{t+1}\leftarrow G(\mu_t)$ $\forall t\in\N_0$ is controlled through the inequality:
	\begin{align}
		\label{eqn: decrement-deterministic}
		V(\mu_{t+1})-V(\mu_t)\le -\CW_2(\mu_t,G(\mu_t))^2 \qquad \forall t\in\N_0.
	\end{align}
	Compared to (\ref{eqn: decrement-deterministic}), the stochastic decrement (\ref{eqn: decrement-pathwise}) in Proposition~\ref{prop: decrement} has two additional terms on the right-hand side:
	\begin{itemize}[beginpenalty=10000]
		\item the term $\frac{2}{K} \sum_{k = 1}^{K}  \big\|\widehat{T}_{t+1,k} - T^{\widehat{\mu}_t}_{\nu_k} \big\|^2_{\CL^2(\widehat{\mu}_t)}$ comes from the inexactness when approximating the true OT map $T^{\widehat{\mu}_t}_{\nu_k}$ by the plug-in OT map estimator $\widehat{T}_{t+1,k}$, i.e., from the approximation in Line~\ref{alglin: conceptual-estimator} of Conceptual Algorithm~\ref{algo: conceptual};
		
		\item the term $2 \CW_2\big(\big[{\textstyle\frac{1}{K}\sum_{k=1}^K}\widehat{T}_{t+1,k}\big]\sharp\widehat{\mu}_{t}, \widehat{\mu}_{t + 1}\big)^2$ comes from the inexactness when approximating \sloppy{$\big[{\textstyle\frac{1}{K}\sum_{k=1}^K}\widehat{T}_{t+1,k}\big]\sharp\widehat{\mu}_{t}$} by $\widehat{\mu}_{t + 1}=\Big(\big[{\textstyle\frac{1}{K}\sum_{k=1}^K}\widehat{T}_{t+1,k}\big]\sharp\widehat{\rho}_{t}\Big)\Big|_{\CX_{R_{t+1}}}$, i.e., from the approximation in Line~\ref{alglin: conceptual-update} of Conceptual Algorithm~\ref{algo: conceptual}.
	\end{itemize}
\end{remark}

To guarantee the convergence of $(\widehat{\mu}_t)_{t\in\N_0}$, we aim to control the two error terms $\frac{2}{K} \sum_{k = 1}^{K} \EXP \Big[ \big\|\widehat{T}_{t+1,k} - T^{\widehat{\mu}_t}_{\nu_k} \big\|^2_{\CL^2(\widehat{\mu}_t)} \Big | \CF_t\Big]$ and $2 \EXP \Big[\CW_2\big(\big[{\textstyle\frac{1}{K}\sum_{k=1}^K}\widehat{T}_{t+1,k}\big]\sharp\widehat{\mu}_{t}, \widehat{\mu}_{t + 1}\big)^2 \Big| \CF_t\Big]$ on the right-hand side of (\ref{eqn: decrement-condexp}) to be arbitrarily close to~0. 
Before presenting our concrete setting of Algorithm~\ref{algo: iteration} that guarantees the convergence of $(\widehat{\mu}_t)_{t\in\N_0}$, let us first establish an intermediate result about choosing the truncation set $\CX_{R_t}$ on Line~\ref{alglin: iteration-radius} presented in the lemma below. 

\begin{lemma}[Choice of the truncation set]
	\label{lem: truncation-choice}
	Let $\nu_1,\ldots,\nu_K\in\CM$ and let $\rho\in\CM_{\mathrm{full}}$.
	Moreover, let $G(\cdot)$ be the operator defined in (\ref{eqn: G-operator}), let $(\CX_r)_{r\in\N}$ be a family of increasing sets satisfying Assumption~\ref{asp: family-truncation}, and let $\widehat{T}^{\mu,m}_{\nu,n}(\,\cdot\,;\theta)$ be a plug-in OT map estimator satisfying Assumption~\ref{asp: plugin-estimator}. 
	Then, for any $\epsilon>0$, there exist two numbers $\overline{r}_1(\rho,\epsilon),\overline{r}_2(\rho,\nu_1,\ldots,\nu_K,\epsilon)\in\N$ where $\overline{r}_1(\rho,\epsilon)$ depends on $\rho,\epsilon$ and $\overline{r}_2(\rho,\nu_1,\ldots,\nu_K,\epsilon)$ depends on $\rho,\nu_1,\ldots,\nu_K,\epsilon$ such that the following statements hold.
	\begin{enumerate}[label=(\roman*), beginpenalty=10000]
		\item\label{lems: truncation-choice-approx}
		For all $r\ge \overline{r}_1(\rho,\epsilon)$, $\mu:=\rho|_{\CX_r}$ satisfies $\CW_2(\mu,\rho)^2\le \epsilon$.

		\item\label{lems: truncation-choice-pushforward}
		For all $r\ge \overline{r}_2(\rho,\nu_1,\ldots,\nu_K,\epsilon)$ and for any $m,n,\theta\in\N$, $\mu:=\rho|_{\CX_r}$ satisfies 
		\begin{align*}
			\EXP\Big[\CW_2\big(\widehat{T}^{\mu,m}_{\nu_k,n}(\,\cdot\,;\theta)\sharp\mu,\widehat{T}^{\mu,m}_{\nu_k,n}(\,\cdot\,;\theta)\sharp\rho\big)^2\Big]\le\epsilon \qquad\forall 1\le k\le K.
		\end{align*}
		Moreover, in this case, $\bar{T}:=\frac{1}{K}\sum_{k=1}^K\widehat{T}^{\mu,m}_{\nu_k,n}(\,\cdot\,;\theta)$ satisfies $\EXP\Big[\CW_2\big(\bar{T}\sharp\mu,\bar{T}\sharp\rho\big)^2\Big]\le \epsilon$.

	\end{enumerate}
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem: truncation-choice}]
	Let us first prove statement~\ref{lems: truncation-choice-approx}. 
	For every $r\in\N$, let us define $\invbreve{\mu}_r:=\rho|_{\CX_r}$ and define $\breve{\mu}_r:=\rho|_{\CX_r^c}$, where $\CX_r^c:=\R^d\setminus\CX_r$. 
	Notice that $\rho=\rho(\CX_r)\invbreve{\mu}_r+(1-\rho(\CX_r))\breve{\mu}_r$ for all $r\in\N$. 
	Let $\pi_{r,1}:=[I_d,I_d]\sharp\invbreve{\mu}_r$ where $I_d:\R^d\to\R^d$ denotes the identity mapping on $\R^d$, let $\pi_{r,2}\in\Pi(\invbreve{\mu}_r,\breve{\mu}_r)$ be arbitrary, and let $\pi_r:=\rho(\CX_r)\pi_{r,1}+(1-\rho(\CX_r))\pi_{r,2}\in\CP(\R^d\times\R^d)$. 
	One may check that $\pi_r\in\Pi(\invbreve{\mu}_r,\rho)$ for all $r\in\N$. 
	Subsequently, it holds for all $r\in\N$ that
	\begin{align*}
		\begin{split}
			\CW_2(\invbreve{\mu}_r,\rho)^2 & \leq \int_{\R^d \times \R^d} \|\BIx - \BIy\|^2 \DIFFM{\pi_r}{ \DIFF \BIx, \DIFF \BIy}\\
			&= \rho(\CX_r) \int_{\R^d} \|\BIx - \BIx\|^2 \DIFFM{\invbreve{\mu}_r}{\DIFF \BIx} + (1 - \rho(\CX_{r}))\int_{\R^d \times \R^d} \|\BIx - \BIy\|^2 \DIFFM{\pi_{r,2}}{\DIFF \BIx, \DIFF \BIy} \\
			& \leq (1 - \rho(\CX_{r})) \int_{\R^d \times \R^d} 2\|\BIx\|^2 + 2\|\BIy\|^2 \DIFFM{\pi_{r,2}}{\DIFF \BIx, \DIFF \BIy} \\
			&= (1 - \rho(\CX_r)) \int_{\R^d} 2\|\BIx\|^2 \DIFFM{\invbreve{\mu}_r}{\DIFF \BIx} + (1 - \rho(\CX_r)) \int_{\R^d} 2\|\BIy\|^2 \DIFFM{\breve{\mu}_r}{\DIFF \BIy} \\
			&\le \frac{1 - \rho(\CX_{r})}{\rho(\CX_{r})} \int_{\R^d}2\|\BIx\|^2 \DIFFM{\rho}{\DIFF \BIx} + \int_{\R^d} 2\|\BIy\|^2 \INDI_{\CX_{r}^c}(\BIy)\DIFFM{\rho}{\DIFF \BIy}. 
		\end{split}
	\end{align*}
	Since $\bigcup_{r\in\N_0}\CX_r=\R^d$ and $\rho\in\CP_{2}(\R^d)$ by assumption, it follows from Lebesgue's dominated convergence theorem that
	\begin{align*}
		\limsup_{r\to\infty} \CW_2(\invbreve{\mu}_r,\rho)^2 \le \limsup_{r\to\infty} \frac{1 - \rho(\CX_{r})}{\rho(\CX_{r})} \int_{\R^d}2\|\BIx\|^2 \DIFFM{\rho}{\DIFF \BIx} + \limsup_{r\to\infty} \int_{\R^d} 2\|\BIy\|^2 \INDI_{\CX_{r}^c}(\BIy)\DIFFM{\rho}{\DIFF \BIy} = 0.
	\end{align*}
	Therefore, for any $\epsilon>0$, there exists $\overline{r}_1(\rho,\epsilon)\in\N$ such that $\CW_2(\invbreve{\mu}_r,\rho)^2\le \epsilon$ for all $r\ge \overline{r}_1(\rho,\epsilon)$. This proves statement~\ref{lems: truncation-choice-approx}.

	Next, let us use the growth condition in Assumption~\ref{asp: plugin-estimator}\ref{asps: plugin-estimator-growth} to prove statement~\ref{lems: truncation-choice-pushforward}. 
	For every $r\in\N$, let $\invbreve{\mu}_r$, $\breve{\mu}_r$, $\pi_{r,1}$, $\pi_{r,2}$, and $\pi_{r}$ be defined as in the proof of statement~\ref{lems: truncation-choice-approx}.
	Recall that $\pi_r\in\Pi(\invbreve{\mu}_r,\rho)$. 
	Moreover, let $m,n,\theta\in\N$ be arbitrary and denote $\dot{T}_{k,r}:=\widehat{T}^{\invbreve{\mu}_r,m}_{\nu_k,n}(\,\cdot\,;\theta)$ for $k=1,\ldots,K$, $\dot{T}_r:=\frac{1}{K}\sum_{k=1}^K\widehat{T}^{\invbreve{\mu}_r,m}_{\nu_k,n}(\,\cdot\,;\theta)$ for notational simplicity. 
	Furthermore, for $k=1,\ldots,K$, we denote by $\dot{T}_{k,r}\otimes\dot{T}_{k,r}$ the function $\R^d\times\R^d\ni (\BIx,\BIy)\mapsto \dot{T}_{k,r}\otimes\dot{T}_{k,r}(\BIx,\BIy):=\big(\dot{T}_{k,r}(\BIx),\dot{T}_{k,r}(\BIy)\big)\in\R^d\times\R^d$. 
	In addition, we denote by $\dot{T}_r\otimes\dot{T}_r$ the function $\R^d\times\R^d\ni (\BIx,\BIy)\mapsto \dot{T}_r\otimes\dot{T}_r(\BIx,\BIy):=\big(\dot{T}_{r}(\BIx),\dot{T}_{r}(\BIy)\big)\in\R^d\times\R^d$. 
	It holds that $\big[\dot{T}_{k,r}\otimes\dot{T}_{k,r}\big]\sharp\pi_r\in \Pi\big(\dot{T}_{k,r}\sharp\invbreve{\mu}_r,\dot{T}_{k,r}\sharp\rho\big)$ for $k=1,\ldots,K$, and $\big[\dot{T}_r\otimes\dot{T}_r\big]\sharp\pi_r\in \Pi\big(\dot{T}_r\sharp\invbreve{\mu}_r,\dot{T}_r\sharp\rho\big)$. 
	Therefore, for $k=1,\ldots,K$, we are able to bound $\CW_2\big(\dot{T}_{k,r}\sharp\invbreve{\mu}_r, \dot{T}_{k,r}\sharp\rho\big)^2$ by 
	\begin{align}
		\label{eqn: truncation-choice-proof-pushforward-1}
		\begin{split}
			\CW_2\big(\dot{T}_{k,r}\sharp\invbreve{\mu}_r, \dot{T}_{k,r}\sharp\rho\big)^2 &\leq \int_{\R^d \times \R^d} \big\|\BIx - \BIy\big\|^2 \DIFFM{\big[\dot{T}_{k,r}\otimes\dot{T}_{k,r}\big] \sharp \pi_r}{\DIFF \BIx, \DIFF \BIy} \\
			&= \int_{\R^d \times \R^d} \big\|\dot{T}_{k,r}(\BIx) - \dot{T}_{k,r}(\BIy)\big\|^2 \DIFFM{\pi_r}{\DIFF \BIx, \DIFF \BIy} \\
			&= \rho (\CX_{r}) \int_{\R^d} \big\|\dot{T}_{k,r}(\BIx) - \dot{T}_{k,r}(\BIx)\big\|^2 \DIFFM{\invbreve{\mu}_r}{\DIFF \BIx} \\
			&\qquad + (1 - \rho (\CX_{r}))\int_{\R^d \times \R^d} \big\|\dot{T}_{k,r}(\BIx) - \dot{T}_{k,r}(\BIy)\big\|^2 \DIFFM{\pi_{r,2}}{\DIFF \BIx, \DIFF \BIy}\\
			&\leq (1 - \rho (\CX_{r})) \int_{\R^d \times \R^d} 2\big\|\dot{T}_{k,r}(\BIx)\big\|^2 + 2\big\|\dot{T}_{k,r}(\BIy)\big\|^2 \DIFFM{\pi_{r,2}}{\DIFF \BIx, \DIFF \BIy} \\
			&= (1 - \rho(\CX_{r})) \int_{\R^d} 2\big\|\dot{T}_{k,r}(\BIx)\big\|^2 \DIFFM{\invbreve{\mu}_r}{\DIFF \BIx} \\
			&\qquad + (1 - \rho(\CX_{r})) \int_{\R^d} 2\big\|\dot{T}_{k,r}(\BIy)\big\|^2 \DIFFM{\breve{\mu}_r}{\DIFF \BIy}\\
			&\leq \frac{1 - \rho(\CX_{r})}{\rho(\CX_{r})} \int_{\R^d} 2\big\|\dot{T}_{k,r}(\BIx)\big\|^2 \DIFFM{\rho}{\DIFF \BIx} + \int_{\R^d} 2\big\|\dot{T}_{k,r}(\BIy)\big\|^2 \INDI_{\CX_{r}^c}(\BIy)\DIFFM{\rho}{\DIFF \BIy}.
		\end{split}
	\end{align}
	For $k=1,\ldots,K$, observe that the growth condition of $\dot{T}_{k,r}$ guarantees $\EXP\Big[\big\|\dot{T}_{k,r}(\BIx)\big\|^2\Big]\le u_1(\nu_k)+u_2(\nu_k)\|\BIx\|^2$ for all $\BIx\in\R^d$, where $u_1(\nu_k)\in\R_+$ and $u_2(\nu_k)\in\R_+$ are constants that only depend on $\nu_k$. 
	Let $\overline{u}_1:=\max_{1\le k\le K}\big\{u_1(\nu_k)\big\}$ and $\overline{u}_2:=\max_{1\le k\le K}\big\{u_2(\nu_k)\big\}$.
	It thus holds that
	\begin{align}
		\label{eqn: truncation-choice-proof-pushforward-2}
		\begin{split}
			\EXP\Big[\big\|\dot{T}_{k,r}(\BIx)\big\|^2\Big]\le \overline{u}_1+\overline{u}_2\|\BIx\|^2 \qquad \forall \BIx\in\R^d,\;\forall 1\le k\le K.
		\end{split}
	\end{align}
	Taking expectations on both sides of (\ref{eqn: truncation-choice-proof-pushforward-1}) then applying Fubini's theorem and (\ref{eqn: truncation-choice-proof-pushforward-2}) yields
	\begin{align*}
		\EXP\Big[\CW_2\big(\dot{T}_{k,r}\sharp\invbreve{\mu}_r, \dot{T}_{k,r}\sharp\rho\big)^2\Big] &\le \frac{1 - \rho(\CX_{r})}{\rho(\CX_{r})} \int_{\R^d} 2\EXP\Big[\big\|\dot{T}_{k,r}(\BIx)\big\|^2\Big] \DIFFM{\rho}{\DIFF \BIx} + \int_{\R^d} 2\EXP\Big[\big\|\dot{T}_{k,r}(\BIy)\big\|^2\Big] \INDI_{\CX_{r}^c}(\BIy)\DIFFM{\rho}{\DIFF \BIy}\\
		&\le \frac{1 - \rho(\CX_{r})}{\rho(\CX_{r})} \int_{\R^d} 2\big(\overline{u}_1+\overline{u}_2\|\BIx\|^2\big) \DIFFM{\rho}{\DIFF \BIx} + \int_{\R^d} 2\big(\overline{u}_1+\overline{u}_2\|\BIy\|^2\big) \INDI_{\CX_{r}^c}(\BIy)\DIFFM{\rho}{\DIFF \BIy} \\
		&\hspace{304pt}\forall 1\le k\le K.
	\end{align*}
	Same as in the proof of statement~\ref{lems: truncation-choice-approx}, since $\bigcup_{r\in\N_0}\CX_r=\R^d$ and $\rho\in\CP_{2}(\R^d)$ by assumption, it follows from Lebesgue's dominated convergence theorem that
	\begin{align*}
		\limsup_{r\to\infty}\EXP\Big[\CW_2\big(\dot{T}_{k,r}\sharp\invbreve{\mu}_r, \dot{T}_{k,r}\sharp\rho\big)^2\Big] &\le \limsup_{r\to\infty} \frac{1 - \rho(\CX_{r})}{\rho(\CX_{r})} \int_{\R^d} 2\big(\overline{u}_1+\overline{u}_2\|\BIx\|^2\big) \DIFFM{\rho}{\DIFF \BIx} \\
		& \qquad + \limsup_{r\to\infty} \int_{\R^d} 2\big(\overline{u}_1+\overline{u}_2\|\BIy\|^2\big) \INDI_{\CX_{r}^c}(\BIy)\DIFFM{\rho}{\DIFF \BIy}\\
		& = 0 \hspace{180pt}\forall 1\le k\le K.
	\end{align*}
	Therefore, for any $\epsilon>0$, there exists $\overline{r}_2(\rho,\nu_1,\ldots,\nu_K,\epsilon)\in\N$ such that for any $m,n,\theta\in\N$, the inequality \sloppy{$\EXP\Big[\CW_2\big(\dot{T}_{k,r}\sharp\invbreve{\mu}_r, \dot{T}_{k,r}\sharp\rho\big)^2\Big]\le \epsilon$} holds for all $k=1,\ldots,K$ and all $r\ge \overline{r}_2(\rho,\nu_1,\ldots,\nu_K,\epsilon)$. 
	Furthermore, repeating the same derivation in (\ref{eqn: truncation-choice-proof-pushforward-1}) with $\dot{T}_{k,r}$ replaced by $\dot{T}_r$ yields
	\begin{align}
		\label{eqn: truncation-choice-proof-pushforward-sum-1}
		\begin{split}
			\CW_2\big(\dot{T}_{r}\sharp\invbreve{\mu}_r, \dot{T}_{r}\sharp\rho\big)^2 &\leq \frac{1 - \rho(\CX_{r})}{\rho(\CX_{r})} \int_{\R^d} 2\big\|\dot{T}_{r}(\BIx)\big\|^2 \DIFFM{\rho}{\DIFF \BIx} + \int_{\R^d} 2\big\|\dot{T}_{r}(\BIy)\big\|^2 \INDI_{\CX_{r}^c}(\BIy)\DIFFM{\rho}{\DIFF \BIy}.
		\end{split}
	\end{align}
	Since the convexity of $\R^d\ni\BIz\mapsto\|\BIz\|^2\in\R$ and Jensen's inequality imply
	\begin{align}
		\label{eqn: truncation-choice-proof-pushforward-sum-2}
		\begin{split}
		\EXP\Big[\big\|\dot{T}_r(\BIx)\big\|^2\Big]&\le \frac{1}{K}\sum_{k=1}^K \EXP\Big[\big\|\dot{T}_{k,r}(\BIx;\theta)\big\|^2\Big] \le \overline{u}_1 + \overline{u}_2\|\BIx\|^2 \qquad\forall \BIx\in\R^d,
		\end{split}
	\end{align}
	taking expectations on both sides of (\ref{eqn: truncation-choice-proof-pushforward-sum-1}) then applying Fubini's theorem and (\ref{eqn: truncation-choice-proof-pushforward-sum-2}) leads to
	\begin{align*}
		\EXP\Big[\CW_2\big(\dot{T}_r\sharp\invbreve{\mu}_r, \dot{T}_r\sharp\rho\big)^2\Big] &\le \frac{1 - \rho(\CX_{r})}{\rho(\CX_{r})} \int_{\R^d} 2\big(\overline{u}_1+\overline{u}_2\|\BIx\|^2\big) \DIFFM{\rho}{\DIFF \BIx} + \int_{\R^d} 2\big(\overline{u}_1+\overline{u}_2\|\BIy\|^2\big) \INDI_{\CX_{r}^c}(\BIy)\DIFFM{\rho}{\DIFF \BIy}.
	\end{align*}
	Consequently, it follows from the same argument as above that $\EXP\Big[\CW_2\big(\dot{T}_r\sharp\invbreve{\mu}_r, \dot{T}_r\sharp\rho\big)^2\Big]\le \epsilon$ holds for all $r\ge \overline{r}_2(\rho,\nu_1,\ldots,\nu_K,\epsilon)$. 
	The proof is now complete.
\end{proof}

The results in Proposition~\ref{prop: decrement} and Lemma~\ref{lem: truncation-choice} suggest the following sufficient conditions for the convergence of Algorithm~\ref{algo: iteration}.

\begin{setting}[Convergence conditions for Algorithm~\ref{algo: iteration}]
	\label{sett: convergence}
	Let $\beta>0$.
	In addition to Setting~\ref{sett: algo-iteration}, let the $(\CF_t)_{t\in\N_0}$-adapted stochastic processes $(R_t)_{t\in\N_0}$, $(N_{t,k})_{t\in\N_0,\,k=1:K}$, and $(\ITheta_{t,k})_{t\in\N_0,\,k=1:K}$ in Algorithm~\ref{algo: iteration} be chosen as follows. 
	\begin{enumerate}[label=(\alph*), beginpenalty=10000]
		\item For every $t\in\N_0$, let $R_t$ be set as follows:
		\begin{align*}
			\hspace{23pt}R_0&:=\overline{r}_2(\widehat{\rho}_0,\nu_1,\ldots,\nu_K,1),\\
			R_t&:=\max\big\{\overline{r}_1\big(\widehat{\rho}_{t},t^{-(1+\beta)}\big), \overline{r}_2\big(\widehat{\rho}_t,\nu_1,\ldots,\nu_K,(t+1)^{-2(1+\beta)}\big)\big\} \qquad \forall t\ge 1,
		\end{align*}
		where \sloppy{$\overline{r}_1(\,\cdot\,,\cdot\,)$} and $\overline{r}_2(\,\cdot\,,\ldots,\cdot\,)$ are given by Lemma~\ref{lem: truncation-choice}. 
		Note that $R_t$ is $\CF_t$-measurable for all $t\in\N_0$.

		\item For every $t\in\N_0$ and for $k=1,\ldots,K$, let $N_{t,k}:=\overline{n}\big(\widehat{\mu}_t,\nu_k,(t+1)^{-2(1+\beta)}\big)$, $\ITheta_{t,k}:=\overline{\theta}\big(\widehat{\mu}_t,\nu_k, N_{t,k}, N_{t,k},(t+1)^{-2(1+\beta)}\big)$, where $\overline{n}(\,\cdot\,,\,\cdot\,,\,\cdot\,)$ and $\overline{\theta}(\,\cdot\,,\,\cdot\,,\,\cdot\,,\,\cdot\,,\,\cdot\,)$ are given by Assumption~\ref{asp: plugin-estimator}\ref{asps: plugin-estimator-consistency}. 
		Note that $N_{t,k}$ and $\ITheta_{t,k}$ are $\CF_t$-measurable for all $t\in\N_0$. 
	\end{enumerate}
\end{setting}

We are now ready to present our main convergence result.

\begin{theorem}[Convergence of Algorithm~\ref{algo: iteration}]
	\label{thm: fixedpoint-convergence}
	Let the inputs of Algorithm~\ref{algo: iteration} satisfy Setting~\ref{sett: algo-iteration} and let \sloppy{$\big(\Omega,\CF,\PROB,(\CF_t)_{t\in\N_0}\big)$} be the filtered probability space constructed by Algorithm~\ref{algo: iteration}. 
	Let the $(\CF_t)_{t\in\N_0}$-adapted stochastic processes $(R_t)_{t\in\N_0}$, $(N_{t,k})_{t\in\N_0,\,k=1:K}$, and $(\ITheta_{t,k})_{t\in\N_0,\,k=1:K}$ in Algorithm~\ref{algo: iteration} be specified by Setting~\ref{sett: convergence}, and let $(\widehat{\mu}_t)_{t\in\N_0}$ be the output of Algorithm~\ref{algo: iteration}.
	Then, the following statements hold.
	\begin{enumerate}[label=(\roman*), beginpenalty=10000]
		\item\label{thms: fixedpoint-convergence-convergence}
		It holds $\PROB$-almost surely that $(\widehat{\mu}_t)_{t\in\N_0}$ is a tight sequence of probability measures, and that every accumulation point of $(\widehat{\mu}_t)_{t\in\N_0}$ with respect to the $\CW_2$ metric is a fixed-point of $G$. 

		\item\label{thms: fixedpoint-convergence-barycenter}
		In particular, if $G$ has a unique fixed-point, then $(\widehat{\mu}_t)_{t\in\N_0}$ converges $\PROB$-almost surely in $\CW_2$ to the Wasserstein barycenter of $\nu_1,\ldots,\nu_K$.
	\end{enumerate}
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm: fixedpoint-convergence}]
	Let us denote $\bar{T}_{t+1}:=\frac{1}{K}\sum_{k=1}^K\widehat{T}_{t+1,k}$.
	Recall that $\widehat{\rho}_{t+1}:=\bar{T}_{t+1}\sharp\widehat{\rho}_{t}$ by Line~\ref{alglin: iteration-pushforward} of Algorithm~\ref{algo: iteration}.  
	As implied by Setting~\ref{sett: convergence} and the properties of $\overline{r}_1(\,\cdot\,,\cdot\,)$, $\overline{r}_2(\,\cdot\,,\ldots,\cdot\,)$, $\overline{n}(\,\cdot\,,\cdot\,,\cdot\,)$, $\overline{\theta}(\,\cdot\,,\,\cdot\,,\,\cdot\,,\cdot\,,\cdot\,)$ in Lemma~\ref{lem: truncation-choice} and Assumption~\ref{asp: plugin-estimator}\ref{asps: plugin-estimator-consistency}, the following inequalities hold $\PROB$-almost surely: 
	\begin{align}
		\CW_2(\widehat{\mu}_{t+1},\widehat{\rho}_{t+1})^2 &\le (t+1)^{-(1+\beta)} 
		&& \phantom{\forall 1\le k\le K,}\;\,\forall t\in\N_0, \label{eqn: fixedpoint-convergence-proof-approx} \\
		\EXP\Big[\CW_2\big(\widehat{T}_{t+1,k}\sharp\widehat{\mu}_t,\widehat{T}_{t+1,k}\sharp\widehat{\rho}_t\big)^2\Big|\CF_t\Big] &\le (t+1)^{-2(1+\beta)} 
		&& \forall 1\le k\le K, \; \forall t\in\N_0, \label{eqn: fixedpoint-convergence-proof-pushforward-k} \\
		\EXP\Big[\CW_2\big(\bar{T}_{t+1}\sharp\widehat{\mu}_t,\widehat{\rho}_{t+1}\big)^2\Big|\CF_t\Big] &\le (t+1)^{-2(1+\beta)} 
		&& \phantom{\forall 1\le k\le K,}\;\, \forall t\in\N_0, \label{eqn: fixedpoint-convergence-proof-pushforward} \\
		\EXP\Big[\big\|\widehat{T}_{t+1,k}-T^{\widehat{\mu}_t}_{\nu_k}\big\|_{\CL^2(\widehat{\mu}_t)}^2\Big|\CF_t\Big] &\le (t+1)^{-2(1+\beta)} 
		&& \forall 1\le k\le K, \;  \forall t\in\N_0, \label{eqn: fixedpoint-convergence-proof-estimator}
	\end{align}
	where $\beta>0$.
	The proof of statement~\ref{thms: fixedpoint-convergence-convergence} is divided into five steps. 

	\emph{\underline{Step~1}: showing that $\widehat{T}_{t+1,k}\sharp\widehat{\rho}_t\overset{\CW_2}{\underset{t\to\infty}{\longrightarrow}}\nu_k$ $\PROB$-almost surely for $k=1,\ldots,K$.} 
	Notice that, for each $t\in\N_0$ and for $k=1,\ldots,K$, it holds that $\big[\widehat{T}_{t+1,k},T^{\widehat{\mu}_t}_{\nu_k}\big]\sharp\widehat{\mu}_t\in\Pi\big(\widehat{T}_{t+1,k}\sharp\widehat{\mu}_t,\nu_k\big)$. 
	Thus, we have
	\begin{align*}
		\begin{split}
		\CW_2\big(\widehat{T}_{t+1,k}\sharp\widehat{\rho}_t,\nu_k\big)^2 &\le \Big(\CW_2\big(\widehat{T}_{t+1,k}\sharp\widehat{\mu}_t, \widehat{T}_{t+1,k}\sharp\widehat{\rho}_t\big)+\CW_2\big(\widehat{T}_{t+1,k}\sharp\widehat{\mu}_t,\nu_k\big)\Big)^2\\
		&\le 2\CW_2\big(\widehat{T}_{t+1,k}\sharp\widehat{\mu}_t, \widehat{T}_{t+1,k}\sharp\widehat{\rho}_t\big)^2 + 2\CW_2\big(\widehat{T}_{t+1,k}\sharp\widehat{\mu}_t,\nu_k\big)^2\\
		&\le 2\CW_2\big(\widehat{T}_{t+1,k}\sharp\widehat{\mu}_t, \widehat{T}_{t+1,k}\sharp\widehat{\rho}_t\big)^2 + 2\int_{\R^d}\big\|\widehat{T}_{t+1,k}(\BIx)-T^{\widehat{\mu}_t}_{\nu_k}(\BIx)\big\|^2\DIFFM{\widehat{\mu}_t}{\BIx}\\
		&= 2\CW_2\big(\widehat{T}_{t+1,k}\sharp\widehat{\mu}_t, \widehat{T}_{t+1,k}\sharp\widehat{\rho}_t\big)^2 + 2\big\|\widehat{T}_{t+1,k}-T^{\widehat{\mu}_t}_{\nu_k}\big\|^2_{\CL^2(\widehat{\mu}_t)}.
		\end{split}
	\end{align*}
	Taking expectations on both sides conditional on $\CF_t$ and then applying (\ref{eqn: fixedpoint-convergence-proof-pushforward-k}) and (\ref{eqn: fixedpoint-convergence-proof-estimator}) yields
	\begin{align*}
		\EXP\Big[\CW_2\big(\widehat{T}_{t+1,k}\sharp\widehat{\rho}_t,\nu_k\big)^2\Big|\CF_t\Big]\le 4(t+1)^{-2(1+\beta)} \qquad\forall 1\le k\le K,\;\forall t\in\N_0. 
	\end{align*}
	Applying the law of total expectation and Markov's inequality then gives
	\begin{align*}
		\begin{split}
		\PROB\Big[\CW_2\big(\widehat{T}_{t+1,k}\sharp\widehat{\rho}_t,\nu_k\big)^2\ge (t+1)^{-(1+\beta)}\Big] \le (t+1)^{1+\beta}\EXP\Big[\CW_2\big(\widehat{T}_{t+1,k}\sharp\widehat{\rho}_t,\nu_k\big)^2\Big]&\le 4(t+1)^{-(1+\beta)} \\
		& \hspace{10pt} \forall 1\le k\le K,\;\forall t\in\N_0.
		\end{split}
	\end{align*}
	Subsequently, since $\sum_{t\in\N_0}4(t+1)^{-(1+\beta)}<\infty$, we conclude by the Borel--Cantelli lemma that, $\PROB$-almost surely, $\CW_2\big(\widehat{T}_{t+1,k}\sharp\widehat{\rho}_t,\nu_k\big)^2\le (t+1)^{-(1+\beta)}$ holds for all but finitely many $t\in\N_0$, and therefore \sloppy{$\lim_{t\to\infty}\CW_2\big(\widehat{T}_{t+1,k}\sharp\widehat{\rho}_t,\nu_k\big)=0$} $\PROB$-almost surely for $k=1,\ldots,K$. 

	\emph{\underline{Step~2}: showing that $(\widehat{\rho}_t)_{t\in\N_0}$ is tight $\PROB$-almost surely.}
	By Prokhorov's theorem, for $k=1,\ldots,K$, $\big(\widehat{T}_{t+1,k}\sharp\widehat{\rho}_t\big)_{t\in\N_0}$ is $\PROB$-almost surely a tight sequence of probability measures since it is $\PROB$-almost surely convergent. 
	Let $\eta_t:=\big[\widehat{T}_{t+1,1},\ldots,\widehat{T}_{t+1,K}\big]\sharp\widehat{\rho}_t\in\CP(\underbrace{\R^d\times\cdots\times\R^d}_{K\text{ copies}})$ for $t\in\N_0$. 
	It hence holds $\PROB$-almost surely that each marginal of each probability measure in $(\eta_t)_{t\in\N_0}$ (on each copy of $\R^d$) belongs to a tight set of probability measures on $\R^d$, and it thus follows from a multi-marginal generalization of \citep[Lemma~4.4]{villani2009optimal} that $(\eta_t)_{t\in\N_0}$ is a tight set of probability measures on $(\R^d)^K$.
	Let $A$ denote the mapping $(\R^d)^K\ni(\BIx_1,\ldots,\BIx_K)\mapsto \frac{1}{K}\sum_{k=1}^K\BIx_k\in\R^d$.
	Hence, we have $\widehat{\rho}_{t+1}=\bar{T}_{t+1}\sharp\widehat{\rho}_t=A\sharp\eta_t$ for all $t\in\N_0$. 
	Consequently, the tightness of $(\eta_t)_{t\in\N_0}$ and the continuity of the mapping $A$ imply the tightness of $(\widehat{\rho}_t)_{t\in\N_0}$ in the $\PROB$-almost sure sense. 

	\emph{\underline{Step~3}: showing that $(\widehat{\mu}_t)_{t\in\N_0}$ is tight $\PROB$-almost surely.}
	It follows from (\ref{eqn: fixedpoint-convergence-proof-approx}) that $\lim_{t\to\infty}\CW_2(\widehat{\mu}_t,\widehat{\rho}_t){=0}$ $\PROB$-almost surely, and hence $(\widehat{\mu}_t)_{t\in\N_0}$ is $\PROB$-almost surely sequentially precompact due to the $\PROB$-almost sure tightness of $(\widehat{\rho}_t)_{t\in\N_0}$ and Prokhorov's theorem. 
	Applying Prokhorov's theorem again then yields the $\PROB$-almost sure tightness of $(\widehat{\mu}_t)_{t\in\N_0}$.

	\emph{\underline{Step~4}: constructing a subset $\widetilde{\Omega}\subseteq\Omega$ with $\PROB[\widetilde{\Omega}]=1$ in which the convergence is analyzed.}
	Similar to the argument used in Step~1, applications of the law of total expectation together with Markov's inequality to (\ref{eqn: fixedpoint-convergence-proof-pushforward}) and (\ref{eqn: fixedpoint-convergence-proof-estimator}) lead to
	\begin{align*}
		\PROB\Big[\CW_2\big(\bar{T}_{t+1}\sharp\widehat{\mu}_t,\widehat{\rho}_{t+1}\big)^2\ge (t+1)^{-(1+\beta)}\Big] &\le (t+1)^{-(1+\beta)} && \phantom{\forall 1\le k\le K,}\,\;\forall t\in\N_0, \\
		\PROB\Big[\big\|\widehat{T}_{t+1,k}-T^{\widehat{\mu}_t}_{\nu_k}\big\|_{\CL^2(\widehat{\mu}_t)}^2 \ge (t+1)^{-(1+\beta)}\Big] &\le (t+1)^{-(1+\beta)} && \forall 1\le k\le K,\;\forall t\in\N_0.
	\end{align*}
	Since $\sum_{t\in\N_0}(t+1)^{-(1+\beta)}<\infty$, we use the Borel--Cantelli lemma again to show that, $\PROB$-almost surely, $\CW_2\big(\bar{T}_{t+1}\sharp\widehat{\mu}_t,\widehat{\rho}_{t+1}\big)^2\le (t+1)^{-(1+\beta)}$ and $\big\|\widehat{T}_{t+1,k}-T^{\widehat{\mu}_t}_{\nu_k}\big\|_{\CL^2(\widehat{\mu}_t)}^2\le (t+1)^{-(1+\beta)}$ $\forall 1\le k\le K$ hold for all but finitely many $t\in\N_0$.
	In the following, for every $\omega\in\Omega$, let us use the notations $\widehat{\rho}_t^{(\omega)}$, $\widehat{\mu}_t^{(\omega)}$, $\widehat{T}_{t+1,k}^{(\omega)}$, $\bar{T}_{t+1}^{(\omega)}$ to explicitly express the dependence of $\widehat{\rho}_t$, $\widehat{\mu}_t$, $\widehat{T}_{t+1,k}$, $\bar{T}_{t+1}$ on~$\omega$. 
	The above analyses have shown the existence of an $\CF$-measurable set $\widetilde{\Omega}\subseteq\Omega$ with $\PROB[\widetilde{\Omega}]=1$, which satisfies:
	\begin{align}
		\label{eqn: fixedpoint-convergence-proof-subsetOmega}
		\forall \omega\in\widetilde{\Omega},\;\exists \overline{t}^{(\omega)}\in\N,\; \begin{cases}
			\big(\widehat{\mu}_{t}^{(\omega)}\big)_{t\in\N_0} \text{ is tight}, \\
			\CW_2\big(\widehat{\mu}_{t+1}^{(\omega)},\widehat{\rho}_{t+1}^{(\omega)}\big)^2 \le (t+1)^{-(1+\beta)} & \forall t\ge \overline{t}^{(\omega)}, \\
			\CW_2\big(\bar{T}_{t+1}^{(\omega)}\sharp\widehat{\mu}_t^{(\omega)},\widehat{\rho}_{t+1}^{(\omega)}\big)^2 \le (t+1)^{-(1+\beta)} & \forall t\ge \overline{t}^{(\omega)}, \\
			\Big\|\widehat{T}_{t+1,k}^{(\omega)}-T^{\widehat{\mu}_t^{(\omega)}}_{\nu_k}\Big\|^2_{\CL^2\big(\widehat{\mu}_t^{(\omega)}\big)} \le (t+1)^{-(1+\beta)} & \forall t\ge \overline{t}^{(\omega)},\;\forall 1\le k\le K.
		\end{cases}
	\end{align}

	\emph{\underline{Step~5}: showing that for every $\omega\in\widetilde{\Omega}$, every accumulation point of $\big(\widehat{\mu}_t^{(\omega)}\big)_{t\in\N_0}$ is a fixed-point of $G$.}
	Let us fix an arbitrary $\omega\in\widetilde{\Omega}$ and suppose there is a subsequence $(t_i)_{i\in\N_0}$ such that $\widehat{\mu}_{t_i}^{(\omega)}\overset{\CW_2}{\underset{i\to\infty}{\longrightarrow}}\widehat{\mu}_{\infty}^{(\omega)}\in\CP_2(\R^d)$.
	The continuity of $V(\cdot)$ on $\CP_2(\R^d)$ then implies that $\lim_{i\to\infty}V\big(\widehat{\mu}_{t_i}^{(\omega)}\big)=V\big(\widehat{\mu}_{\infty}^{(\omega)}\big)$.
	Removing finitely many initial terms from $(t_i)_{i\in\N_0}$ if necessary, we assume without loss of generality that $t_0\ge \overline{t}^{(\omega)}$. 
	For each $i\in\N_0$, summing (\ref{eqn: decrement-pathwise}) over $s=t_i,t_i+1,\ldots,t_{i+1}-1$, using the inequality $\CW_2\big(\bar{T}_{s+1}^{(\omega)}\sharp\widehat{\mu}_{s}^{(\omega)}, \widehat{\mu}_{s + 1}^{(\omega)}\big)^2\le 2 \CW_2\big(\bar{T}_{s+1}^{(\omega)}\sharp\widehat{\mu}_{s}^{(\omega)}, \widehat{\rho}_{s + 1}^{(\omega)}\big)^2 + 2 \CW_2\big(\widehat{\mu}_{s + 1}^{(\omega)}, \widehat{\rho}_{s + 1}^{(\omega)}\big)^2$, and using the properties in (\ref{eqn: fixedpoint-convergence-proof-subsetOmega}) show that
	\begin{align*}
		V\big(\widehat{\mu}_{t_{i+1}}^{(\omega)}\big)-V\big(\widehat{\mu}_{t_i}^{(\omega)}\big) &=\sum_{s=t_i}^{t_{i+1}-1} V\big(\widehat{\mu}_{s + 1}^{(\omega)}\big) - V\big(\widehat{\mu}_{s}^{(\omega)}\big) \\
		&\leq - \left(\sum_{s=t_i}^{t_{i+1}-1}\CW_2\Big(\widehat{\mu}_{s}^{(\omega)}, G\big(\widehat{\mu}_s^{(\omega)}\big) \Big)^2\right)  + \left( \sum_{s=t_i}^{t_{i+1}-1}\frac{2}{K} \sum_{k = 1}^{K}  \Big\|\widehat{T}_{s+1,k}^{(\omega)} - T^{\widehat{\mu}_s^{(\omega)}}_{\nu_k} \Big\|^2_{\CL^2\big(\widehat{\mu}_s^{(\omega)}\big)} \right) \\
		&\qquad + \left( \sum_{s=t_i}^{t_{i+1}-1} 4 \CW_2\big(\bar{T}_{s+1}^{(\omega)}\sharp\widehat{\mu}_{s}^{(\omega)}, \widehat{\rho}_{s + 1}^{(\omega)}\big)^2 \right) + \left( \sum_{s=t_i}^{t_{i+1}-1} 4 \CW_2\big(\widehat{\mu}_{s + 1}^{(\omega)}, \widehat{\rho}_{s + 1}^{(\omega)} \big)^2 \right) \\
		&\le - \left(\sum_{s=t_i}^{t_{i+1}-1}\CW_2\Big(\widehat{\mu}_{s}^{(\omega)}, G\big(\widehat{\mu}_s^{(\omega)}\big) \Big)^2\right)  + \left( \sum_{s=t_i}^{t_{i+1}-1} 10(s+1)^{-(1+\beta)}\right)\\
		&\le - \CW_2\Big(\widehat{\mu}_{t_i}^{(\omega)}, G\big(\widehat{\mu}_{t_i}^{(\omega)}\big) \Big)^2  + \left( \sum_{s=t_i}^{\infty} 10(s+1)^{-(1+\beta)}\right) \qquad\qquad\forall i\in\N_0.
	\end{align*}
	Rearranging the terms above leads to
	\begin{align}
		\label{eqn: fixedpoint-convergence-proof-telescoping}
		\CW_2\Big(\widehat{\mu}_{t_i}^{(\omega)}, G\big(\widehat{\mu}_{t_i}^{(\omega)}\big) \Big)^2 \le \Big|V\big(\widehat{\mu}_{t_{i+1}}^{(\omega)}\big)-V\big(\widehat{\mu}_{t_i}^{(\omega)}\big)\Big| + \left( \sum_{s=t_i}^{\infty} 10(s+1)^{1+\beta}\right) \qquad\qquad\forall i\in\N_0.
	\end{align}
	Since $\sum_{s=0}^{\infty}(s+1)^{-(1+\beta)}$ is a convergent series, (\ref{eqn: fixedpoint-convergence-proof-telescoping}) implies that
	\begin{align*}
		\limsup_{i\to\infty}\CW_2\Big(\widehat{\mu}_{t_i}^{(\omega)}, G\big(\widehat{\mu}_{t_i}^{(\omega)}\big) \Big)^2 \le \limsup_{i\to\infty}\Big|V\big(\widehat{\mu}_{t_{i+1}}^{(\omega)}\big)-V\big(\widehat{\mu}_{t_i}^{(\omega)}\big)\Big| + \limsup_{i\to\infty}\left( \sum_{s=t_i}^{\infty} 10(s+1)^{1+\beta}\right)=0.
	\end{align*}
	This shows that $G\big(\widehat{\mu}_{t_i}^{(\omega)}\big)\overset{\CW_2}{\underset{i\to\infty}{\longrightarrow}} \widehat{\mu}_{\infty}^{(\omega)}$.
	Moreover, for any $\mu\in\CP_{2,\mathrm{ac}}(\R^d)$, the analysis in \citep[Remark~3.2]{alvarez2016fixed} demonstrates that the density function $f_{G(\mu)}$ of $G(\mu)\in\CP_{2,\mathrm{ac}}(\R^d)$ satisfies $\sup_{\BIx\in\R^d}\big\{f_{G(\mu)}(\BIx)\big\}\le K^d \sup_{\BIx\in\support(\nu_1)}\big\{f_{\nu_1}(\BIx)\big\}<\infty$, where $f_{\nu_1}$ denotes the density function of $\nu_1$. 
	Consequently, it holds for every open set $E\subseteq\R^d$ that $\widehat{\mu}_{\infty}^{(\omega)}(E)\le \liminf_{i\to\infty} G\big(\widehat{\mu}_{t_i}^{(\omega)}\big)(E)\le K^d \sup_{\BIx\in\support(\nu_1)}\big\{f_{\nu_1}(\BIx)\big\}\lebesgue(E)$, where $\lebesgue$ denotes the Lebesgue measure on $\R^d$. 
	It thus follows that $\widehat{\mu}_{\infty}^{(\omega)}\in\CP_{2,\mathrm{ac}}(\R^d)$.
	Now, the continuity of the mapping $\CP_{2,\mathrm{ac}}(\R^d)\ni\mu \mapsto \CW_2\big(\mu,G(\mu)\big)^2\in\R_+$ in Theorem~\ref{thm: G-property}\ref{thms: G-property-continuity} implies that $\CW_2\Big(\widehat{\mu}_{\infty}^{(\omega)}, G\big(\widehat{\mu}_{\infty}^{(\omega)}\big) \Big)^2=\lim_{i\to\infty} \CW_2\Big(\widehat{\mu}_{t_i}^{(\omega)}, G\big(\widehat{\mu}_{t_i}^{(\omega)}\big) \Big)^2=0$, which shows that $\widehat{\mu}_{\infty}^{(\omega)}$ is a fixed-point of $G$. 
	Since $\PROB[\widetilde{\Omega}]=1$, it holds $\PROB$-almost surely that every accumulation point of $(\widehat{\mu}_t)_{t\in\N_0}$ is a fixed-point of~$G$. 
	We have thus completed the proof of statement~\ref{thms: fixedpoint-convergence-convergence}.

	Finally, if $G$ has a unique fixed-point $\bar{\mu}\in\CP_{2,\mathrm{ac}}(\R^d)$, then statement~\ref{thms: fixedpoint-convergence-convergence} implies that, $\PROB$-almost surely, every accumulation point of $(\widehat{\mu}_t)_{t\in\N_0}$ is equal to $\bar{\mu}$.
	Therefore, $(\widehat{\mu}_t)_{t\in\N_0}$ converges $\PROB$-almost surely to $\bar{\mu}$, which is the unique Wasserstein barycenter of $\nu_1,\ldots,\nu_K$ by Theorem~\ref{thm: G-property}\ref{thms: G-property-fixedpoint}.
	The proof is now complete. 
\end{proof}

\begin{remark}[Computational tractability of Algorithm~\ref{algo: iteration}]
	Assume that: (i)~independent random samples from $\nu_1,\ldots,\nu_K$, and $\rho_0$ can be efficiently generated; (ii)~the plug-in OT map estimator $\widehat{T}^{\mu,m}_{\nu,n}(\,\cdot\,;\theta)$ can be tractably computed and can be tractably evaluated at any point $\BIx\in\R^d$; (iii)~for all $r\in\N$, checking whether a point $\BIx\in\R^d$ belongs to $\CX_{r}$ is computationally tractable.
	Then, Algorithm~\ref{algo: iteration} is computationally tractable. 
	Indeed, for $t\in\N$, a random sample from $\widehat{\mu}_t$ can be generated by rejection sampling. 
	Specifically, one first generates a random sample $\BIX\in\R^d$ from $\rho_0$ and evaluates the composition $\widehat{\BIX}:=\big[\sum_{k=1}^K \widehat{T}_{t,k}\big]\circ \cdots \circ \big[\sum_{k=1}^K \widehat{T}_{1,k}\big](\BIX)$.
	This sample $\widehat{\BIX}$ is subsequently accepted if $\widehat{\BIX}\in \CX_{R_t}$. 
	Otherwise, this process repeatedly generates $\widehat{\BIX}$ until $\widehat{\BIX}$ is accepted. 
	The computational tractability of the plug-in OT map estimator $\widehat{T}^{\mu,m}_{\nu,n}(\,\cdot\,;\theta)$ is discussed in Remark~\ref{rmk: estimator-tractability-kernel} and Remark~\ref{rmk: estimator-tractability-barrier} in Section~\ref{sec: concrete-estimators}. 
\end{remark}

\begin{remark}
	We would like to remark that the operator~$G$ in (\ref{eqn: G-operator}) does not always have a unique fixed-point for general input probability measures $\nu_1,\ldots,\nu_K\in\CP_{2,\mathrm{ac}}(\R^d)$; see, e.g., Example~3.1 of \citep{alvarez2016fixed} for a concrete counterexample. 
	It is known that $G$ has a unique fixed-point when $\nu_1,\ldots,\nu_K$ belong to the same parametric family of elliptical distributions \citep[Section~4]{alvarez2016fixed}, e.g., Gaussian distributions. 
	However, to the best of our knowledge, sufficient conditions to guarantee the uniqueness of the fixed-point of~$G$ for non-parametric $\nu_1,\ldots,\nu_K$ is still an open problem. 
\end{remark}

\begin{remark}
	Same as the deterministic fixed-point iterative scheme of \citet{alvarez2016fixed}, our stochastic extension in Algorithm~\ref{algo: iteration} does not provide any non-asymptotic rate of convergence. 
	\TODO{[To discuss: add discussions about empirical evidence?]}
\end{remark}

\section{Concrete Examples of Plug-In OT Map Estimators}
\label{sec: concrete-estimators}


As stated in Setting~\ref{sett: convergence}, the convergence of Algorithm~\ref{algo: iteration} depends crucially on the plug-in OT map estimator $\widehat{T}^{\mu,m}_{\nu,n}(\,\cdot\,;\theta)$, specifically on its shape, growth, and consistency properties required by Assumption~\ref{asp: plugin-estimator}.
In this section, we consider two admissible compactly supported probability measures $\mu,\nu\in\CM$ and introduce two concrete examples of plug-in OT map estimators that satisfy Assumption~\ref{asp: plugin-estimator}. 
For the sake of notational simplicity, we will omit $\mu,\nu,m,n$ in the notations for estimators in this section. 
Nonetheless, $m$ and $n$ will always be understood as the numbers of samples from $\mu$ and $\nu$, respectively. 

Both of our examples are inspired by the estimation error bound of plug-in OT map estimators developed by \citet{manole2021plugin} as well as the shape-constrained convex least squares regression and shape-constrained convex interpolation methods developed by \citet{taylor2017convex, taylor2017smooth}. 
We will first introduce these preliminary results in Section~\ref{ssec: estimators-preliminaries}.
Subsequently, we will introduce our kernel-smoothed OT map estimator $\widehat{T}_{\mathrm{kern}}(\,\cdot\,;\theta)$ in Section~\ref{ssec: estimators-kernelsmoothed} and introduce our barrier-based map estimator $\widehat{T}_{\mathrm{barr}}(\,\cdot\,;\theta)$ in Section~\ref{ssec: estimators-barrier}. 


\subsection{Preliminaries on plug-in OT map estimators and shaped-constrained interpolation}
\label{ssec: estimators-preliminaries}

Both of our proposed estimators are based on the following shape-constrained convex least squares OT map estimator, defined as follows. 

\begin{definition}[Shape-constrained convex least squares OT map estimator]
	\label{def: convex-leastsquares}
	Let $\mu,\nu\in\CM$ (recall Definition~\ref{def: admissible-measures}) with $\veczero_d\in\support(\mu)$, let $T^{\mu}_{\nu}$ be the OT map from $\mu$ to $\nu$, and let $(\Omega,\CF,\PROB)$ be a probability space.
	Let $\underline{\lambda}\equiv \underline{\lambda}(\mu,\nu)$ and $\overline{\lambda}\equiv \overline{\lambda}(\mu,\nu)$ be chosen based on $\mu$ and $\nu$ such that $0<\underline{\lambda}<\overline{\lambda}<\infty$ satisfy $\underline{\lambda}\BI_d\preceq \nabla T^{\mu}_{\nu}(\BIx) \preceq \overline{\lambda}\BI_d$ for all $\BIx\in\support(\mu)$ (recall Lemma~\ref{lem: curvature}).
	For $m\in\N$ independent random samples $\BIX_1,\ldots,\BIX_m:\Omega\to\R^d$ from $\mu$ and $n\in\N$ independent random samples $\BIY_1,\ldots,\BIY_n:\Omega\to\R^d$ from $\nu$, 
	let $(\widehat{\pi}^\star_{i,j})_{i=1:m,\,j=1:n}$ be an optimal solution of the following linear programming (LP) problem:
	\begin{align}
		\label{eqn: estimator-discreteOT}
		\begin{split}
			\minimize_{(\widehat{\pi}_{i,j})} \quad & \sum_{i=1}^m\sum_{j=1}^n\widehat{\pi}_{i,j}\|\BIX_i-\BIY_j\|^2 \\
			\text{subject to} \quad & \sum_{j=1}^n\widehat{\pi}_{i,j}=\frac{1}{m} \hspace{94pt} \forall 1\le i\le m,\\
			& \sum_{i=1}^m\widehat{\pi}_{i,j}=\frac{1}{n} \hspace{99pt} \forall 1\le j\le n, \\
			& \widehat{\pi}_{i,j}\ge 0  \hspace{60pt} \forall 1\le i\le m,\;\forall 1\le j\le n.
		\end{split}
	\end{align}
	Subsequently, let $(\widetilde{\varphi}_i^\star)_{i=1:m}$, $(\widetilde{\BIg}_i^\star)_{i=1:m}$ be an optimal solution of the following quadratically constrained quadratic programming (QCQP) problem:
	\begin{align}
		\label{eqn: estimator-QCQPregression}
		\begin{split}
		\minimize_{(\widetilde{\varphi}_i),\,(\widetilde{\BIg}_i)} \quad & \sum_{i=1}^m\sum_{j=1}^n\widehat{\pi}_{i,j}^\star\|\widetilde{\BIg}_i+\underline{\lambda}\BIX_i-\BIY_j\|^2\\
		\text{subject to} \quad & \widetilde{\varphi}_j\ge \widetilde{\varphi}_i + \langle\widetilde{\BIg}_i,\BIX_j-\BIX_i\rangle +\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\|\widetilde{\BIg}_i-\widetilde{\BIg}_j\|^2 \quad \forall 1\le i\le m,\;\forall 1\le j\le m,\\
		& \|\widetilde{\BIg_i} + \underline{\lambda}\BIX_i\|^2 \leq \overline{u}_0(\nu)^2 \hspace{166.5pt} \quad \forall 1\le i\le m,
		\end{split}
  	\end{align}
	where $\overline{u}_0(\nu):=\inf\big\{r\in\R_+: \support(\nu)\subseteq \bar{B}(\veczero, r)\big\}$.
	Let $\varphi^\star_i:=\widetilde{\varphi}^\star_i+\frac{\underline{\lambda}}{2}\|\BIX_i\|^2$, $\BIg^\star_i:=\widetilde{\BIg}^\star_i+\underline{\lambda}\BIX_i$ for $i=1,\ldots,m$. 
	We call $\widehat{T}_{\mathrm{CLS}}:\R^d\to\R^d$ a shaped-constrained convex least squares OT map estimator of $T^{\mu}_{\nu}$ if there exists $\widehat{\varphi}_{\mathrm{CLS}}\in\FC_{\underline{\lambda},\overline{\lambda}}(\R^d)$ such that $\widehat{T}_{\mathrm{CLS}}=\nabla \widehat{\varphi}_{\mathrm{CLS}}$ and $\widehat{\varphi}_{\mathrm{CLS}}(\BIX_i)=\varphi^\star_i$, $\widehat{T}_{\mathrm{CLS}}(\BIX_i)=\BIg^\star_i$ for $i=1,\ldots,m$.
\end{definition}

Note that the linear programming problem~(\ref{eqn: estimator-discreteOT}) computes an optimal coupling of the empirical measures $\dot{\mu}_{m}:=\frac{1}{m}\sum_{i=1}^m\delta_{\BIX_i}\in\CP_{2}(\R^d)$ and $\dot{\nu}_{n}:=\frac{1}{n}\sum_{j=1}^n\delta_{\BIY_j}\in\CP_2(\R^d)$, i.e., $\widehat{\pi}^\star:=\sum_{i=1}^m\sum_{j=1}^n\widehat{\pi}^\star_{i,j}\delta_{(\BIX_i,\BIY_j)}\in \Pi(\dot{\mu}_{m},\dot{\nu}_{n})$ satisfies \sloppy{$\int_{\R^d\times\R^d}{\|\BIx-\BIy\|}^2\DIFFM{\widehat{\pi}^\star}{\DIFF\BIx,\DIFF\BIy}=\CW_2(\dot{\mu}_{m},\dot{\nu}_{n})^2$}. 
The QCQP problem (\ref{eqn: estimator-QCQPregression}) utilizes the tractable formulation of smoothness and strong convexity constraints developed by \citet{taylor2017convex}. 
Below we present a version of their results adapted to our settings.

\begin{theorem}[Formulation of shape constraints; {adapted from \citep[Theorem~3.8]{taylor2017convex}}]
	\label{thm: shape-constraints}
	Under the settings of Definition~\ref{def: convex-leastsquares},
	$\widehat{T}_{\mathrm{CLS}}:\R^d\to\R^d$ is a shaped-constrained convex least squares OT map estimator of $T^{\mu}_{\nu}$ if and only if 
	$\widehat{T}_{\mathrm{CLS}}=\nabla \widehat{\varphi}_{\mathrm{CLS}}$ and $\widehat{\varphi}_{\mathrm{CLS}}$ is an optimizer of the following minimization problem over the set of all convex functions on $\R^d$ subject to shape constraints:
	\begin{align}
		\label{eqn: shape-constrained-regression}
		\begin{split}
		\minimize_{\varphi} \quad & \sum_{i=1}^m\sum_{j=1}^n\widehat{\pi}^\star_{i,j}\big\|\nabla\varphi(\BIX_i)-\BIY_j\big\|^2 \\
		\text{subject to} \quad & \varphi\in\FC_{\underline{\lambda},\overline{\lambda}}(\R^d), \; \big\|\nabla\varphi(\BIX_i)\big\|\le \overline{u}_0(\nu) \quad \forall 1\le i\le m.
		\end{split}
	\end{align}
\end{theorem}

Let us first state the following result from \citep{taylor2017convex} as a lemma, which will be used in the proofs in this section. 

\begin{lemma}[{\citep[Theorem~3.8]{taylor2017convex}}]
	\label{lem: convex-interpolability}
	For any $0\le \underline{l}<\overline{l} \le \infty$, we call $(\BIx_i,\BIg_i,\varphi_i)_{i=1:m}$ $\FC_{\underline{l},\overline{l}}(\R^d)$-interpolable, if there exists $\varphi\in\FC_{\underline{l},\overline{l}}(\R^d)$ such that $\varphi(\BIx_i)=\varphi_i$ and $\BIg_i\in\partial\varphi(\BIx_i)$ for $i=1,\ldots,m$. 
	In this case, we say $(\BIx_i,\BIg_i,\varphi_i)_{i=1:m}$ $\FC_{\underline{l},\overline{l}}(\R^d)$ is interpolated by $\varphi$. 
	Then, the following statements are equivalent:
	\begin{enumerate}[label=(\alph*), beginpenalty=10000, leftmargin=20pt]

		\item\label{lems: convex-interpolatility-zeroinfty-dual}
		$\big(\frac{\overline{l}}{\overline{l}-\underline{l}}\BIx_i-\frac{1}{\overline{l}-\underline{l}}\BIg_i,\BIg_i-\underline{l}\BIx_i,\varphi_i+\frac{\underline{l}}{\overline{l}-\underline{l}}\langle\BIg_i,\BIx_i\rangle-\frac{1}{2(\overline{l}-\underline{l})}\|\BIg_i\|^2-\frac{\underline{l}\overline{l}}{2(\overline{l}-\underline{l})}\|\BIx_i\|^2\big)_{i=1:m}$ is interpolated by $\varphi\in\FC_{0,\infty}(\R^d)$.

		\item\label{lems: convex-interpolatility-zeroinfty}
		$\big(\BIg_i-\underline{l}\BIx_i,\frac{\overline{l}}{\overline{l}-\underline{l}}\BIx_i-\frac{1}{\overline{l}-\underline{l}}\BIg_i,\frac{\overline{l}}{\overline{l}-\underline{l}}\langle\BIg_i,\BIx_i\rangle-\varphi_i-\frac{1}{2(\overline{l}-\underline{l})}\|\BIg_i\|^2-\frac{\underline{l}\overline{l}}{2(\overline{l}-\underline{l})}\|\BIx_i\|^2\big)_{i=1:m}$ is interpolated by $\varphi^*\in\FC_{0,\infty}(\R^d)$; 

		\item\label{lems: convex-interpolatility-sc}
		$\big(\BIg_i-\underline{l}\BIx_i,\BIx_i,\langle\BIg_i,\BIx_i\rangle-\varphi_i-\frac{\underline{l}}{2}\|\BIx_i\|^2\big)_{i=1:m}$ is interpolated by $\varphi^*+\frac{1}{2(\overline{l}-\underline{l})}\|\cdot\|^2\in \FC_{\frac{1}{\overline{l}-\underline{l}},\infty}(\R^d)$;

		\item\label{lems: convex-interpolatility-sm}
		$\big(\BIx_i,\BIg_i-\underline{l}\BIx_i,\varphi_i-\frac{\underline{l}}{2}\|\BIx_i\|^2\big)_{i=1:m}$ is interpolated by $\big(\varphi^*+\frac{1}{2(\overline{l}-\underline{l})}\|\cdot\|^2\big)^*\in \FC_{0,\overline{l}-\underline{l}}(\R^d)$;

		\item\label{lems: convex-interpolatility-sm-sc}
		$(\BIx_i,\BIg_i,\varphi_i)_{i=1:m}$ is interpolated by $\big(\varphi^*+\frac{1}{2(\overline{l}-\underline{l})}\|\cdot\|^2\big)^*+\frac{\underline{l}}{2}\|\cdot\|^2\in \FC_{\underline{l},\overline{l}}(\R^d)$; 
		
	\end{enumerate}
\end{lemma}


\begin{proof}[Proof of Theorem~\ref{thm: shape-constraints}]
	It holds by the equivalence of~\ref{lems: convex-interpolatility-sm-sc} and~\ref{lems: convex-interpolatility-sm} in Lemma~\ref{lem: convex-interpolability} and the statement of \citep[Theorem~3.8]{taylor2017convex} that:
	\begin{align*}
		& \phantom{\Leftrightarrow}\;\; \quad \exists \varphi\in\FC_{\underline{\lambda},\overline{\lambda}}(\R^d),\; \varphi(\BIX_i)=\varphi_i,\; \nabla\varphi(\BIX_i)=\BIg_i \; \forall 1\le i\le m \\
		& \Leftrightarrow \quad \exists \widetilde{\varphi}\in\FC_{0,\overline{\lambda}-\underline{\lambda}}(\R^d),\;\widetilde{\varphi}(\BIX_i)=\widetilde{\varphi}_i:=\varphi_i-{\textstyle\frac{\underline{\lambda}}{2}}\|\BIX_i\|^2, \; \nabla\widetilde{\varphi}(\BIX_i)=\widetilde{\BIg}_i:=\BIg_i-\underline{\lambda}\BIx_i \; \forall 1\le i\le m \\
		& \Leftrightarrow \quad \widetilde{\varphi}_j\ge \widetilde{\varphi}_i + \langle\widetilde{\BIg}_i,\BIX_j-\BIX_i\rangle +\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\|\widetilde{\BIg}_i-\widetilde{\BIg}_j\|^2 \; \forall 1\le i\le m,\;\forall 1\le j\le m.
	\end{align*}
	Thus, the two optimization problems (\ref{eqn: estimator-QCQPregression}) and (\ref{eqn: shape-constrained-regression}) are equivalent under the reparametrization of the decision variables: $\varphi(\BIX_i)\leftrightarrow \widetilde{\varphi}_i+{\textstyle\frac{\underline{\lambda}}{2}}\|\BIX_i\|^2$, $\nabla\varphi(\BIX_i) \leftrightarrow \widetilde{\BIg}_i+\underline{\lambda}\BIX_i$ for $i=1,\ldots,m$. This completes the proof. 
\end{proof}

\begin{remark}
	\label{remark: ADMM}
	Since solving the QCQP in \eqref{eqn: estimator-QCQPregression} suffers from prohibitive computational complexity when the sample size and the dimension grow, we provide in Appendix~\ref{apx: ADMM} a paralleled implementation of it using the first-order alternating direction method of multipliers (ADMM). The algorithm is adapted from \citet{simonetto2021smooth} by exploiting the underlying decomposable structure in \eqref{eqn: estimator-QCQPregression}, and similar techniques for efficient shape-constrained convex regression have been considered in literature; see, e.g.,\@ \citet{aybat2014parallel} and \citet*{mazumder2019computational}.
\end{remark}

In the following, we adapt the results of \citet{manole2021plugin} to derive the estimation error bound of any shape-constrained convex least squares OT map estimator in Definition~\ref{def: convex-leastsquares}. 

\begin{theorem}[Estimation error bound of $\widehat{T}_{\mathrm{CLS}}$; {adapted from \citep[Proposition~15]{manole2021plugin}}]
	\label{thm: plugin-estimation-error}
	Under the settings of Definition~\ref{def: convex-leastsquares}, let $\widehat{T}_{\mathrm{CLS}}:\R^d\to\R^d$ be a shape-constrained convex least squares OT map estimator of $T^{\mu}_{\nu}$ based on $m\in\N$ independent random samples $\BIX_1,\ldots,\BIX_m:\Omega\to\R^d$ from $\mu$ and $n\in\N$ independent random samples $\BIY_1,\ldots,\BIY_n:\Omega\to\R^d$ from $\nu$. 
	Then, there exists a constant $C(\mu,\nu,\underline{\lambda},\overline{\lambda})>0$ that depends on $\mu$, $\nu$ and the choices of $\underline{\lambda}\equiv\underline{\lambda}(\mu,\nu)$, $\overline{\lambda}\equiv\overline{\lambda}(\mu,\nu)$, such that
	\begin{align*}
		\EXP\Big[\big\|\widehat{T}_{\mathrm{CLS}}-T^{\mu}_{\nu}\big\|^2_{\CL^2(\mu)}\Big] \le C(\mu,\nu,\underline{\lambda},\overline{\lambda})\log(m)^2 \kappa\big({\min\{m,n\}}\big),
	\end{align*}
	where
	\begin{align*}
		\kappa(q):=\begin{cases}
			q^{-\frac{1}{2}} & d\le 3, \\
			q^{-\frac{1}{2}}\log(q) & d=4, \\
			q^{-\frac{2}{d}} & d\ge 5
		\end{cases} \qquad\qquad \forall q\in\N.
	\end{align*}
\end{theorem}


\begin{proof}[Proof of Theorem~\ref{thm: plugin-estimation-error}]
	Before applying \citep[Proposition~15]{manole2021plugin}, let us show that the support of $\mu$ and $\nu$ satisfy the premises of \citep[Proposition~15]{manole2021plugin}. 
	Let $\lebesgue$ denote the Lebesgue measure on $\R^d$. 
	Specifically, we will prove the following claim: for every $\CX\in\CS$, there exist $\epsilon_0>0$ and $\delta_0>0$ such that $\lebesgue\big(\CX\cap B(\BIx,\epsilon)\big) \ge \delta_0 \lebesgue\big(B(\BIx,\epsilon)\big)$ for all $\BIx\in\CX$ and for all $\epsilon\in(0,\epsilon_0)$.
	To that end, let us fix an arbitrary $\CX\in\CS$ and let $r>0$ satisfy $B(\veczero,r)\supset \CX$.
	Subsequently, let $\epsilon_0:=2r$ and let $\delta_0:=\frac{\lebesgue(\CX)}{\lebesgue(B(\veczero,\epsilon_0))}>0$.
	Then, for any $\BIx,\BIx'\in\CX$, it holds that $\|\BIx-\BIx'\|\le\|\BIx\|+\|\BIx'\|\le 2r=\epsilon_0$, which implies that $\CX\subset B(\BIx,\epsilon_0)$.
	Therefore, $\lebesgue\big(\CX\cap B(\BIx,\epsilon_0)\big)=\lebesgue(\CX)=\delta_0\lebesgue\big(B(\veczero, \epsilon_0)\big)=\delta_0\lebesgue\big(B(\BIx,\epsilon_0)\big)$. 
	Next, for any $\epsilon\in(0,\epsilon_0)$ and for any $\BIy\in\CX\cap B(\BIx,\epsilon_0)$, note that $\BIx+\frac{\epsilon}{\epsilon_0}(\BIy-\BIx)\in\CX\cap B(\BIx,\epsilon)$ due to the convexity of $\CX$.
	This implies that 
	\begin{align*}
		\lebesgue\big(\CX\cap B(\BIx,\epsilon)\big) &\ge \lebesgue\big(\big\{\BIx+{\textstyle\frac{\epsilon}{\epsilon_0}}(\BIy-\BIx):\BIy\in \CX\cap B(\BIx,\epsilon_0)\big\}\big) \\
		&= \lebesgue\big({\textstyle\frac{\epsilon_0-\epsilon}{\epsilon_0}}\BIx+{\textstyle\frac{\epsilon}{\epsilon_0}}\big(\CX\cap B(\BIx,\epsilon_0)\big)\big)\\
		&=\big({\textstyle\frac{\epsilon}{\epsilon_0}}\big)^d\lebesgue\big(\CX\cap B(\BIx,\epsilon_0)\big) \\
		&=\delta_0\big({\textstyle\frac{\epsilon}{\epsilon_0}}\big)^d\lebesgue\big(B(\BIx,\epsilon_0)\big)\\
		&=\delta_0\lebesgue\big(B(\BIx,\epsilon)\big) \qquad\qquad\qquad\qquad \forall \epsilon\in(0,\epsilon_0).
	\end{align*}
	This proves the claim, and hence the assumption~(S2) of \citep[Proposition~15]{manole2021plugin} holds. 

	Moreover, let $\dot{\mu}_{m}:=\frac{1}{m}\sum_{i=1}^m\delta_{\BIX_i}\in\CP_{2}(\R^d)$. We have by the constraints in the linear programming problem (\ref{eqn: estimator-discreteOT}) that
	\begin{align}
		\label{eqn: plugin-estimation-error-proof-consistency}
		\begin{split}
		\big\|\widehat{T}_{\mathrm{CLS}}-T^{\mu}_{\nu}\big\|^2_{\CL^2(\dot{\mu}_m)} & = \frac{1}{m}\sum_{i=1}^m  \big\|\widehat{T}_{\mathrm{CLS}}(\BIX_i)-T^{\mu}_{\nu}(\BIX_i)\big\|^2 \\
		&=\sum_{i=1}^m\sum_{j=1}^n\widehat{\pi}^\star_{i,j} \big\|\widehat{T}_{\mathrm{CLS}}(\BIX_i)-T^{\mu}_{\nu}(\BIX_i)\big\|^2\\
		&\le 2\sum_{i=1}^m\sum_{j=1}^n\widehat{\pi}^\star_{i,j} \Big(\big\|\widehat{T}_{\mathrm{CLS}}(\BIX_i)-\BIY_j\big\|^2 + \big\|T^{\mu}_{\nu}(\BIX_i)-\BIY_j\big\|^2\Big).
		\end{split}
	\end{align}
	Notice that $T^{\mu}_{\nu}=\nabla\varphi^{\mu}_{\nu}$, and by Lemma~\ref{lem: curvature} we can assume without loss of generality that $\varphi^{\mu}_{\nu}\in\FC_{\underline{\lambda},\overline{\lambda}}(\R^d)$. 
	Furthermore, since $(\BIX_i)_{i=1:m}\subset \support(\mu)$ $\PROB$-almost surely and $\nabla \varphi^{\mu}_{\nu}(\support(\mu))=\support(\nu)\subseteq \bar{B}\big(\veczero,\overline{u}_0(\nu)\big)$, the inequality $\big\|\nabla \varphi^{\mu}_{\nu}(\BIX_i)\big\|\le \overline{u}_0(\nu)$ holds $\PROB$-almost surely for {$i=1,\ldots,m$}. 
	Consequently, $\varphi^{\mu}_{\nu}$ is $\PROB$-almost surely feasible for the optimization problem~(\ref{eqn: shape-constrained-regression}).
	Since $\widehat{T}_{\mathrm{CLS}}=\nabla\widehat{\varphi}_{\mathrm{CLS}}$ where $\widehat{\varphi}_{\mathrm{CLS}}$ is optimal for (\ref{eqn: shape-constrained-regression}) by Theorem~\ref{thm: shape-constraints}, we get
	\begin{align*}
		\sum_{i=1}^m\sum_{j=1}^n\widehat{\pi}^\star_{i,j} \big\|\widehat{T}_{\mathrm{CLS}}(\BIX_i)-\BIY_j\big\|^2 \le \sum_{i=1}^m\sum_{j=1}^n\widehat{\pi}^\star_{i,j} \big\|T^{\mu}_{\nu}(\BIX_i)-\BIY_j\big\|^2 \qquad\PROB\text{-a.s.}
	\end{align*}
	Substituting this into (\ref{eqn: plugin-estimation-error-proof-consistency}) yields
	\begin{align*}
		\big\|\widehat{T}_{\mathrm{CLS}}-T^{\mu}_{\nu}\big\|^2_{\CL^2(\dot{\mu}_m)} &\le 4 \sum_{i=1}^m\sum_{j=1}^n\widehat{\pi}^\star_{i,j} \big\|T^{\mu}_{\nu}(\BIX_i)-\BIY_j\big\|^2 \hspace{38pt} \qquad\PROB\text{-a.s.}
	\end{align*}
	The rest of the proof is then identical to the proof of Proposition~15 in \citep{manole2021plugin}, up to rescaling $\support(\mu)$ and $\support(\nu)$ to be contained in $[0,1]^d$ as well as extending to the case where $\support(\mu)\ne\support(\nu)$.
\end{proof}

\begin{remark}[Choice of $\underline{\lambda}(\mu,\nu)$ and $\overline{\lambda}(\mu,\nu)$]
	\label{rmk: choice-of-shape-parameters}
	In Theorem~\ref{thm: plugin-estimation-error}, the dependence of the constant term $C(\mu,\nu,\underline{\lambda},\overline{\lambda})$ on the choices of $\underline{\lambda}\equiv\underline{\lambda}(\mu,\nu)$, $\overline{\lambda}\equiv\overline{\lambda}(\mu,\nu)$ is explicitly stated. 
	Even though any choices of $0<\underline{\lambda}(\mu,\nu)<\overline{\lambda}(\mu,\nu)<\infty$ that satisfy $\underline{\lambda}(\mu,\nu)\BI_d\preceq \nabla T^{\mu}_{\nu}(\BIx) \preceq \overline{\lambda}(\mu,\nu)\BI_d$ for all $\BIx\in\support(\mu)$ would be valid, 
	both a decrease in $\underline{\lambda}(\mu,\nu)$ and an increase in $\overline{\lambda}(\mu,\nu)$ will lead to an increase of $C\big(\mu,\nu,\underline{\lambda}(\mu,\nu),\overline{\lambda}(\mu,\nu)\big)$. 
	Despite this, we assume that $\underline{\lambda}\equiv\underline{\lambda}(\mu,\nu)$, $\overline{\lambda}\equiv\overline{\lambda}(\mu,\nu)$ can be unambiguously chosen given any $\mu,\nu\in\CM$. 
\end{remark}

Given $(\varphi^\star_i)_{i=1:m}$, $(\BIg^\star_i)_{i=1:m}$ that are constructed via Definition~\ref{def: convex-leastsquares}, the functions $\widehat{\varphi}_{\mathrm{CLS}}\in\FC_{\underline{\lambda},\overline{\lambda}}(\R^d)$ and $\widehat{T}_{\mathrm{CLS}}=\nabla\widehat{\varphi}_{\mathrm{CLS}}$ are only specified at the sample points $(\BIX_i)_{i=1:m}$. 
In fact, interpolation (and extrapolation) of $\widehat{\varphi}_{\mathrm{CLS}}$ and $\widehat{T}_{\mathrm{CLS}}$ to the entire $\R^d$ is non-unique. 
Thus, let us introduce the following quadratic programming (QP) formulation for computing the smallest of such interpolation functions, which is a simplified version of the formulation developed by \citet{taylor2017convex}. 

\begin{theorem}[Smallest shape-constrained interpolation function {\citep[Theorem~3.14]{taylor2017convex}}]
	\label{thm: convex-leastsquares-LB}
	Under the settings of Definition~\ref{def: convex-leastsquares}, let us define the following terms:
	\begin{align}
		\label{eqn: convex-leastsquares-LB-notations}
		\begin{split}
		\Delta&:=\Big\{\BIw=(w_1,\ldots,w_m)^\TRANSP:\textstyle\sum_{i=1}^mw_i=1,\;w_i\ge 0\;\forall 1\le i\le m\Big\}\subset\R^m,\\
		\widetilde{\BG}^\star&:=\left(\begin{smallmatrix}
			  | & | &  & | \\
			  \widetilde{\BIg}^\star_1 & \widetilde{\BIg}^\star_2 & \cdots & \widetilde{\BIg}^\star_m \\
			  | & | &  & |
		\end{smallmatrix}\right)\in\R^{d\times m},\\
		v_i&:=\varphi^\star_i+\textstyle\frac{1}{2(\overline{\lambda} - \underline{\lambda})}\|\BIg^\star_i\|^2+\frac{\underline{\lambda}\overline{\lambda}}{2(\overline{\lambda}-\underline{\lambda})}\|\BIX_i\|^2-\frac{\overline{\lambda}}{\overline{\lambda} - \underline{\lambda}}\langle\BIg^\star_i,\BIX_i\rangle \in\R  \qquad \forall 1\le i\le m,\\
		\BIv&:=(v_1,\ldots,v_m)^\TRANSP\in\R^m.
		\end{split}
  	\end{align}
	Let $\widehat{\varphi}_{\mathrm{CLS\text{-}LB}}:\R^d\to\R$ and $\widehat{T}_{\mathrm{CLS\text{-}LB}}:\R^d\to\R^d$ be defined as follows:
  	\begin{align}
		\widehat{\varphi}_{\mathrm{CLS\text{-}LB}}(\BIx) &:={\textstyle\frac{\underline{\lambda}}{2}}\|\BIx\|^2+{\displaystyle\sup_{\BIw\in\Delta}}\Big\{\langle\widetilde{\BG}^{\star\TRANSP}\BIx+\BIv,\BIw\rangle-\textstyle\frac{1}{2(\overline{\lambda} - \underline{\lambda})}\|\widetilde{\BG}^\star\BIw\|^2\Big\} \qquad\qquad \forall \BIx\in\R^d, 
		\label{eqn: convex-leastsquares-LB} \\
		\begin{split}
		\widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx) &:=\underline{\lambda}\BIx+ \widetilde{\BG}^\star\widehat{\BIw}(\BIx), \\
		\text{where }&\widehat{\BIw}(\BIx)\in{\displaystyle\argmax_{\BIw\in\Delta}}\Big\{\langle\widetilde{\BG}^{\star\TRANSP}\BIx+\BIv,\BIw\rangle-\textstyle\frac{1}{2(\overline{\lambda} - \underline{\lambda})}\|\widetilde{\BG}^\star\BIw\|^2\Big\} \hspace{49pt} \forall\BIx\in\R^d.
		\end{split}
		\label{eqn: convex-leastsquares-LB-gradient}
	\end{align}
	Then, the following statements hold.
	\begin{enumerate}[label=(\roman*), beginpenalty=10000]
		\item\label{thms: convex-leastsquares-LB-smallest}
		$\widehat{\varphi}_{\mathrm{CLS\text{-}LB}}\in\FC_{\underline{\lambda},\overline{\lambda}}(\R^d)$ and $\widehat{\varphi}_{\mathrm{CLS\text{-}LB}}$ is the smallest interpolation function of $(\varphi^\star_i,\BIg^\star_i)_{i=1:m}$ in $\FC_{\underline{\lambda},\overline{\lambda}}(\R^d)$; i.e., for any $\varphi\in\FC_{\underline{\lambda},\overline{\lambda}}(\R^d)$ satisfying $\varphi(\BIX_i)=\varphi^\star_i$ and $\nabla\varphi(\BIX_i)=\BIg^\star_i$ for $i=1,\ldots,m$, it holds that $\widehat{\varphi}_{\mathrm{CLS\text{-}LB}}(\BIx)\le \varphi(\BIx)$ $\forall \BIx\in\R^d$.
		
		\item\label{thms: convex-leastsquares-LB-gradient}
		$\widehat{T}_{\mathrm{CLS\text{-}LB}}$ is uniquely defined by (\ref{eqn: convex-leastsquares-LB-gradient}) for every $\BIx\in\R^d$ and $\widehat{T}_{\mathrm{CLS\text{-}LB}}=\nabla \widehat{\varphi}_{\mathrm{CLS\text{-}LB}}$.
	\end{enumerate}
\end{theorem}

Since \citet{taylor2017convex} did not provide a detailed proof of this result, we will present a detailed derivation for the sake of completeness. 

\begin{proof}[Proof of Theorem~\ref{thm: convex-leastsquares-LB}]
	To begin, for arbitrary $(\bar{\BIx}_i,\bar{\BIg}_i,\bar{\varphi}_i)_{i=1:m}$ that is $\FC_{0,\infty}(\R^d)$-interpolable, the smallest function $\varphi_{\mathrm{LB}}\in\FC_{0,\infty}(\R^d)$ that interpolates $(\bar{\BIx}_i,\bar{\BIg}_i,\bar{\varphi}_i)_{i=1:m}$ is given by 
	\begin{align}
	\label{eqn: convex-leastsquares-LB-proof-piecewise}
	\varphi_{\mathrm{LB}}(\BIx)=\max_{1\le i\le m}\left\{\bar{\varphi}_i+\langle\bar{\BIg}_i,\BIx-\bar{\BIx}_i\rangle\right\} \qquad\forall \BIx\in\R^d.
	\end{align}
	It holds that $\varphi_{\mathrm{LB}}(\BIx)\le \varphi(\BIx)$ for all $\BIx\in\R^d$ for any other $\varphi\in\FC_{0,\infty}(\R^d)$ that interpolates $(\bar{\BIx}_i,\bar{\BIg}_i,\bar{\varphi}_i)_{i=1:m}$; see, e.g., \citep[Remark~3.5]{taylor2017convex}. 

	To prove statement~\ref{thms: convex-leastsquares-LB-smallest}, let us first use (\ref{eqn: convex-leastsquares-LB-proof-piecewise}) to derive the smallest interpolation function in Lemma~\ref{lem: convex-interpolability}\ref{lems: convex-interpolatility-zeroinfty-dual} with $\BIx_i\leftarrow\BIX_i$, $\BIg_i\leftarrow\BIg^\star_i$, $\varphi_i\leftarrow \varphi^\star_i$ for $i=1,\ldots,m$, $\underline{l}\leftarrow\underline{\lambda}$, $\overline{l}\leftarrow\overline{\lambda}$. 
	Let us define $\varphi_{\mathrm{LB}}^{(a)}:\R^d\to\R$ as follows:
	\begin{align*}
	%\label{def: convex-leastsquares-LB-proof-zeroinfty-dual}
	\begin{split}
		\varphi^{(a)}_{\mathrm{LB}}(\BIx)&:=\sup_{1\le i\le m}\textstyle\Big\{\varphi^\star_i+\frac{\underline{\lambda}}{\overline{\lambda}- \underline{\lambda}}\langle\BIg^\star_i,\BIX_i\rangle-\frac{1}{2(\overline{\lambda} - \underline{\lambda})}\|\BIg^\star_i\|^2-\frac{\underline{\lambda}\overline{\lambda}}{2(\overline{\lambda} - \underline{\lambda})}\|\BIX_i\|^2 \\
		& \hspace{130pt}\textstyle + \big\langle\BIg^\star_i-\underline{\lambda}\BIX_i,\BIx-\frac{\overline{\lambda}}{\overline{\lambda} - \underline{\lambda}}\BIX_i+\frac{1}{\overline{\lambda} - \underline{\lambda}}\BIg^\star_i\big\rangle\Big\}\\
		&\phantom{:}=\sup_{1\le i\le m}\textstyle\Big\{\langle\BIg^\star_i-\underline{\lambda}\BIX_i,\BIx\rangle+\varphi^\star_i+\frac{1}{2(\overline{\lambda} - \underline{\lambda})}\|\BIg^\star_i\|^2+\frac{\underline{\lambda}\overline{\lambda}}{2(\overline{\lambda} - \underline{\lambda})}\|\BIX_i\|^2-\frac{\overline{\lambda}}{\overline{\lambda} - \underline{\lambda}}\langle\BIg^\star_i,\BIX_i\rangle\Big\}\\
		&\phantom{:}=\sup_{1\le i\le m}\textstyle\big\{\langle\widetilde{\BIg}^\star_i,\BIx\rangle+v_i\big\} \hspace{200pt} \forall \BIx\in\R^d.
	\end{split}
	\end{align*}
	Next, we transform $\varphi^{(a)}_{\mathrm{LB}}$ using the equivalence between Lemma~\ref{lem: convex-interpolability}\ref{lems: convex-interpolatility-zeroinfty-dual} and Lemma~\ref{lem: convex-interpolability}\ref{lems: convex-interpolatility-zeroinfty} by taking the convex conjugate of $\varphi^{(a)}_{\mathrm{LB}}$. 
	We define $\varphi^{(b)}_{\mathrm{UB}}:\R^d\to\R\cup\{\infty\}$ as follows:
	\begin{align*}
	%\label{def: convex-leastsquares-LB-proof-zeroinfty-step1}
	\begin{split}
		\varphi^{(b)}_{\mathrm{UB}}(\widetilde{\BIg})&:=\sup_{\BIx\in\R^d}\big\{\langle\widetilde{\BIg},\BIx\rangle-\varphi^{(a)}_{\mathrm{LB}}(\BIx)\big\}\\
		&\phantom{:}=\sup_{\BIx\in\R^d}\Big\{\inf_{1\le i\le m}\Big\{\textstyle\langle\widetilde{\BIg}-\widetilde{\BIg}^\star_i,\BIx\rangle-v_i\Big\}\Big\}\\
		&\phantom{:}=\sup_{\BIx\in\R^d}\Bigg\{\inf_{(w_i)_{i=1:n}\in\Delta}\Bigg\{\sum_{i=1}^nw_i\big(\textstyle\langle\widetilde{\BIg}-\widetilde{\BIg}^\star_i,\BIx\rangle-v_i\big)\Bigg\}\Bigg\}\\
		&\phantom{:}=\sup_{\BIx\in\R^d}\Big\{\inf_{\BIw\in\Delta}\big\{\langle\widetilde{\BIg}-\widetilde{\BG}^\star\BIw,\BIx\rangle-\langle\BIv,\BIw\rangle\big\}\Big\} \hspace{40pt} \forall \widetilde{\BIg}\in\R^d.
	\end{split}
	\end{align*}
	Since $\R^d\times\Delta\ni(\BIx,\BIw)\mapsto \langle\widetilde{\BIg}-\widetilde{\BG}^\star\BIw,\BIx\rangle-\langle\BIv,\BIw\rangle\in\R$ is bilinear in $\BIx$ and $\BIw$, it follows from the compactness of $\Delta$ and Sion's minimax theorem \citep{sion1958on} that 
	\begin{align*}
	%\label{def: convex-leastsquares-LB-proof-zeroinfty}
	\begin{split}
		\varphi^{(b)}_{\mathrm{UB}}(\widetilde{\BIg})&=\sup_{\BIx\in\R^d}\Big\{\inf_{\BIw\in\Delta}\big\{\langle\widetilde{\BIg}-\widetilde{\BG}^\star\BIw,\BIx\rangle-\langle\BIv,\BIw\rangle\big\}\Big\}\\
		&=\inf_{\BIw\in\Delta}\bigg\{\sup_{\BIx\in\R^d}\Big\{\textstyle\langle\widetilde{\BIg}-\widetilde{\BG}^\star\BIw,\BIx\rangle\Big\}-\langle\BIv,\BIw\rangle\bigg\}\\
		&=-\sup_{\BIw\in\Delta,\,\widetilde{\BG}^\star\BIw=\widetilde{\BIg}}\Big\{\langle\BIv,\BIw\rangle \Big\} \hspace{100pt} \forall \widetilde{\BIg}\in\R^d.
	\end{split}
	\end{align*}
	The order reversing property of convex conjugation implies that $\varphi^{(b)}_{\mathrm{UB}}$ is the largest function in $\FC_{0,\infty}(\R^d)$ that interpolates $\big(\BIg^\star_i-\underline{\lambda}\BIX_i,\frac{\overline{\lambda}}{\overline{\lambda} - \underline{\lambda}}\BIX_i-\frac{1}{\overline{\lambda} - \underline{\lambda}}\BIg^\star_i,\frac{\overline{\lambda}}{\overline{\lambda} - \underline{\lambda}}\langle\BIg^\star_i,\BIX_i\rangle-\varphi^\star_i-\frac{1}{2(\overline{\lambda} - \underline{\lambda})}\|\BIg^\star_i\|^2-\frac{\underline{\lambda}\overline{\lambda}}{2(\overline{\lambda} - \underline{\lambda})}\|\BIX_i\|^2\big)_{i=1:m}$. 
	Using the equivalence between Lemma~\ref{lem: convex-interpolability}\ref{lems: convex-interpolatility-zeroinfty} and Lemma~\ref{lem: convex-interpolability}\ref{lems: convex-interpolatility-sc}, we add $\widetilde{\BIg}\mapsto\frac{1}{2(\overline{\lambda} - \underline{\lambda})}\|\widetilde{\BIg}\|^2$ to $\varphi^{(b)}_{\mathrm{UB}}$ and define $\varphi^{(c)}_{\mathrm{UB}}:\R^d\to\R\cup\{\infty\}$ as follows:
	\begin{align*}
	%\label{def: convex-leastsquares-LB-proof-sc}
		\varphi^{(c)}_{\mathrm{UB}}(\widetilde{\BIg})&:=\varphi^{(b)}_{\mathrm{UB}}(\widetilde{\BIg})+\textstyle\frac{1}{2(\overline{\lambda} - \underline{\lambda})}\|\widetilde{\BIg}\|^2={\textstyle\frac{1}{2(\overline{\lambda} - \underline{\lambda})}}\|\widetilde{\BIg}\|^2-\displaystyle\sup_{\BIw\in\Delta,\,\widetilde{\BG}^\star\BIw=\widetilde{\BIg}}\Big\{\langle\BIv,\BIw\rangle \Big\} \qquad\forall \widetilde{\BIg}\in\R^d.
	\end{align*}
	Thus, $\varphi^{(c)}_{\mathrm{UB}}$ is the largest function in $\FC_{\frac{1}{\overline{\lambda} - \underline{\lambda}},\infty}(\R^d)$ that interpolates $\big(\BIg^\star_i-\underline{\lambda}\BIX_i,\BIX_i,\langle\BIg^\star_i,\BIX_i\rangle-\varphi^\star_i-\frac{\underline{\lambda}}{2}\|\BIX_i\|^2\big)_{i=1:m}$. 
	Subsequently, we use the equivalence between Lemma~\ref{lem: convex-interpolability}\ref{lems: convex-interpolatility-sc} and Lemma~\ref{lem: convex-interpolability}\ref{lems: convex-interpolatility-sm} and define $\varphi^{(d)}_{\mathrm{LB}}:\R^d\to\R$ by taking the convex conjugate of $\varphi^{(c)}_{\mathrm{UB}}$:
	\begin{align*}
	%\label{def: convex-leastsquares-LB-proof-sm}
	\begin{split}
		\varphi^{(d)}_{\mathrm{LB}}(\BIx)&:=\sup_{\widetilde{\BIg}\in\R^d}\big\{\langle\widetilde{\BIg},\BIx\rangle-\varphi^{(c)}_{\mathrm{UB}}(\widetilde{\BIg})\big\}\\
		&\phantom{:}=\sup_{\widetilde{\BIg}\in\R^d,\,\BIw\in\Delta,\,\widetilde{\BG}^\star\BIw=\widetilde{\BIg}}\Big\{\langle\widetilde{\BIg},\BIx\rangle-{\textstyle\frac{1}{2(\overline{\lambda} - \underline{\lambda})}}\|\widetilde{\BIg}\|^2+\langle\BIv,\BIw\rangle\Big\}\\
		&\phantom{:}=\sup_{\BIw\in\Delta}\Big\{\langle\widetilde{\BG}^{\star\TRANSP}\BIx+\BIv,\BIw\rangle-\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\|\widetilde{\BG}^\star\BIw\|^2\Big\} \qquad\qquad\qquad \forall \BIx\in\R^d.
	\end{split}
	\end{align*}
	It follows again from the order reversing property of convex conjugation that $\varphi^{(d)}_{\mathrm{LB}}$ is the smallest function in $\FC_{0,\overline{\lambda}-\underline{\lambda}}(\R^d)$ that interpolates $\big(\BIX_i,\BIg^\star_i-\underline{\lambda}\BIX_i,\varphi^\star_i-\frac{\underline{\lambda}}{2}\|\BIX_i\|^2\big)_{i=1:m}$. 
	Finally, using the equivalence between Lemma~\ref{lem: convex-interpolability}\ref{lems: convex-interpolatility-sm} and Lemma~\ref{lem: convex-interpolability}\ref{lems: convex-interpolatility-sm-sc}, we add $\BIx\mapsto\frac{\underline{\lambda}}{2}\|\BIx\|^2$ to $\varphi^{(d)}_{\mathrm{LB}}$ and define $\varphi^{(e)}_{\mathrm{LB}}:\R^d\to\R$ as follows:
	\begin{align*}
		%\label{def: convex-leastsquares-LB-proof-sm-sc}
		\varphi^{(e)}_{\mathrm{LB}}(\BIx):=\varphi^{(d)}_{\mathrm{LB}}(\BIx)+\textstyle\frac{\underline{\lambda}}{2}\|\BIx\|^2={\textstyle\frac{\underline{\lambda}}{2}}\|\BIx\|^2+{\displaystyle\sup_{\BIw\in\Delta}}\Big\{\langle\widetilde{\BG}^{\star\TRANSP}\BIx+\BIv,\BIw\rangle-\textstyle\frac{1}{2(\overline{\lambda} - \underline{\lambda})}\|\widetilde{\BG}^\star\BIw\|^2\Big\} \qquad\forall \BIx\in\R^d.
	\end{align*}
	Therefore, $\varphi^{(e)}_{\mathrm{LB}}$ is the smallest function in $\FC_{\underline{\lambda},\overline{\lambda}}(\R^d)$ that interpolates $\big(\BIX_i,\BIg^\star_i,\varphi^\star_i\big)_{i=1:m}$.
	Since $\widehat{\varphi}_{\mathrm{CLS\text{-}LB}}=\varphi^{(e)}_{\mathrm{LB}}$, we have completed the proof of statement~\ref{thms: convex-leastsquares-LB-smallest}.

	
	To prove statement~\ref{thms: convex-leastsquares-LB-gradient}, observe that $\widehat{T}_{\mathrm{CLS\text{-}LB}}$ can be equivalently expressed as follows:
	\begin{align}
		\label{eqn: convex-leastsquares-LB-proof-gradient-alternative}
		\begin{split}
		\widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx) &=\underline{\lambda}\BIx+ {\displaystyle\argmax_{\widetilde{\BIg}\in\R^d}}\Big\{\langle\BIx,\widetilde{\BIg}\rangle-\varphi^{(b)}_{\mathrm{UB}}(\widetilde{\BIg})-\textstyle\frac{1}{2(\overline{\lambda} - \underline{\lambda})}\|\widetilde{\BIg}\|^2\Big\} \qquad\forall\BIx\in\R^d.
		\end{split}
	\end{align}
	For every $\BIx\in\R^d$, since the maximization in (\ref{eqn: convex-leastsquares-LB-proof-gradient-alternative}) is equivalent to the minimization of a $\frac{1}{\overline{\lambda}-\underline{\lambda}}$-strongly convex function over $\Delta$, a unique optimizer $\widetilde{\BIg}^\star\in\R^d$ is attained. 
	This $\widetilde{\BIg}^\star$ also satisfies $\varphi^{(d)}_{\mathrm{LB}}(\BIx)=\langle\widetilde{\BIg}^\star,\BIx\rangle-\varphi^{(c)}_{\mathrm{UB}}(\widetilde{\BIg}^\star)$, which implies that $\widetilde{\BIg}^\star=\nabla \varphi^{(d)}_{\mathrm{LB}}(\BIx)$ (recall that $\varphi^{(d)}_{\mathrm{LB}}\in\FC_{0,\overline{\lambda}-\underline{\lambda}}(\R^d)$ is the convex conjugate of $\varphi^{(c)}_{\mathrm{UB}}$). 
	Consequently, $\widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx)=\underline{\lambda}\BIx+\widetilde{\BIg}^\star=\nabla\big(\varphi^{(d)}_{\mathrm{LB}}(\BIx)+\frac{\underline{\lambda}}{2}\|\BIx\|^2\big)=\nabla\varphi^{(e)}_{\mathrm{LB}}(\BIx)=\nabla\widehat{\varphi}_{\mathrm{CLS\text{-}LB}}(\BIx)$ for all $\BIx\in\R^d$. 
	The proof is now complete.
\end{proof}

In summary, Definition~\ref{def: convex-leastsquares} and Theorem~\ref{thm: convex-leastsquares-LB} have provided a computationally tractable plug-in OT map estimator $\widehat{T}_{\mathrm{CLS\text{-}LB}}$, and Theorem~\ref{thm: plugin-estimation-error} has shown the consistency of $\widehat{T}_{\mathrm{CLS\text{-}LB}}$ as well as its estimation error bound.
However, $\widehat{T}_{\mathrm{CLS\text{-}LB}}$ does not satisfy Assumption~\ref{asp: plugin-estimator} since the condition $\widehat{\varphi}_{\mathrm{CLS\text{-}LB}}\in\CC^{2,\alpha}_{\mathrm{loc}}(\R^d)$ does not hold in general. 
In the next two subsections, we will introduce two smoothing procedures for $\widehat{T}_{\mathrm{CLS\text{-}LB}}$ to build plug-in OT map estimators that satisfy Assumption~\ref{asp: plugin-estimator}.


\subsection{Kernel-smoothed OT map estimator}
\label{ssec: estimators-kernelsmoothed}

Let us now introduce our first concrete example of plug-in OT map estimators named kernel-smoothed convex least squares estimator denoted by $\widehat{T}_{\mathrm{kern}}(\,\cdot\,;\theta)$, which satisfies Assumption~\ref{asp: plugin-estimator}.
The idea of $\widehat{T}_{\mathrm{kern}}(\,\cdot\,;\theta)$ is to convolute $\widehat{T}_{\mathrm{CLS\text{-}LB}}$ in Theorem~\ref{thm: convex-leastsquares-LB} with an infinitely differentiable kernel in order to make it satisfy the differentiability condition in Assumption~\ref{asp: plugin-estimator}\ref{asps: plugin-estimator-shape}. 
\TODO{[To be discussed: theorem or proposition?]}

\begin{theorem}[Kernel-smoothed convex least squares OT map estimator]
	\label{thm: estimator-kernel}
	Let the settings of Definition~\ref{def: convex-leastsquares} hold. 
Replacing $\underline{\lambda}(\mu,\nu)$ with $\min\big\{\underline{\lambda}(\mu,\nu),1\big\}$ if necessary, let us assume without loss of generality that $\underline{\lambda}\equiv\underline{\lambda}(\mu,\nu)\le 1$.
Let $(\widehat{\pi}^\star_{i,j})_{i=1:m,\,j=1:n}$, $(\widetilde{\varphi}^\star_i)_{i=1:m}$, and $(\widetilde{\BIg}^\star_i)_{i=1:m}$ be defined via (\ref{eqn: estimator-discreteOT}) and (\ref{eqn: estimator-QCQPregression}), and let $\overline{u}_0(\nu):=\inf\big\{r\in\R_+: \support(\nu)\subseteq \bar{B}(\veczero, r)\big\}$.
Moreover, let $\Delta$, $\widetilde{\BG}^\star$, and $\BIv$ be defined via (\ref{eqn: convex-leastsquares-LB-notations}) and let $\widehat{\varphi}_{\mathrm{CLS\text{-}LB}}$, $\widehat{T}_{\mathrm{CLS\text{-}LB}}$ be defined as in Theorem~\ref{thm: convex-leastsquares-LB}. 
For every $\theta\in\N$, let $\Psi_{\theta}\in\CC^{\infty}(\R^d)$ be the density function of a $d$-dimensional probability measure with mean $\veczero_d$ and covariance matrix $\frac{1}{\theta^2}\BI_d$, and let $\widehat{\varphi}_{\mathrm{kern}}(\,\cdot\,;\theta):\R^d\to\R$ and $\widehat{T}_{\mathrm{kern}}(\,\cdot\,;\theta):\R^d\to\R^d$ be defined as follows (the $\R^d$-valued integral in (\ref{eqn: estimator-kernel-gradient}) is evaluated entry-wise):
\begin{align}
	\label{eqn: estimator-kernel}
	\begin{split}
	\widehat{\varphi}_{\mathrm{kern}}(\BIx;\theta)&:=
	\int_{\R^d}\Psi_{\theta}(\BBeta)\widehat{\varphi}_{\mathrm{CLS\text{-}LB}}(\BIx-\BBeta) \DIFFX{\BBeta} \qquad \forall \BIx\in\R^d,
	\end{split}\\
	\begin{split}
		\widehat{T}_{\mathrm{kern}}(\BIx;\theta)&:= \int_{\R^d}\Psi_{\theta}(\BBeta)\widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx-\BBeta) \DIFFX{\BBeta} \hspace{1pt}\qquad\forall \BIx\in\R^d.
	\end{split}
	\label{eqn: estimator-kernel-gradient}
\end{align}
Then, the following statements hold.
\begin{enumerate}[label=(\roman*), beginpenalty=10000]
	\item\label{thms: estimator-kernel-shape}
	For all $m,n,\theta\in\N$, $\widehat{T}_{\mathrm{kern}}(\BIx;\theta)=\nabla\widehat{\varphi}_{\mathrm{kern}}(\BIx;\theta)$ for all $\BIx\in\R^d$ and $\widehat{\varphi}_{\mathrm{kern}}(\,\cdot\,;\theta)\in\FC^{\infty}_{\underline{\lambda},\overline{\lambda}}(\R^d)$. 
	
	\item\label{thms: estimator-kernel-growth}
	For all $m,n,\theta\in\N$, 
	it holds $\PROB$-almost surely that $\big\|\widehat{T}_{\mathrm{kern}}(\BIx;\theta)\big\|^2\le 18\overline{u}_0(\nu)^2 + 2\|\BIx\|^2$ for all $\BIx\in\R^d$.
	
	\item\label{thms: estimator-kernel-consistency}
	For all $m,n,\theta\in\N$, 
	it holds $\PROB$-almost surely that $\big\|\widehat{T}_{\mathrm{kern}}(\BIx;\theta)-\widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx)\big\|^2\le \frac{(\overline{\lambda}-\underline{\lambda})^2d}{\theta^2}$ for all $\BIx\in\R^d$. 

	\item\label{thms: estimator-kernel-admissibility}
	In particular, $\widehat{T}_{\mathrm{kern}}(\,\cdot\,;\theta)$ satisfies Assumption~\ref{asp: plugin-estimator} with respect to $u_1(\nu):=18\overline{u}_0(\nu)^2$, $u_2(\nu):=2$, $\overline{n}(\mu,\nu,\epsilon):=\min\big\{n\in\N:C\big(\mu,\nu,\underline{\lambda}(\mu,\nu),\overline{\lambda}(\mu,\nu)\big)\log(n)^2\kappa(n)\le \frac{\epsilon}{4}\big\}$, and $\overline{\theta}(\mu,\nu,m,n,\epsilon):=\Big\lceil\big(\frac{4d}{\epsilon}\big)^{\frac{1}{2}}(\overline{\lambda}(\mu,\nu)-\underline{\lambda}(\mu,\nu))\Big\rceil$, where $C(\,\cdot\,,\cdot\,,\cdot\,,\cdot\,)$ and $\kappa(\cdot)$ are given by Theorem~\ref{thm: plugin-estimation-error}.
\end{enumerate}
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm: estimator-kernel}]
	Throughout this proof, let us fix arbitrary $m,n,\theta\in\N$ and let us denote $\widetilde{\varphi}_{\mathrm{CLS\text{-}LB}}(\BIx):=\widehat{\varphi}_{\mathrm{CLS\text{-}LB}}(\BIx)-\frac{\underline{\lambda}}{2}\|\BIx\|^2$, $\widetilde{T}_{\mathrm{CLS\text{-}LB}}(\BIx):=\widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx)-\underline{\lambda}\BIx$, $\widetilde{\varphi}_{\mathrm{kern}}(\BIx;\theta):=\int_{\R^d}\Psi_{\theta}(\BBeta)\widetilde{\varphi}_{\mathrm{CLS\text{-}LB}}(\BIx-\BBeta)\DIFFX{\BBeta}$, $\widetilde{T}_{\mathrm{kern}}(\BIx;\theta):=\int_{\R^d}\Psi_{\theta}(\BBeta)\widetilde{T}_{\mathrm{CLS\text{-}LB}}(\BIx-\BBeta)\DIFFX{\BBeta}$ for all $\BIx\in\R^d$.
	Observe that $\widehat{\varphi}_{\mathrm{kern}}(\BIx;\theta)=\widetilde{\varphi}_{\mathrm{kern}}(\BIx;\theta)+\frac{\underline{\lambda}}{2}\|\BIx\|^2+\frac{\underline{\lambda}d}{2\theta^2}$ and 
	$\widehat{T}_{\mathrm{kern}}(\BIx;\theta)=\widetilde{T}_{\mathrm{kern}}(\BIx;\theta) + \underline{\lambda}\BIx$ for all $\BIx\in\R^d$. 

	To prove statement~\ref{thms: estimator-kernel-shape}, note that the proof of Theorem~\ref{thm: convex-leastsquares-LB} has shown that $\widetilde{T}_{\mathrm{CLS\text{-}LB}}=\nabla\widetilde{\varphi}_{\mathrm{CLS\text{-}LB}}$ and that $\widetilde{T}_{\mathrm{CLS\text{-}LB}}$ is $(\overline{\lambda}-\underline{\lambda})$-Lipschitz continuous. 
	Consequently, we may apply the Leibniz integral rule to exchange integration and differentiation as follows:
	\begin{align*}
		\nabla\widetilde{\varphi}_{\mathrm{kern}}(\BIx;\theta)=\int_{\R^d}\Psi_{\theta}(\BBeta)\nabla\widetilde{\varphi}_{\mathrm{CLS\text{-}LB}}(\BIx-\BBeta)\DIFFX{\BBeta}=\int_{\R^d}\Psi_{\theta}(\BBeta)\widetilde{T}_{\mathrm{CLS\text{-}LB}}(\BIx-\BBeta)\DIFFX{\BBeta}=\widetilde{T}_{\mathrm{kern}}(\BIx;\theta) \;\;\;\forall\BIx\in\R^d.
	\end{align*}
	Thus, it holds that $\widehat{T}_{\mathrm{kern}}(\BIx;\theta)=\widetilde{T}_{\mathrm{kern}}(\BIx;\theta)+\underline{\lambda}\BIx=\nabla\big(\widetilde{\varphi}_{\mathrm{kern}}(\BIx;\theta)+\frac{\underline{\lambda}}{2}\|\BIx\|^2+\frac{\underline{\lambda}d}{2\theta^2}\big)=\nabla\widehat{\varphi}_{\mathrm{kern}}(\BIx;\theta)$. 
	Moreover, since $\widetilde{\varphi}_{\mathrm{kern}}$ is the convolution of $\widetilde{\varphi}_{\mathrm{CLS\text{-}LB}}$ with $\Psi_{\theta}\in\CC^\infty(\R^d)$, it holds that $\widetilde{\varphi}_{\mathrm{kern}}\in\CC^{\infty}(\R^d)$.
	Furthermore, observe that $\widetilde{\varphi}_{\mathrm{kern}}$ is convex due to the convexity of $\widetilde{\varphi}_{\mathrm{CLS\text{-}LB}}$, and that 
	\begin{align*}
		\big\|\widetilde{T}_{\mathrm{kern}}(\BIx;\theta)-\widetilde{T}_{\mathrm{kern}}(\BIy;\theta)\big\|\le \int_{\R^d} \big\|\widetilde{T}_{\mathrm{CLS\text{-}LB}}(\BIx)-\widetilde{T}_{\mathrm{CLS\text{-}LB}}(\BIy)\big\| \Psi_{\theta}(\BBeta)  \DIFFX{\BBeta} \le (\overline{\lambda}-\underline{\lambda})\|\BIx-\BIy\| \quad\forall \BIx,\BIy\in\R^d,
	\end{align*}
	which show that $\widetilde{\varphi}_{\mathrm{kern}}\in\FC^{\infty}_{0,\overline{\lambda}-\underline{\lambda}}(\R^d)$.
	We thus conclude that $\widehat{\varphi}_{\mathrm{kern}}\in\FC^{\infty}_{\underline{\lambda},\overline{\lambda}}(\R^d)$. The proof of statement~\ref{thms: estimator-kernel-shape} is now complete. 

	To prove statement~\ref{thms: estimator-kernel-growth}, observe from (\ref{eqn: convex-leastsquares-LB-gradient}) that 
	\begin{align}
		\label{eqn: estimator-kernel-proof-growth-convexhull}
		\widetilde{T}_{\mathrm{CLS\text{-}LB}}(\BIx)\in\conv\big(\{\widetilde{\BIg}^\star_1,\ldots,\widetilde{\BIg}^\star_m\}\big) \qquad\forall \BIx\in\R^d.
	\end{align}
	Moreover, since $(\widetilde{\varphi}^\star_i)_{i=1:m}$, $(\widetilde{\BIg}^\star_i)_{i=1:m}$ is an optimizer of the QCQP problem (\ref{eqn: estimator-QCQPregression}), it follows from the constraints in (\ref{eqn: estimator-QCQPregression}) that
	\begin{align*}
		\max_{1\le i\le m}\big\{\|\widetilde{\BIg}^\star_i\|\big\} &\le \max_{1\le i\le m}\big\{\|\widetilde{\BIg}^\star_i+\underline{\lambda}\BIX_i\|+\underline{\lambda}\|\BIX_i\|\big\}\le \overline{u}_0(\nu)+\underline{\lambda}\max_{1\le i\le m}\big\{\|\BIX_i\|\big\}.
	\end{align*}
	Thus, we have
	\begin{align}
		\label{eqn: estimator-kernel-proof-growth-1}
		\max_{1\le i\le m}\big\{\|\widetilde{\BIg}^\star_i\|\big\}\le\overline{u}_0(\nu) + \underline{\lambda}\sup_{\BIy\in\support(\mu)}\big\{\|\BIy\|\big\} \qquad \PROB\text{-a.s.}
	\end{align}
	Furthermore, since $\varphi^{\mu}_{\nu}$ is $\underline{\lambda}$-strongly convex by assumption, it holds for all $\BIy\in\support(\mu)$ that
	\begin{align*}
		\varphi^{\mu}_{\nu}(\BIy)-\varphi^{\mu}_{\nu}(\veczero_d)-\langle T^{\mu}_{\nu}(\veczero_d),\BIy\rangle &\ge \frac{\underline{\lambda}}{2}\|\BIy\|^2, \\
		\varphi^{\mu}_{\nu}(\veczero_d)-\varphi^{\mu}_{\nu}(\BIy)+\langle T^{\mu}_{\nu}(\BIy),\BIy\rangle &\ge \frac{\underline{\lambda}}{2}\|\BIy\|^2.
	\end{align*}
	Adding both sides of the two inequalities, applying the Cauchy--Schwarz inequality, and using the assumption $\veczero_d\in\support(\mu)$ in Definition~\ref{def: convex-leastsquares} yields 
	\begin{align}
		\label{eqn: estimator-kernel-proof-growth-2}
		\underline{\lambda}\|\BIy\|\le \big\|T^{\mu}_{\nu}(\BIy)-T^{\mu}_{\nu}(\veczero_d)\big\|\le 2 \sup_{\BIz\in\support(\nu)}\big\{\|\BIz\|\big\}\le 2\overline{u}_0(\nu) \qquad\forall \BIy\in\support(\mu).
	\end{align}
	Combining (\ref{eqn: estimator-kernel-proof-growth-1}) and (\ref{eqn: estimator-kernel-proof-growth-2}) yields
	\begin{align}
		\label{eqn: estimator-kernel-proof-growthineq}
		\max_{1\le i\le m}\big\{\|\widetilde{\BIg}^\star_i\|\big\}\le 3\overline{u}_0(\nu) \qquad\PROB\text{-a.s.}
	\end{align}
	Subsequently, Jensen's inequality together with the convexity of $\R^d\ni\BIx\mapsto\|\BIx\|^2\in\R$, (\ref{eqn: estimator-kernel-proof-growth-convexhull}), (\ref{eqn: estimator-kernel-proof-growthineq}), and the assumption $\underline{\lambda}\le 1$ imply that
	\begin{align*}
		\big\|\widehat{T}_{\mathrm{kern}}(\BIx;\theta)\big\|^2 &\le 2\big\|\widetilde{T}_{\mathrm{kern}}(\BIx,\theta)\big\|^2 + 2\underline{\lambda}^2\|\BIx\|^2 \\
		&\le 2\int_{\R^d} \big\|\widetilde{T}_{\mathrm{CLS\text{-}LB}}(\BIx-\BBeta)\big\|^2\Psi_{\theta}(\BBeta) \DIFFX{\BBeta} + 2\underline{\lambda}^2\|\BIx\|^2 \\
		&\le 2\max_{1\le i\le m}\big\{\|\widetilde{\BIg}^\star_i\|\big\}^2 + 2\|\BIx\|^2\\
		&\le 18 \overline{u}_0(\nu)^2 + 2 \|\BIx\|^2 \qquad \PROB\text{-a.s.}
	\end{align*}
	We have thus completed the proof of statement~\ref{thms: estimator-kernel-growth}.

	Statement~\ref{thms: estimator-kernel-consistency} follows from the convexity of $\R^d\ni\BIx\mapsto\|\BIx\|^2\in\R$, Jensen's inequality, the $(\overline{\lambda}-\underline{\lambda})$-Lipschitz continuity of $\widetilde{T}_{\mathrm{CLS\text{-}LB}}$, and the assumption that $\Psi_{\theta}$ is the density function of a distribution with mean $\veczero_d$ and covariance $\frac{1}{\theta^2}\BI_d$:
	\begin{align*}
		\big\|\widehat{T}_{\mathrm{kern}}(\BIx;\theta)-\widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx)\big\|^2&=\big\|\widetilde{T}_{\mathrm{kern}}(\BIx;\theta)-\widetilde{T}_{\mathrm{CLS\text{-}LB}}(\BIx)\big\|^2\\
		&=\bigg\|\int_{\R^d}\big(\widetilde{T}_{\mathrm{CLS\text{-}LB}}(\BIx-\BBeta)-\widetilde{T}_{\mathrm{CLS\text{-}LB}}(\BIx)\big)\Psi_{\theta}(\BBeta)\DIFFX{\BBeta}\bigg\|^2 \\
		&\le \int_{\R^d} \big\|\widetilde{T}_{\mathrm{CLS\text{-}LB}}(\BIx-\BBeta)-\widetilde{T}_{\mathrm{CLS\text{-}LB}}(\BIx)\big\|^2 \Psi_{\theta}(\BBeta)\DIFFX{\BBeta} \\
		&\le \int_{\R^d} (\overline{\lambda}-\underline{\lambda})^2 \|\BBeta\|^2 \Psi_{\theta}(\BBeta)\DIFFX{\BBeta}\\
		&=\frac{(\overline{\lambda}-\underline{\lambda})^2d}{\theta^2} \qquad\qquad\qquad\forall \BIx\in\R^d.
	\end{align*}
	
	Finally, observe that statement~\ref{thms: estimator-kernel-shape} shows that $\widehat{T}_{\mathrm{kern}}(\,\cdot\,;\theta)$ satisfies the shape condition in Assumption~\ref{asp: plugin-estimator}\ref{asps: plugin-estimator-shape}.
	Moreover, statement~\ref{thms: estimator-kernel-growth} implies that $\EXP\Big[\big\|\widehat{T}_{\mathrm{kern}}(\BIx;\theta)\big\|^2\Big]\le 18\overline{u}_0(\nu)^2+2\|\BIx\|^2$ for all $\BIx$, which shows that $\widehat{T}_{\mathrm{kern}}(\,\cdot\,;\theta)$ satisfies the growth condition in Assumption~\ref{asp: plugin-estimator}\ref{asps: plugin-estimator-growth} with respect to $u_1(\nu):=18\overline{u}_0(\nu)^2$, $u_2(\nu):=2$.
	Furthermore, statement~\ref{thms: estimator-kernel-consistency} shows that
	\begin{align*}
		\big\|\widehat{T}_{\mathrm{kern}}(\BIx;\theta)-T^{\mu}_{\nu}(\BIx)\big\|^2 &\le 2\big\|\widehat{T}_{\mathrm{kern}}(\BIx;\theta)-\widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx)\big\|^2 + 2\big\|\widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx)-T^{\mu}_{\nu}(\BIx)\big\|^2 \\
		&\le \frac{2(\overline{\lambda}(\mu,\nu)-\underline{\lambda}(\mu,\nu))^2d}{\theta^2} + 2\big\|\widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx)-T^{\mu}_{\nu}(\BIx)\big\|^2 \qquad\qquad\forall\BIx\in\R^d.
	\end{align*}
	Combining this with Theorem~\ref{thm: convex-leastsquares-LB}, Theorem~\ref{thm: shape-constraints}, and Theorem~\ref{thm: plugin-estimation-error} leads to
	\begin{align*}
		\EXP\Big[\big\|\widehat{T}_{\mathrm{kern}}(\,\cdot\,;\theta)-T^{\mu}_{\nu}\big\|_{\CL^2(\mu)}^2\Big] &\le \frac{2(\overline{\lambda}(\mu,\nu)-\underline{\lambda}(\mu,\nu))^2d}{\theta^2}\\
		&\qquad+ 2C\big(\mu,\nu,\underline{\lambda}(\mu,\nu),\overline{\lambda}(\mu,\nu)\big)\log(m)^2\kappa\big(\min\{m,n\}\big).
	\end{align*}
	Consequently, for any $\epsilon>0$, with $\overline{n}(\mu,\nu,\epsilon):=\min\big\{n\in\N:C\big(\mu,\nu,\underline{\lambda}(\mu,\nu),\overline{\lambda}(\mu,\nu)\big)\log(n)^2\kappa(n)\le \frac{\epsilon}{4}\big\}$, $\overline{\theta}(\mu,\nu,m,n,\epsilon):=\Big\lceil\big(\frac{4d}{\epsilon}\big)^{\frac{1}{2}}(\overline{\lambda}(\mu,\nu)-\underline{\lambda}(\mu,\nu))\Big\rceil$, 
	it holds for all $m\ge \overline{n}(\mu,\nu,\epsilon)$, $n\ge\overline{n}(\mu,\nu,\epsilon)$ and all $\theta\ge \overline{\theta}(\mu,\nu,m,n,\epsilon)$ that
	\begin{align*}
		&\EXP\Big[\big\|\widehat{T}_{\mathrm{kern}}(\,\cdot\,;\theta)-T^{\mu}_{\nu}\big\|_{\CL^2(\mu)}^2\Big] \\
		&\qquad \le \frac{2(\overline{\lambda}(\mu,\nu)-\underline{\lambda}(\mu,\nu))^2d}{\theta^2}+ 2C\big(\mu,\nu,\underline{\lambda}(\mu,\nu),\overline{\lambda}(\mu,\nu)\big)\log(m)^2\kappa\big(\min\{m,n\}\big) \\
		&\qquad \le \frac{2(\overline{\lambda}(\mu,\nu)-\underline{\lambda}(\mu,\nu))^2d}{\frac{4d}{\epsilon}(\overline{\lambda}(\mu,\nu)-\underline{\lambda}(\mu,\nu))^2}+ 2C\big(\mu,\nu,\underline{\lambda}(\mu,\nu),\overline{\lambda}(\mu,\nu)\big)\log\big(\overline{n}(\mu,\nu,\epsilon)\big)^2\kappa\big(\overline{n}(\mu,\nu,\epsilon)\big)\\
		&\qquad  \le \frac{\epsilon}{2} + \frac{\epsilon}{2}=\epsilon.
	\end{align*}
	We have thus shown that $\widehat{T}_{\mathrm{kern}}(\,\cdot\,;\theta)$ satisfies the consistency condition in Assumption~\ref{asp: plugin-estimator}\ref{asps: plugin-estimator-consistency} with respect to $\overline{n}(\mu,\nu,\epsilon):=\min\big\{n\in\N:C\big(\mu,\nu,\underline{\lambda}(\mu,\nu),\overline{\lambda}(\mu,\nu)\big)\log(n)^2\kappa(n)\le \frac{\epsilon}{4}\big\}$ and $\overline{\theta}(\mu,\nu,m,n,\epsilon):=\Big\lceil\big(\frac{4d}{\epsilon}\big)^{\frac{1}{2}}(\overline{\lambda}(\mu,\nu)-\underline{\lambda}(\mu,\nu))\Big\rceil$. 
	The proof is now complete. 
\end{proof}


\begin{remark}[Computational tractability of $\widehat{T}_{\mathrm{kern}}(\,\cdot\,;\theta)$]
	\label{rmk: estimator-tractability-kernel}
	The computation of $\widehat{T}_{\mathrm{kern}}(\,\cdot\,;\theta)$ is done in two phases. 
	In the first phase, given the $m$~samples $\BIX_1,\ldots,\BIX_m$ from $\mu$ and the $n$~samples $\BIY_1,\ldots,\BIY_n$ from $\nu$, one computes $(\widetilde{\varphi}_i^\star)_{i=1:m}$ and $(\widetilde{\BIg}_i^\star)_{i=1:m}$ by first solving the LP problem (\ref{eqn: estimator-discreteOT}) and then solving the QCQP problem (\ref{eqn: estimator-QCQPregression}); see Definition~\ref{def: convex-leastsquares}. 
	These two problems can be solved by state-of-the-art LP and QCQP solvers such as Gurobi~\citep{gurobi}.
	Subsequently, in the second phase, for every $\BIx\in\R^d$ at which $\widehat{T}_{\mathrm{kern}}(\,\cdot\,;\theta)$ needs to be evaluated, one can compute $\widehat{T}_{\mathrm{kern}}(\BIx;\theta)$ by Monte Carlo approximation. 
	Specifically, one generates $S\in\N$ independent random samples $\{\BBeta_{[s]}\}_{s=1:S}$ from the probability distribution whose density function is equal to $\Psi_\theta$, and then approximates $\widehat{T}_{\mathrm{kern}}(\BIx;\theta)$ by $\widehat{T}_{\mathrm{kern}}(\BIx;\theta)\approx \frac{1}{S} \sum_{s=1}^S \widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx-\BBeta_{[s]})$, where each $\widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx-\BBeta_{[s]})$ is computed by solving the QP problem in (\ref{eqn: convex-leastsquares-LB-gradient}). 
	This can be done using state-of-the-art QP solvers, which is also provided by Gurobi~\citep{gurobi}. 
	Consequently, using $\widehat{T}_{\mathrm{kern}}(\,\cdot\,;\theta)$ as the plug-in OT map estimator in Algorithm~\ref{algo: iteration} results in a computationally tractable algorithm for $\CW_2$-barycenter that is also provably convergent. 
\end{remark}


\subsection{Barrier-based OT map estimator}
\label{ssec: estimators-barrier}

In this subsection, we introduce the following barrier-based convex least squares estimator $\widehat{T}_{\mathrm{barr}}(\,\cdot\,;\theta)$ inspired by the smoothing technique of \citet{nesterov2005smooth}. \TODO{[To discuss: theorem or proposition?]}

\begin{theorem}[Barrier-based convex least squares OT map estimator]
\label{thm: estimator-barrier}
Let the settings of Definition~\ref{def: convex-leastsquares} hold. 
Replacing $\underline{\lambda}(\mu,\nu)$ with $\min\big\{\underline{\lambda}(\mu,\nu),1\big\}$ if necessary, let us assume without loss of generality that $\underline{\lambda}\equiv\underline{\lambda}(\mu,\nu)\le 1$.
Let $(\widehat{\pi}^\star_{i,j})_{i=1:m,\,j=1:n}$, $(\widetilde{\varphi}^\star_i)_{i=1:m}$, and $(\widetilde{\BIg}^\star_i)_{i=1:m}$ be defined via (\ref{eqn: estimator-discreteOT}) and (\ref{eqn: estimator-QCQPregression}), and let $\overline{u}_0(\nu):=\inf\big\{r\in\R_+: \support(\nu)\subseteq \bar{B}(\veczero, r)\big\}$.
Moreover, let $\Delta$, $\widetilde{\BG}^\star$, and $\BIv$ be defined via (\ref{eqn: convex-leastsquares-LB-notations}) and let $\eta:\Delta\to\R_+$ be given by $\eta(w_1,\ldots,w_m):=-\sum_{i=1}^m\log(w_i)$.
For every $\theta\in\N$, let $\widehat{\varphi}_{\mathrm{barr}}(\,\cdot\,;\theta):\R^d\to\R$ and $\widehat{T}_{\mathrm{barr}}(\,\cdot\,;\theta):\R^d\to\R^d$ be defined as follows:
\begin{align}
	\label{eqn: estimator-barrier}
	\widehat{\varphi}_{\mathrm{barr}}(\BIx;\theta)&:= {\textstyle\frac{\underline{\lambda}}{2}}\|\BIx\|^2 + 
	\sup_{\BIw\in\Delta}\Big\{\langle\widetilde{\BG}^{\star\TRANSP}\BIx+\BIv,\BIw\rangle
	-\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\|\widetilde{\BG}^\star\BIw\|^2 -\frac{\eta(\BIw)}{\theta}  \Big\} \hspace{9pt} \qquad \forall \BIx\in\R^d, \\
	\begin{split}
		\widehat{T}_{\mathrm{barr}}(\BIx;\theta)&:= \underline{\lambda}\BIx+\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta), \\
		\text{where }& \widehat{\BIw}(\BIx; \theta):=\argmax_{\BIw\in\Delta}\Big\{\langle\widetilde{\BG}^{\star\TRANSP}\BIx+\BIv,\BIw\rangle
		-\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\|\widetilde{\BG}^\star\BIw\|^2 -\frac{\eta(\BIw)}{\theta}  \Big\} \qquad \forall \BIx\in\R^d.
	\end{split}
	\label{eqn: estimator-barrier-gradient}
\end{align}
Then, the following statements hold.
\begin{enumerate}[label=(\roman*), beginpenalty=10000]
	\item\label{thms: estimator-barrier-shape}
	For all $m,n,\theta\in\N$, $\widehat{\BIw}(\BIx;\theta)$ in (\ref{eqn: estimator-barrier-gradient}) is uniquely defined and $\widehat{T}_{\mathrm{barr}}(\BIx;\theta)=\nabla\widehat{\varphi}_{\mathrm{barr}}(\BIx;\theta)$ for all $\BIx\in\R^d$.
	Moreover, it holds that $\widehat{\varphi}_{\mathrm{barr}}(\,\cdot\,;\theta)\in\FC^{\infty}_{\underline{\lambda},\overline{\lambda}}(\R^d)$. 
	
	\item\label{thms: estimator-barrier-growth}
	For all $m,n,\theta\in\N$, 
	it holds $\PROB$-almost surely that $\big\|\widehat{T}_{\mathrm{barr}}(\BIx;\theta)\big\|^2\le 18\overline{u}_0(\nu)^2 + 2\|\BIx\|^2$ for all $\BIx\in\R^d$.
	
	\item\label{thms: estimator-barrier-consistency}
	For all $m,n,\theta\in\N$, 
	it holds $\PROB$-almost surely that $\big\|\widehat{T}_{\mathrm{barr}}(\BIx;\theta)-\widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx)\big\|^2\le \frac{2 m (\overline{\lambda}-\underline{\lambda})}{\theta}$ for all $\BIx\in\R^d$, where $\widehat{T}_{\mathrm{CLS\text{-}LB}}$ is defined in Theorem~\ref{thm: convex-leastsquares-LB}. 

	\item\label{thms: estimator-barrier-admissibility}
	In particular, $\widehat{T}_{\mathrm{barr}}(\,\cdot\,;\theta)$ satisfies Assumption~\ref{asp: plugin-estimator} with respect to $u_1(\nu):=18\overline{u}_0(\nu)^2$, $u_2(\nu):=2$, $\overline{n}(\mu,\nu,\epsilon):=\min\big\{n\in\N:C\big(\mu,\nu,\underline{\lambda}(\mu,\nu),\overline{\lambda}(\mu,\nu)\big)\log(n)^2\kappa(n)\le \frac{\epsilon}{4}\big\}$, and $\overline{\theta}(\mu,\nu,m,n,\epsilon):=\big\lceil\frac{8m}{\epsilon}(\overline{\lambda}(\mu,\nu)-\underline{\lambda}(\mu,\nu))\big\rceil$, where $C(\,\cdot\,,\cdot\,,\cdot\,,\cdot\,)$ and $\kappa(\cdot)$ are given by Theorem~\ref{thm: plugin-estimation-error}.
\end{enumerate}
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm: estimator-barrier}]
	Throughout this proof, let us fix arbitrary $m,n,\theta\in\N$. 
	To begin, observe that we can extend the definition of $\eta$ to $\R^d$ such that $\eta\in\FC_{1,\infty}(\R^d)$. 
	To prove statement~\ref{thms: estimator-barrier-shape}, let us define $\psi^{(a)}_\theta:\R^d\to\R$ and $\psi^{(b)}_{\theta}:\R^d\to\R\cup\{\infty\}$ as follows:
	\begin{align*}
		\psi^{(a)}_\theta(\BIx)&:=\sup_{\BIw\in\Delta}\big\{\langle\widetilde{\BG}^{\star\TRANSP}\BIx+\BIv,\BIw\rangle-{\textstyle\frac{\eta(\BIw)}{\theta}}\big\}  && \forall\BIx\in\R^d,\\
		\psi^{(b)}_{\theta}(\widetilde{\BIg})&:=\sup_{\BIx\in\R^d}\big\{\langle\widetilde{\BIg},\BIx\rangle-\psi^{(a)}_{\theta}(\BIx)\big\}\\
		&\phantom{:}=\sup_{\BIx\in\R^d}\Big\{\inf_{\BIw\in\Delta}\Big\{\langle\widetilde{\BIg}-\widetilde{\BG}^\star\BIw,\BIx\rangle-\langle\BIv,\BIw\rangle+{\textstyle\frac{\eta(\BIw)}{\theta}}\Big\}\Big\} && \forall \widetilde{\BIg}\in\R^d.
	\end{align*}
	Since $\R^d\times\Delta\ni(\BIx,\BIw)\mapsto \langle\widetilde{\BIg}-\widetilde{\BG}^\star\BIw,\BIx\rangle-\langle\BIv,\BIw\rangle+\frac{\eta(\BIw)}{\theta}\in\R$ is concave in $\BIx$ for every $\BIw\in\Delta$ and convex in $\BIw$ for every $\BIx\in\R^d$, it follows from the compactness of $\Delta$ and Sion's minimax theorem \citep{sion1958on} that 
	\begin{align*}
		\psi^{(b)}_{\theta}(\widetilde{\BIg})&=\sup_{\BIx\in\R^d}\Big\{\inf_{\BIw\in\Delta}\Big\{\langle\widetilde{\BIg}-\widetilde{\BG}^\star\BIw,\BIx\rangle-\langle\BIv,\BIw\rangle+{\textstyle\frac{\eta(\BIw)}{\theta}}\Big\}\Big\}\\
		&=\inf_{\BIw\in\Delta}\left\{\sup_{\BIx\in\R^d}\big\{\langle\widetilde{\BIg}-\widetilde{\BG}^\star\BIw,\BIx\rangle\big\}-\langle\BIv,\BIw\rangle+{\textstyle\frac{\eta(\BIw)}{\theta}}\right\}\\
		&=-\sup_{\BIw\in\Delta,\,\widetilde{\BG}^\star\BIw=\widetilde{\BIg}}\Big\{\langle\BIv,\BIw\rangle-{\textstyle\frac{\eta(\BIw)}{\theta}}\Big\} \hspace{89pt} \forall \widetilde{\BIg}\in\R^d.
	\end{align*}
	We have $\psi^{(b)}_{\theta}\in\FC_{0,\infty}(\R^d)$ by its definition.
	Subsequently, let us define $\psi^{(c)}_{\theta}:\R^d\to\R\cup\{\infty\}$ as follows:
	\begin{align*}
		\psi^{(c)}_{\theta}(\widetilde{\BIg}):=\psi^{(b)}_{\theta}(\widetilde{\BIg})+{\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}}\|\widetilde{\BIg}\|^2={\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}}\|\widetilde{\BIg}\|^2-\sup_{\BIw\in\Delta,\,\widetilde{\BG}^\star\BIw=\widetilde{\BIg}}\Big\{\langle\BIv,\BIw\rangle-{\textstyle\frac{\eta(\BIw)}{\theta}}\Big\} \qquad\forall \widetilde{\BIg}\in\R^d.
	\end{align*}
	Thus, $\psi^{(c)}_{\theta}\in\FC_{\frac{1}{\overline{\lambda}-\underline{\lambda}},\infty}(\R^d)$.
	Continuing on, let us define $\psi^{(d)}_{\theta}:\R^d\to\R$ as the convex conjugate of $\psi^{(c)}_\theta$:
	\begin{align}
		\label{eqn: estimator-barrier-proof-interpolate-sm}
		\begin{split}
			\psi^{(d)}_{\theta}(\BIx)&:=\sup_{\widetilde{\BIg}\in\R^d}\big\{\langle\widetilde{\BIg},\BIx\rangle-\psi^{(c)}_{\theta}(\widetilde{\BIg})\big\}\\
			&\phantom{:}=\sup_{\widetilde{\BIg}\in\R^d,\,\BIw\in\Delta,\,\widetilde{\BG}^\star\BIw=\widetilde{\BIg}}\Big\{\langle\widetilde{\BIg},\BIx\rangle-{\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}}\|\widetilde{\BIg}\|^2+\langle\BIv,\BIw\rangle-{\textstyle\frac{\eta(\BIw)}{\theta}}\Big\}\\
			&\phantom{:}=\sup_{\BIw\in\Delta}\Big\{\langle\widetilde{\BG}^{\star\TRANSP}\BIx+\BIv,\BIw\rangle
			-\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\|\widetilde{\BG}^\star\BIw\|^2 - {\textstyle\frac{\eta(\BIw)}{\theta}} \Big\} \qquad\qquad\forall \BIx\in\R^d.
		\end{split}
	\end{align}
	It hence holds by the well-known duality between smooth convex functions and strongly convex functions (see, e.g., \citep[Proposition~12.60]{rockafellar1998variational}) that $\psi^{(d)}_{\theta}\in\FC_{0,\overline{\lambda}-\underline{\lambda}}(\R^d)$.
	Lastly, since $\widehat{\varphi}_{\mathrm{barr}}(\,\cdot\,;\theta)=\psi^{(d)}_{\theta}(\cdot)+\frac{\underline{\lambda}}{2}\|\cdot\|^2$, we get $\widehat{\varphi}_{\mathrm{barr}}(\,\cdot\,;\theta)\in\FC_{\underline{\lambda},\overline{\lambda}}(\R^d)$.  
	Moreover, for every $\BIx\in\R^d$, the maximization in (\ref{eqn: estimator-barrier-gradient}) is equivalent to the minimization of a $\frac{1}{\theta}$-strongly convex and l.s.c.\@ function over the compact set $\Delta$, and is hence always attained at a unique maximizer $\widehat{\BIw}(\BIx;\theta)$. 
	Observe from (\ref{eqn: estimator-barrier-proof-interpolate-sm}) that $\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)$ is a maximizer of $\sup_{\widetilde{\BIg}\in\R^d}\big\{\langle\widetilde{\BIg},\BIx\rangle-\psi^{(c)}_{\theta}(\widetilde{\BIg})\big\}$, and thus $\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)\in\partial\psi^{(d)}_{\theta}(\BIx)$. 
	Since $\psi^{(d)}_{\theta}\in\FC_{0,\overline{\lambda}-\underline{\lambda}}(\R^d)$ and $\overline{\lambda}-\underline{\lambda}<\infty$, it holds that $\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)=\nabla\psi^{(d)}_{\theta}(\BIx)$.
	Consequently, we get $\widehat{T}_{\mathrm{barr}}(\BIx;\theta):=\underline{\lambda}\BIx+\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)=\underline{\lambda}\BIx+\nabla\psi^{(d)}_{\theta}(\BIx)=\nabla\widehat{\varphi}_{\mathrm{barr}}(\BIx;\theta)$, which shows that $\widehat{T}_{\mathrm{barr}}(\,\cdot\,;\theta)=\nabla \widehat{\varphi}_{\mathrm{barr}}(\,\cdot\,;\theta)$.

	To complete the proof of statement~\ref{thms: estimator-barrier-shape}, it remains to show that $\widehat{\varphi}_{\mathrm{barr}}(\,\cdot\,;\theta)\in\CC^\infty(\R^d)$. 
	By denoting the entry-wise inverse of $\BIw$ by $\BIw^{\circ-1}\in\R^m$ and introducing Lagrange multiplier variables $\Bzeta\in\R^m_+$ and $\xi\in\R$, the Karush--Kuhn--Tucker (KKT) optimality conditions associated with the maximization problem in (\ref{eqn: estimator-barrier-gradient}) are given by:
	\begin{align*}
		\begin{cases}
			-{\textstyle\frac{1}{\theta}}\BIw^{\circ-1}+{\textstyle\frac{1}{\overline{\lambda}-\underline{\lambda}}}\widetilde{\BG}^{\star\TRANSP}\widetilde{\BG}^\star\BIw-\widetilde{\BG}^{\star\TRANSP}\BIx-\BIv-\Bzeta-\xi\vecone_m=\veczero_m,\\
			\BIw\ge\veczero_m, \\
			\langle\vecone_m,\BIw\rangle=1,\\
			\Bzeta\ge\veczero_m,\\ 
			\langle\Bzeta,\BIw\rangle=0,
		\end{cases}
	\end{align*}
	where $\vecone_m$ denotes the vector in $\R^m$ with all entries equal to~1. 
	For every $\BIx\in\R^d$, the log-barrier $\eta(\cdot)$ guarantees that the maximizer $\widehat{\BIw}(\BIx;\theta)$ of the maximization problem in (\ref{eqn: estimator-barrier-gradient}) must be in the relative interior of $\Delta$. 
	Hence, $\Bzeta=\veczero_m$ holds necessarily and the above KKT conditions can be simplified into the following system of equations: $\BIF(\BIx,\BIw,\xi)=\veczero_{m+1}$,
	where the vector-valued function $\BIF:\R^{d}\times(0,1)^{m}\times\R\to\R^{m+1}$ is defined by
	\begin{align}
		\label{eqn: estimator-barrier-proof-shape-KKTequation}
		\begin{split}
		\BIF(\BIx,\BIw,\xi)&:=\left(\begin{tabular}{c}
				$-{\textstyle\frac{1}{\theta}}\BIw^{\circ-1}+{\textstyle\frac{1}{\overline{\lambda}-\underline{\lambda}}}\widetilde{\BG}^{\star\TRANSP}\widetilde{\BG}^\star\BIw-\widetilde{\BG}^{\star\TRANSP}\BIx-\BIv-\xi\vecone_m$ \\
				$\langle\vecone_m,\BIw\rangle-1$
		\end{tabular}\right) \in \R^{m+1} \\
		& \hspace{170pt} \qquad\forall \BIx\in\R^d,\; \forall \BIw\in(0,1)^m, \; \forall \xi\in\R.
		\end{split}
	\end{align}
	Note that $\BIF$ is infinitely differentiable on $\R^{d}\times(0,1)^{m}\times\R$. 
	Since we have already shown that the maximizer $\widehat{\BIw}(\BIx;\theta)$ in (\ref{eqn: estimator-barrier-gradient}) is unique for every $\BIx\in\R^d$, and that $\BIF(\BIx,\BIw,\xi)=\veczero_{m+1}$ are the sufficient and necessary optimality conditions, the function $\widehat{\xi}(\,\cdot\,;\theta):\R^d\to\R$ defined below satisfies $F\big(\BIx,\widehat{\BIw}(\BIx;\theta),\widehat{\xi}(\BIx;\theta)\big)=\veczero_{m+1}$ for all $\BIx\in\R^d$:
	\begin{align*}
		\widehat{\xi}(\BIx;\theta)=-\textstyle\frac{1}{m\theta}\langle\widehat{\BIw}(\BIx;\theta)^{\circ-1},\vecone_m\rangle+\frac{1}{m(\overline{\lambda}-\underline{\lambda})}\langle\widetilde{\BG}^{\star\TRANSP}\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta),\vecone_m\rangle-\frac{1}{m}\langle\widetilde{\BG}^{\star\TRANSP}\BIx+\BIv,\vecone_m\rangle \quad\forall \BIx\in\R^d.
	\end{align*}
	Let $\nabla_{\BIx}\BIF(\BIx,\BIw,\xi)$ denote the partial derivative of $\BIF$ with respect to $\BIx$ and let $\nabla_{\BIw,\xi}\BIF(\BIx,\BIw,\xi)$ denote the partial derivative of $\BIF$ with respect to $(\BIw,\xi)$. 
	They are given by the following matrix-valued functions:
	\begin{align*}
		\nabla_{\BIx}\BIF(\BIx,\BIw,\xi)&=-\left(\begin{tabular}{c}
				$\widetilde{\BG}^\star$ \\
				$\veczero_d^\TRANSP$
		\end{tabular}\right) \in \R^{(m+1)\times d} \hspace{60pt} \qquad\forall \BIx\in\R^d,\; \forall \BIw\in(0,1)^m, \; \forall \xi\in\R,\\
		\nabla_{\BIw,\xi}\BIF(\BIx,\BIw,\xi)&=\left(\begin{tabular}{cc}
				$\frac{1}{\theta}\diag(\BIw)^{-2}+{\textstyle\frac{1}{\overline{\lambda}-\underline{\lambda}}}\widetilde{\BG}^{\star\TRANSP}\widetilde{\BG}^\star$ & $-\vecone_m$ \\
				$\vecone_m^\TRANSP$ & $0$
		\end{tabular}\right) \in \R^{(m+1)\times(m+1)}\\
		& \hspace{206pt}\forall \BIx\in\R^d,\; \forall \BIw\in(0,1)^m, \; \forall \xi\in\R.
	\end{align*}
	Note that since $\frac{1}{\theta}\diag(\BIw)^{-2}+{\textstyle\frac{1}{\overline{\lambda}-\underline{\lambda}}}\widetilde{\BG}^{\star\TRANSP}\widetilde{\BG}^{\star} \succeq \frac{1}{\theta}\BI_m$ for every $\BIw\in(0,1)^m$, we have $\det\Big(\frac{1}{\theta}\diag(\BIw)^{-2}+{\textstyle\frac{1}{\overline{\lambda}-\underline{\lambda}}}\widetilde{\BG}^{\star\TRANSP}\widetilde{\BG}^{\star}\Big)\ge \frac{1}{\theta^m}$.
	Consequently, we get via the determinant formula of block matrices that
	\begin{align*}
		\det\big(\nabla_{\BIw,\xi}\BIF(\BIx,\BIw,\xi)\big)&=\det\Big({\textstyle\frac{1}{\theta}}\diag(\BIw)^{-2}+{\textstyle\frac{1}{\overline{\lambda}-\underline{\lambda}}}\widetilde{\BG}^{\star\TRANSP}\widetilde{\BG}^\star\Big)\det\Big(\vecone_m^\TRANSP\Big({\textstyle\frac{1}{\theta}}\diag(\BIw)^{-2}+{\textstyle\frac{1}{\overline{\lambda}-\underline{\lambda}}}\widetilde{\BG}^{\star\TRANSP}\widetilde{\BG}^{\star}\Big)\vecone_m\Big)\\
		&\ge {\textstyle\frac{m}{\theta^{m+1}}}>0 \hspace{118pt} \qquad\forall \BIx\in\R^d,\; \forall \BIw\in(0,1)^m, \; \forall \xi\in\R.
	\end{align*}
	It thus follows from the implicit function theorem (see, e.g., \citep[Theorem~1B.1]{dontchev2009implicit}) that $\widehat{\BIw}(\,\cdot\,;\theta)$ and $\widehat{\xi}(\,\cdot\,;\theta)$ are both continuous, and that
	\begin{align*}
		\nabla\left(\!\!\begin{tabular}{c}
				$\widehat{\BIw}(\BIx;\theta)$ \\
				$\widehat{\xi}(\BIx;\theta)$
		\end{tabular}\!\!\right)&=-\big[\nabla_{\BIw,\xi}\BIF\big(\BIx,\widehat{\BIw}(\BIx;\theta),\widehat{\xi}(\BIx;\theta)\big)\big]^{-1}\nabla_{\BIx}\BIF\big(\BIx,\widehat{\BIw}(\BIx;\theta),\widehat{\xi}(\BIx;\theta)\big) \in \R^{(m+1)\times d} \\
		& \hspace{216pt} \forall \BIx\in\R^d,\; \forall \BIw\in(0,1)^m, \; \forall \xi\in\R.
	\end{align*}
	Therefore, it follows from the chain rule of differentiation, the infinite differentiability of $\BIF$, the continuity of $\widehat{\BIw}(\,\cdot\,;\theta)$, $\widehat{\xi}(\,\cdot\,;\theta)$, and an inductive argument that $\nabla\widehat{\BIw}(\,\cdot\,;\theta)$ is infinitely differentiable.  
	Since $\nabla\widehat{\varphi}_{\mathrm{barr}}(\BIx;\theta)=\widehat{T}_{\mathrm{barr}}(\BIx;\theta)=\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)+\underline{\lambda}\BIx$ for all $\BIx\in\R^d$, it holds that $\widehat{\varphi}_{\mathrm{barr}}(\,\cdot\,;\theta)\in\CC^{\infty}(\R^d)$.
	The proof of statement~\ref{thms: estimator-barrier-shape} is now complete. 

	Next, we fix an arbitrary $\BIx\in\R^d$ and prove statement~\ref{thms: estimator-barrier-growth}.
	Since $\widehat{\BIw}(\BIx;\theta)\in\Delta$, it holds that $\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)\in \conv\big(\{\tilde{\BIg}^\star_1,\ldots,\tilde{\BIg}^\star_n\}\big)$. 
	Hence, using (\ref{eqn: estimator-kernel-proof-growthineq}) and the assumption $\underline{\lambda}\le 1$, we get
	\begin{align*}
		\big\|\widehat{T}_{\mathrm{barr}}(\BIx;\theta)\big\|^2 = \big\|\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)+\underline{\lambda}\BIx\big\|^2 &\le 2\big\|\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)\big\|^2 + 2\underline{\lambda}^2\|\BIx\|^2 \\
		&\le 2\max_{1\le i\le m}\big\{\|\widetilde{\BIg}^\star_i\|\big\}^2 + 2\|\BIx\|^2\\
		&\le 18 \overline{u}_0(\nu)^2 + 2 \|\BIx\|^2 \qquad \PROB\text{-a.s.}
	\end{align*}
	This proves statement~\ref{thms: estimator-barrier-growth}.

	Now, let us turn to the proof of statement~\ref{thms: estimator-barrier-consistency}. 
	Let us fix an arbitrary $\BIx\in\R^d$ and let $\varphi^{(a)}_{\mathrm{LB}}(\cdot)$, $\varphi^{(b)}_{\mathrm{UB}}(\cdot)$, $\varphi^{(c)}_{\mathrm{UB}}(\cdot)$, $\varphi^{(d)}_{\mathrm{LB}}(\cdot)$, and $\varphi^{(e)}_{\mathrm{LB}}(\cdot)$ be defined as in the proof of Theorem~\ref{thm: convex-leastsquares-LB}.
	We will focus on the maximization problem characterizing $\varphi^{(d)}_{\mathrm{LB}}(\BIx)$:
	\begin{align}
		p^\star:=\varphi^{(d)}_{\mathrm{LB}}(\BIx)=\sup_{\BIw\in\Delta}\Big\{\langle\widetilde{\BG}^{\star\TRANSP}\BIx+\BIv,\BIw\rangle
		-\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\|\widetilde{\BG}^\star\BIw\|^2\Big\}.
		\label{eqn: estimator-barrier-proof-consistency-primal}
	\end{align}
	By introducing the Lagrange multiplier variables $\Bzeta\in\R^m_+$ and $\xi\in\R$, the Lagrangian $L:\R^m\times\R^m_+\times\R\to\R$ associated with (\ref{eqn: estimator-barrier-proof-consistency-primal}) is given by:
	\begin{align*}
		L(\BIw,\Bzeta,\xi)&:= \langle\widetilde{\BG}^{\star\TRANSP}\BIx+\BIv,\BIw\rangle-\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\|\widetilde{\BG}^\star\BIw\|^2 + \langle\Bzeta,\BIw\rangle + \xi(\langle\vecone_m,\BIw\rangle-1) \\
		& \hspace{200pt}\forall \BIw\in\R^m,\;\forall \Bzeta\in\R^m_+,\;\forall \xi\in\R.
	\end{align*}
	Let $p:\R^m\to\R\cup\{-\infty\}$ and $q:\R^m_+\times\R\to\R$ be defined as follows:
	\begin{align*}
		p(\BIw)&:=\inf_{\Bzeta\in\R^m_+,\,\xi\in\R}\big\{L(\BIw,\Bzeta,\xi)\big\} && \forall \BIw\in\R^m, \\
		q(\Bzeta,\xi)&:= \sup_{\BIw\in\R^m}\big\{L(\BIw,\Bzeta,\xi)\big\} && \forall \Bzeta\in\R^m_+,\;\forall \xi\in\R. 
	\end{align*}
	In particular, we have $p^\star=\sup_{\BIw\in\R^m}\big\{p(\BIw)\big\}$ and 
	\begin{align}
		\label{eqn: estimator-barrier-proof-consistency-dualineq}
		q(\Bzeta,\xi)\ge \sup_{\BIw\in\R^m}\big\{p(\BIw)\big\}=p^\star \qquad \forall \Bzeta\in\R^m_+,\;\forall \xi\in\R. 
	\end{align}
	Moreover, let $\widehat{\xi}(\BIx;\theta)$ be defined as in the proof of statement~\ref{thms: estimator-barrier-shape}. 
	Recall that we have shown by the KKT optimality conditions in (\ref{eqn: estimator-barrier-proof-shape-KKTequation}) that 
	\begin{align}
	\begin{split}
		-\textstyle\frac{1}{\theta}\widehat{\BIw}(\BIx;\theta)^{\circ-1}+\frac{1}{\overline{\lambda}-\underline{\lambda}}\widetilde{\BG}^{\star\TRANSP}\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)-\widetilde{\BG}^{\star\TRANSP}\BIx-\BIv-\widehat{\xi}(\BIx;\theta)\vecone_m&=\veczero_m,\\
		\langle\vecone_m,\widehat{\BIw}(\BIx;\theta)\rangle-1&=0.
	\end{split}
	\label{eqn: estimator-barrier-proof-consistency-KKT-Lagrange}
	\end{align}
	Subsequently, let $\bar{\Bzeta}:=\frac{1}{\theta}\widehat{\BIw}(\BIx;\theta)^{\circ-1}\in\R^m_+$ and let $\bar{\xi}:=\widehat{\xi}(\BIx;\theta)\in\R$. 
	Let $\nabla_{\BIw}L(\BIw,\Bzeta,\xi)$ denote the partial derivative of $L$ with respect to $\BIw$. 
	Observe that (\ref{eqn: estimator-barrier-proof-consistency-KKT-Lagrange}) implies that
	\begin{align}
		\label{eqn: estimator-barrier-proof-consistency-Lagrangesaddle}
		\begin{split}
		\nabla_{\BIw}L\big(\widehat{\BIw}(\BIx;\theta),\bar{\Bzeta},\bar{\xi}\big)
		&=\textstyle\widetilde{\BG}^{\star\TRANSP}\BIx+\BIv-\frac{1}{\overline{\lambda}-\underline{\lambda}}\widetilde{\BG}^{\star\TRANSP}\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)+\frac{1}{\theta}\widehat{\BIw}(\BIx;\theta)^{\circ-1}+\widehat{\xi}(\BIx;\theta)\vecone_m =\veczero_m.
		\end{split}
	\end{align}
	Since $L(\,\cdot\,,\bar{\Bzeta},\bar{\xi})$ is concave, (\ref{eqn: estimator-barrier-proof-consistency-Lagrangesaddle}) shows that $\widehat{\BIw}(\BIx;\theta)\in\argmax_{\BIw\in\R^d}\big\{L(\BIw,\bar{\Bzeta},\bar{\xi})\big\}$.
	Thus, we have by (\ref{eqn: estimator-barrier-proof-consistency-dualineq}) and (\ref{eqn: estimator-barrier-proof-consistency-KKT-Lagrange}) that 
	\begin{align}
	\begin{split}
		p^\star&\le q(\bar{\Bzeta},\bar{\xi})\\
		&=L\big(\widehat{\BIw}(\BIx;\theta),\bar{\Bzeta},\bar{\xi}\big)\\
		&= \langle\widetilde{\BG}^{\star\TRANSP}\BIx+\BIv,\widehat{\BIw}(\BIx;\theta)\rangle-\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\big\|\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)\big\|^2 + \frac{1}{\theta} \langle\widehat{\BIw}(\BIx;\theta)^{\circ-1},\widehat{\BIw}(\BIx;\theta)\rangle \\
		&\hspace{230pt} + \textstyle\widehat{\xi}(\BIx;\theta)(\langle\vecone_m,\widehat{\BIw}(\BIx;\theta)\rangle-1)\\
		&=\langle\widetilde{\BG}^{\star\TRANSP}\BIx+\BIv,\widehat{\BIw}(\BIx;\theta)\rangle-\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\big\|\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)\big\|^2 + \frac{m}{\theta}.
	\end{split}
	\label{eqn: estimator-barrier-proof-consistency-step1}
	\end{align}
	The definition of $p^\star$ in (\ref{eqn: estimator-barrier-proof-consistency-primal}) then shows that $\widehat{\BIw}(\BIx;\theta)$ is a $\frac{m}{\theta}$-optimal solution of (\ref{eqn: estimator-barrier-proof-consistency-primal}).
	Next, observe that 
	\begin{align}
	\begin{split}
		p^\star=\varphi^{(d)}_{\mathrm{LB}}(\BIx)&=\sup_{\widetilde{\BIg}\in\R^d}\Big\{\langle\widetilde{\BIg},\BIx\rangle-\varphi^{(c)}_{\mathrm{UB}}(\widetilde{\BIg})\Big\}=\sup_{\widetilde{\BIg}\in\R^d}\Big\{\langle\widetilde{\BIg},\BIx\rangle-\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\|\widetilde{\BIg}\|^2-\varphi^{(b)}_{\mathrm{UB}}(\widetilde{\BIg})\Big\}.
	\end{split}
	\label{eqn: estimator-barrier-proof-consistency-gradobj}
	\end{align}
	Let $h:\R^d\to\R\cup\{\infty\}$ be defined by $h(\widetilde{\BIg}):=\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\|\widetilde{\BIg}\|^2-\langle\widetilde{\BIg},\BIx\rangle+\varphi^{(b)}_{\mathrm{LB}}(\widetilde{\BIg})$ for all $\widetilde{\BIg}\in\R^d$. 
	We hence have $h\in\FC_{\frac{1}{\overline{\lambda}-\underline{\lambda}},\infty}(\R^d)$.
	It follows from (\ref{eqn: estimator-barrier-proof-consistency-gradobj}) that $\nabla\varphi_{\mathrm{LB}}^{(d)}(\BIx)$ is the unique minimizer of $h$.  
	We can subsequently derive from (\ref{eqn: estimator-barrier-proof-consistency-gradobj}) and (\ref{eqn: estimator-barrier-proof-consistency-step1}) that
	\begin{align}
	\begin{split}
		h\big(\nabla\varphi_{\mathrm{LB}}^{(d)}(\BIx)\big)&=\inf_{\widetilde{\BIg}\in\R^d}\Big\{\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\|\widetilde{\BIg}\|^2-\langle\widetilde{\BIg},\BIx\rangle+\varphi^{(b)}_{\mathrm{UB}}(\widetilde{\BIg})\Big\}\\
		&=-p^\star\\
		&\ge \textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\big\|\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)\big\|^2 - \langle\widetilde{\BG}^{\star\TRANSP}\BIx+\BIv,\widehat{\BIw}(\BIx;\theta)\rangle -\frac{m}{\theta}\\
		&\ge \textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\big\|\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)\big\|^2-\langle\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta),\BIx\rangle-{\displaystyle\sup_{\BIw\in\Delta,\,\widetilde{\BG}^\star\BIw=\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)}}\big\{\langle\BIv,\BIw\rangle\big\}-\frac{m}{\theta}\\
		&=\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\big\|\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)\big\|^2-\langle\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta),\BIx\rangle+\varphi^{(b)}_{\mathrm{UB}}\big(\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)\big)-\frac{m}{\theta}\\
		&=h\big(\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)\big)-\textstyle\frac{m}{\theta}.
	\end{split}
	\label{eqn: estimator-barrier-proof-consistency-step2}
	\end{align}
	Moreover, it follows from the first-order optimality condition that $\veczero\in\partial h\big(\nabla\varphi_{\mathrm{LB}}^{(d)}(\BIx)\big)$, and hence the \mbox{$\frac{1}{\overline{\lambda}-\underline{\lambda}}$-strong} convexity of $h$ implies that
	\begin{align}
	\begin{split}
		h\big(\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)\big)&\ge h\big(\nabla\varphi_{\mathrm{LB}}^{(d)}(\BIx)\big)+\big\langle\veczero,\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)-\nabla\varphi_{\mathrm{LB}}^{(d)}(\BIx)\big\rangle\\
		&\qquad\qquad+\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\big\|\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)-\nabla\varphi_{\mathrm{LB}}^{(d)}(\BIx)\big\|^2\\
		&=h\big(\nabla\varphi_{\mathrm{LB}}^{(d)}(\BIx)\big)+\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\big\|\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)-\nabla\varphi_{\mathrm{LB}}^{(d)}(\BIx)\big\|^2.
	\end{split}
	\label{eqn: estimator-barrier-proof-consistency-step3}
	\end{align}
	Subsequently, combining (\ref{eqn: estimator-barrier-proof-consistency-step2}) and (\ref{eqn: estimator-barrier-proof-consistency-step3}) yields $\big\|\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)-\nabla\varphi_{\mathrm{LB}}^{(d)}(\BIx)\big\|^2\le \frac{2m(\overline{\lambda}-\underline{\lambda})}{\theta}$.
	Lastly, recall from (\ref{eqn: convex-leastsquares-LB-proof-gradient-alternative}) that $\widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx)=\underline{\lambda}\BIx+\nabla\varphi_{\mathrm{LB}}^{(d)}(\BIx)$.
	It hence follows from (\ref{eqn: estimator-barrier-gradient}) that 
	\begin{align*}
		\big\|\widehat{T}_{\mathrm{barr}}(\BIx;\theta)-\widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx)\big\|^2&=\big\|\big(\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)+\underline{\lambda}\BIx\big)-\big(\nabla\varphi_{\mathrm{LB}}^{(d)}(\BIx)+\underline{\lambda}\BIx\big)\big\|^2\\
		&=\big\|\widetilde{\BG}^\star\widehat{\BIw}(\BIx;\theta)-\nabla\varphi_{\mathrm{LB}}^{(d)}(\BIx)\big\|^2\\
		&\le \frac{2m(\overline{\lambda}-\underline{\lambda})}{\theta}.
	\end{align*}
	This completes the proof of statement~\ref{thms: estimator-barrier-consistency}.

	Finally, observe that statement~\ref{thms: estimator-barrier-shape} shows that $\widehat{T}_{\mathrm{barr}}(\,\cdot\,;\theta)$ satisfies the shape condition in Assumption~\ref{asp: plugin-estimator}\ref{asps: plugin-estimator-shape}.
	Moreover, statement~\ref{thms: estimator-barrier-growth} implies that $\EXP\Big[\big\|\widehat{T}_{\mathrm{barr}}(\BIx;\theta)\big\|^2\Big]\le 18\overline{u}_0(\nu)^2+2\|\BIx\|^2$ for all $\BIx$, which shows that $\widehat{T}_{\mathrm{barr}}(\,\cdot\,;\theta)$ satisfies the growth condition in Assumption~\ref{asp: plugin-estimator}\ref{asps: plugin-estimator-growth} with respect to $u_1(\nu):=18\overline{u}_0(\nu)^2$, $u_2(\nu):=2$.
	Furthermore, statement~\ref{thms: estimator-barrier-consistency} shows that
	\begin{align*}
		\big\|\widehat{T}_{\mathrm{barr}}(\BIx;\theta)-T^{\mu}_{\nu}(\BIx)\big\|^2 &\le 2\big\|\widehat{T}_{\mathrm{barr}}(\BIx;\theta)-\widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx)\big\|^2 + 2\big\|\widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx)-T^{\mu}_{\nu}(\BIx)\big\|^2 \\
		&\le \frac{4m(\overline{\lambda}(\mu,\nu)-\underline{\lambda}(\mu,\nu))}{\theta} + 2\big\|\widehat{T}_{\mathrm{CLS\text{-}LB}}(\BIx)-T^{\mu}_{\nu}(\BIx)\big\|^2 \qquad\qquad\forall\BIx\in\R^d.
	\end{align*}
	Combining this with Theorem~\ref{thm: convex-leastsquares-LB}, Theorem~\ref{thm: shape-constraints}, and Theorem~\ref{thm: plugin-estimation-error} leads to
	\begin{align*}
		\EXP\Big[\big\|\widehat{T}_{\mathrm{barr}}(\,\cdot\,;\theta)-T^{\mu}_{\nu}\big\|_{\CL^2(\mu)}^2\Big] &\le \frac{4m(\overline{\lambda}(\mu,\nu)-\underline{\lambda}(\mu,\nu))}{\theta}\\
		&\qquad+ 2C\big(\mu,\nu,\underline{\lambda}(\mu,\nu),\overline{\lambda}(\mu,\nu)\big)\log(m)^2\kappa\big(\min\{m,n\}\big).
	\end{align*}
	Consequently, for any $\epsilon>0$, with $\overline{n}(\mu,\nu,\epsilon):=\min\big\{n\in\N:C\big(\mu,\nu,\underline{\lambda}(\mu,\nu),\overline{\lambda}(\mu,\nu)\big)\log(n)^2\kappa(n)\le \frac{\epsilon}{4}\big\}$ and
	$\overline{\theta}(\mu,\nu,m,n,\epsilon):=\big\lceil\frac{8m}{\epsilon}(\overline{\lambda}(\mu,\nu)-\underline{\lambda}(\mu,\nu))\big\rceil$, 
	it holds for all $m\ge \overline{n}(\mu,\nu,\epsilon)$, $n\ge\overline{n}(\mu,\nu,\epsilon)$ and all $\theta\ge \overline{\theta}(\mu,\nu,m,n,\epsilon)$ that
	\begin{align*}
		&\EXP\Big[\big\|\widehat{T}_{\mathrm{barr}}(\,\cdot\,;\theta)-T^{\mu}_{\nu}\big\|_{\CL^2(\mu)}^2\Big] \\
		&\qquad \le \frac{4m(\overline{\lambda}(\mu,\nu)-\underline{\lambda}(\mu,\nu))}{\theta}+ 2C\big(\mu,\nu,\underline{\lambda}(\mu,\nu),\overline{\lambda}(\mu,\nu)\big)\log(m)^2\kappa\big(\min\{m,n\}\big) \\
		&\qquad \le \frac{4m(\overline{\lambda}(\mu,\nu)-\underline{\lambda}(\mu,\nu))}{\frac{8}{\epsilon}m(\overline{\lambda}(\mu,\nu)-\underline{\lambda}(\mu,\nu))}+ 2C\big(\mu,\nu,\underline{\lambda}(\mu,\nu),\overline{\lambda}(\mu,\nu)\big)\log\big(\overline{n}(\mu,\nu,\epsilon)\big)^2\kappa\big(\overline{n}(\mu,\nu,\epsilon)\big)\\
		&\qquad  \le \frac{\epsilon}{2} + \frac{\epsilon}{2}=\epsilon.
	\end{align*}
	We have thus shown that $\widehat{T}_{\mathrm{barr}}(\,\cdot\,;\theta)$ satisfies the consistency condition in Assumption~\ref{asp: plugin-estimator}\ref{asps: plugin-estimator-consistency} with respect to $\overline{n}(\mu,\nu,\epsilon):=\min\big\{n\in\N:C\big(\mu,\nu,\underline{\lambda}(\mu,\nu),\overline{\lambda}(\mu,\nu)\big)\log(n)^2\kappa(n)\le \frac{\epsilon}{4}\big\}$ and $\overline{\theta}(\mu,\nu,m,n,\epsilon):=\big\lceil\frac{8m}{\epsilon}(\overline{\lambda}(\mu,\nu)-\underline{\lambda}(\mu,\nu))\big\rceil$. 
	The proof is now complete. 
\end{proof}

\begin{remark}[Computational tractability of $\widehat{T}_{\mathrm{barr}}(\,\cdot\,;\theta)$]
	\label{rmk: estimator-tractability-barrier}
	Same as $\widehat{T}_{\mathrm{kern}}(\,\cdot\,;\theta)$ discussed in Remark~\ref{rmk: estimator-tractability-kernel}, the computation of $\widehat{T}_{\mathrm{barr}}(\,\cdot\,;\theta)$ is also done in two phases. 
	In the first phase, given the $m$~samples $\BIX_1,\ldots,\BIX_m$ from $\mu$ and the $n$~samples $\BIY_1,\ldots,\BIY_n$ from $\nu$, one can compute $(\widetilde{\varphi}_i^\star)_{i=1:m}$ and $(\widetilde{\BIg}_i^\star)_{i=1:m}$ via state-of-the-art LP and QCQP solvers such as Gurobi~\citep{gurobi}, as discussed in Remark~\ref{rmk: estimator-tractability-kernel}.
	Subsequently, in the second phase, for every $\BIx\in\R^d$ at which $\widehat{T}_{\mathrm{barr}}(\,\cdot\,;\theta)$ needs to be evaluated, one solves the convex optimization problem in~(\ref{eqn: estimator-barrier-gradient}). 
	This can be numerically implemented by, for example, Newton's method with equality constraints; see, e.g., \citep[Section~10.2]{boyd2004convex}. 
	Consequently, using $\widehat{T}_{\mathrm{barr}}(\,\cdot\,;\theta)$ as the plug-in OT map estimator in Algorithm~\ref{algo: iteration} results in a computationally tractable algorithm for $\CW_2$-barycenter that is also provably convergent. 
\end{remark}


\section{Numerical experiments}
\label{sec: numerics}

\TODO{[To be done]}

\appendix

\section{Discussion and proof related to measurability issues}
\label{apx: measurability}

\TODO{[To be discussed]}


\section{A Paralleled implementation for soving \eqref{eqn: estimator-QCQPregression} via ADMM}
\label{apx: ADMM}

As mentioned in Remark~\eqref{remark: ADMM}, we provide the setting and the implementation details of our paralleled algorithm for solving the quadratically constrained quadratic program (QCQP) in \eqref{eqn: estimator-QCQPregression} via the alternating direction method of multipliers (ADMM), adapted from \citet*[Section~3.C]{simonetto2021smooth}. 

Let us consider the complete digraph $\CG := (\CV, \CE)$ where $\CV := \{1, 2, \dots, m\}$ and $\CE := \{e(i \rightarrow j) : i, j \in \CV, i \neq j\}$. For any $e(i \rightarrow j) \in \CE$, we refer $i$ as the \textit{source node} and $j$ as the \textit{target node}. For $i = 1, \dots, m$, besides the existing node variables $\widetilde{\varphi}_i$ and $\widetilde{\BIg}_i$ in \eqref{eqn: estimator-QCQPregression}, we additionally assign on each edge $e(i \rightarrow j) \in \CE$ decision variables $\widetilde{\varphi}_{i, j}^{(s)} \in \R$ and $\widetilde{\BIg}_{i, j}^{(s)} \in \R^d$. Meanwhile, when $i$ serves as a target node, we correspondingly assign $\widetilde{\varphi}_{i, j}^{(t)} \in \R$ and $\widetilde{\BIg}_{i, j}^{(t)} \in \R^d$ for each $e(j \rightarrow i) \in \CE$. We are therefore able to define, for each $e(i \rightarrow j) \in \CE$, a concatenated decision variable $\Bxi_{i \rightarrow j} := \big(\widetilde{\varphi}_{i, j}^{(s)}, \widetilde{\BIg}_{i, j}^{(s) \TRANSP}, \widetilde{\varphi}_{j, i}^{(t)}, \widetilde{\BIg}_{j, i}^{(t)\TRANSP}\big)^\TRANSP \in \R^{2d + 2}$.

We now show that the original QCQP in \eqref{eqn: estimator-QCQPregression} can now be reformulated in a separable form in cater for the application of ADMM. Indeed, by denoting $\CC_{i \rightarrow j} := \big\{\Bxi_{i \rightarrow j}: \widetilde{\varphi}_{j. i}^{(t)} \ge \widetilde{\varphi}_{i, j}^{(s)} + \langle\widetilde{\BIg}_{i, j}^{(s)},\BIX_j-\BIX_i\rangle +\textstyle\frac{1}{2(\overline{\lambda}-\underline{\lambda})}\|\widetilde{\BIg}_{i, j}^{(s)}-\widetilde{\BIg}_{j, i}^{(t)}\|^2\big\}$ whenever $i \neq j$ and denoting $\delta_{\CS}$ as the indicator function for $\CS \subseteq \R^d$, one can rewrite \eqref{eqn: estimator-QCQPregression} in the following equivalent form:

\begin{align}
	\label{eqn: estimator-QCQPregression-ADMM}
	\begin{split}
	\minimize_{(\widetilde{\varphi}_i),\,(\widetilde{\BIg}_i), \, (\Bxi_{i\rightarrow j})} \quad & \sum_{i=1}^m\sum_{j=1}^m \widehat{\pi}_{i,j}^\star \big\|\widetilde{\BIg}_i + \underline{\lambda}\BIX_i - \BIY_j \big\|^2 + \sum_{i = 1}^m \delta_{\bar{B}(-\underline{\lambda}\BIX_i, \overline{u}_0(\nu))}(\widetilde{\BIg}_i)+ \sum_{i = 1}^m \sum_{j = 1}^m \delta_{\CC_{i \rightarrow j}}(\Bxi_{i \rightarrow j})\\
	\text{subject to} \quad & \widetilde{\varphi}_{i, j}^{(s)} = \widetilde{\varphi}_{i, j}^{(t)} = \widetilde{\varphi}_i \quad \forall i = 1 : m, \; \forall j= 1 : m, \; i \neq j\\
	& \widetilde{\BIg}_{i, j}^{(s)} = \widetilde{\BIg}_{i, j}^{(t)} = \widetilde{\BIg}_i \quad \forall i = 1 : m, \; \forall j= 1 : m, \; i \neq j
	\end{split}
  \end{align}

Since \eqref{eqn: estimator-QCQPregression-ADMM} contains a separable objective function and equality constraints, it can be tackled by ADMM; see, e.g.,\@ \citep[Section~3]{boyd2011distributed}. For every $e(i \rightarrow j) \in \CE$, we define the dual scaled variables $u_{i, j}^{(s)}, u_{j, i}^{(t)} \in \R$ and $\BIv_{i, j}^{(s)}, \BIv_{j, i}^{(t)} \in \R^d$, and denote $\Bvartheta_{i \rightarrow j} := \bigl(u_{i, j}^{(s)}, \BIv_{i, j}^{(s)\TRANSP}, u_{j, i}^{(t)}, \BIv_{j, i}^{(t)\TRANSP}\bigr) \in \R^{2d + 2}$. The associated augmented Lagrangian of \eqref{eqn: estimator-QCQPregression-ADMM} with a penalty term $\rho > 0$ is formed as 

\begin{align*}
	\begin{split}
		L_\rho\big((\widetilde{\varphi}_i),(\widetilde{\BIg}_i), (\Bxi_{i\rightarrow j}), (\Bvartheta_{i \rightarrow j})\big) :=& \sum_{i=1}^m\sum_{j=1}^m \widehat{\pi}_{i,j}^\star \big\|\widetilde{\BIg}_i + \underline{\lambda}\BIX_i - \BIY_j \big\|^2 + \sum_{i = 1}^m \delta_{\bar{B}(-\underline{\lambda}\BIX_i, \overline{u}_0(\nu))}(\widetilde{\BIg}_i)\\ 
		& \quad + \sum_{i = 1}^m \sum_{j = 1, j \neq i}^m \delta_{\CC_{i \rightarrow j}}(\Bxi_{i \rightarrow j}) \\
		& \quad + \frac{\rho}{2} \sum_{i = 1}^m \sum_{j = 1, j \neq i}^m \Big[\big\|\widetilde{\BIg}_{i, j}^{(s)} - \widetilde{\BIg}_i + \BIv_{i, j}^{(s)} \big\|^2 + \big\|\widetilde{\BIg}_{j, i}^{(t)} - \widetilde{\BIg}_j + \BIv_{j, i}^{(t)} \big\|^2 \\
		& \quad + \bigl(\widetilde{\varphi}_{i,j}^{(s)} - \widetilde{\varphi}_i + u_{i, j}^{(s)}\bigr)^2 + \bigl(\widetilde{\varphi}_{j,i}^{(t)} - \widetilde{\varphi}_j + u_{j, i}^{(t)}\bigr)^2 \\
		& \quad - \big\|\BIv_{i, j}^{(s)}\big\|^2 - \big\|\BIv_{j, i}^{(t)}\big\|^2 - \big(u_{i, j}^{(s)}\big)^2 - \big(u_{j, i}^{(t)}\big)^2\Big]
	\end{split}
\end{align*}

As such, with certain initializations $(\widetilde{\varphi}_i^0),(\widetilde{\BIg}_i^0), (\Bxi_{i\rightarrow j}^0), (\Bvartheta_{i \rightarrow j}^0)$, the decomposable blockwise ADMM updating the $k$-th iteration can be performed in the following procedure:

\begin{enumerate}[label = (\roman*)]
	\item \textbf{(node variables update)} We solve for each $i = 1, \dots, m$:
	\begin{align}
		\begin{split}
			\label{eqn: node_update_g}
			\widetilde{\BIg}_i^k := \argmin_{\BIg \in \bar{B}(-\underline{\lambda} \BIX_i, \overline{u}_0(\nu))} &\sum_{j = 1}^m \widehat{\pi}_{ij}^\star \big\| \BIg + \underline{\lambda} \BIX_i - \BIY_j\big\|^2 + \frac{\rho}{2} \sum_{j = 1, j \neq i}^m \Big(\big\| \widetilde{\BIg}_{i, j}^{(s), k - 1} + \BIv_{i, j}^{(s), k - 1}- \BIg \big\|^2 \\
			& + \big\| \widetilde{\BIg}_{i, j}^{(t), k - 1} + \BIv_{i, j}^{(t), k - 1}- \BIg\big\|^2 \Big).
		\end{split}\\
		\label{eqn: node_update_varphi}
		\widetilde{\varphi}_i^k := \argmin_{\varphi \in \R}& \sum_{j = 1, j \neq i}^m \Bigl(\big(\widetilde{\varphi}_{i, j}^{(s), k - 1} + u_{i, j}^{(s), k - 1} - \varphi\big)^2 + \big(\widetilde{\varphi}_{i, j}^{(t), k - 1} + u_{i, j}^{(t), k - 1} - \varphi\big)^2 \Bigr)
	\end{align}
	\item \textbf{(edge variables update)} We solve for each $e(i \rightarrow j) \in \CE$:
	\begin{align}
		\begin{split}
			\label{eqn: edge_update}
			\Bxi_{i \rightarrow j}^{k} := \argmin_{\Bxi \in \CC_{i \rightarrow j}}& \Big(\big\| \big(\begin{array}{c|c}
				\BII_{d+1} & \mathbf{\mathit{0}}_{(d+1) \times (d+1)}
				\end{array}\big) \Bxi - (\widetilde{\varphi}_i^{k- 1}, \widetilde{\BIg}_i^{k - 1\, \TRANSP})^\TRANSP + (u_{i, j}^{(s), k - 1}, \BIv_{i, j}^{(s), k - 1\, \TRANSP})^\TRANSP \big\|^2 \\
				& + \big\| \big(\begin{array}{c|c}
					\mathbf{\mathit{0}}_{(d+1) \times (d+1)} & \BII_{d+1} 
					\end{array}\big) \Bxi - (\widetilde{\varphi}_j^{k- 1}, \widetilde{\BIg}_j^{k - 1\, \TRANSP})^\TRANSP + (u_{j, i}^{(t), k - 1}, \BIv_{j, i}^{(t), k - 1\, \TRANSP})^\TRANSP \big\|^2\Big)
		\end{split}
	\end{align}
	where $\mathbf{\mathit{0}}_{(d+1) \times (d+1)}$ denotes the all-zero $(d + 1) \times (d + 1)$ square matrix.
	\item \textbf{(dual variables update)} We compute for $i, j = 1, \dots, m$ with $i \neq j$:
	\begin{align}
		u_{i, j}^{(s), k} &:= u_{i, j}^{(s), k - 1} + \widetilde{\varphi}_{i, j}^{(s), k} - \widetilde{\varphi}_i^{k}, \quad
		&&\BIv_{i, j}^{(s), k} := \BIv_{i, j}^{(s), k - 1} + \widetilde{\BIg}_{i, j}^{(s), k - 1} - \widetilde{\BIg}_i^{k - 1}. \\
		u_{i, j}^{(t), k} &:= u_{i, j}^{(t), k - 1} + \widetilde{\varphi}_{i, j}^{(t), k} - \widetilde{\varphi}_i^{k}, \quad
		&&\BIv_{i, j}^{(t), k} := \BIv_{i, j}^{(t), k - 1} + \widetilde{\BIg}_{i, j}^{(t), k - 1} - \widetilde{\BIg}_i^{k - 1}.
	\end{align}
\end{enumerate}

We now delve into each block and solve the optimization problems therein for alternating updates on nodes and edges. One can observe that both \eqref{eqn: node_update_g} and \eqref{eqn: node_update_varphi} attain closed-form solutions. Indeed, we have 
\begin{align}
	\begin{split}
		\widetilde{\BIg}_i^k =& \argmin_{\BIg \in \bar{B}(-\underline{\lambda} \BIX_i, \overline{u}_0(\nu))} \big[\textstyle \frac{1}{m} + \rho(m - 1)\big] \|\BIg\|^2 + 2 \sum_{j = 1}^m \big[\widehat{\pi}_{i, j}^\star (\underline{\lambda}\BIX_i - \BIY_j)\big]^\TRANSP \BIg \\
		& \hskip15em- \rho \textstyle \sum_{j = 1, j \neq i}^m \big(\widetilde{\BIg}_{i, j}^{(s), k - 1} + \BIv_{i, j}^{(s), k - 1} + \widetilde{\BIg}_{i, j}^{(t), k - 1} + \BIv_{i, j}^{(t), k - 1}\big)^\TRANSP \BIg \\
		=& \argmin_{\BIg \in \bar{B}(-\underline{\lambda} \BIX_i, \overline{u}_0(\nu))} \|\BIg - \BIz_i\|^2
	\end{split}
\end{align}
where
\begin{align*}
	\BIz_i := - \textstyle \frac{1}{\frac{1}{m} + \rho(m - 1)} \Big[\sum_{j = 1}^m \widehat{\pi}_{i, j}^\star (\underline{\lambda}\BIX_i - \BIY_j) - \frac{\rho}{2} \sum_{j = 1, j \neq i}^m \big(\widetilde{\BIg}_{i, j}^{(s), k - 1} + \BIv_{i, j}^{(s), k - 1} + \widetilde{\BIg}_{i, j}^{(t), k - 1} + \BIv_{i, j}^{(t), k - 1}\big)\Big].
\end{align*}
Therefore, 
\begin{align}
	\widetilde{\BIg}_i^k = \begin{cases}
		\BIz_i,  & \BIz_i \in \bar{B}(-\underline{\lambda} \BIX_i, \overline{u}_0(\nu)) \\
		-\underline{\lambda} \BIX_i + \frac{\overline{u}_0(\nu)}{\|\BIz_i + \underline{\lambda}\BIX_i\|}(\BIz_i + \underline{\lambda}\BIX_i), & \BIz_i \notin \bar{B}(-\underline{\lambda} \BIX_i, \overline{u}_0(\nu)) 
	\end{cases}
\end{align}
In terms of \eqref{eqn: node_update_varphi}, it is a standard least square problem, thus the solution is 
\begin{align}
	\widetilde{\varphi}_i^k = \frac{1}{2m - 2} \sum_{j = 1, j \neq i}^m \big(\widetilde{\varphi}_{i, j}^{(s), k - 1} + u_{i, j}^{(s), k - 1} + \widetilde{\varphi}_{i, j}^{(t), k - 1} + u_{i, j}^{(t), k - 1}\big) 
\end{align}

It is left to solve \eqref{eqn: edge_update} which is itself a QCQP with a single scalar constraint. To this end, we rewrite it in the following standard from of QCQP as elucidated in \citep{simonetto2021smooth}:
\begin{align}
	\begin{split}
	\label{eqn: edge_update_standard}
	\minimize_{\Bxi \in \R^{2d + 2}} \quad & \Bxi^\TRANSP\Bxi + \BIq_0^\TRANSP \Bxi \\
	\mathrm{subject~to} \quad & \Bxi^\TRANSP \BIQ \Bxi + \BIq_1^\TRANSP \Bxi \leq 0
	\end{split}
\end{align}
where 
\begin{align*}
	\BIq_0 :=&  2 \big(u_{i, j}^{(s), k - 1} - \widetilde{\varphi}_i^{k - 1}, \BIv_{i, j}^{(s), k - 1\, \TRANSP} - \widetilde{\BIg}_i^{k - 1\, \TRANSP}, u_{j, i}^{(t), k - 1} - \widetilde{\varphi}_j^{k - 1}, \BIv_{j, i}^{(t), k - 1\, \TRANSP} - \widetilde{\BIg}_j^{k - 1\, \TRANSP}\big)^\TRANSP \in \R^{2d + 2}\\
	\BIQ := & \frac{1}{2(\overline{\lambda} - \underline{\lambda})}\begin{pmatrix}
		0 & 0 & 0 & 0 & \cdots & 0 & 0 & 0 & 0 \\
		0 & 1 & 0 & 0 & \cdots & 0 & -1 & 0 & 0 \\
		0 & 0 & 1 & 0 & \cdots & 0 & 0 & -1 & 0 \\
		0 & 0 & 0 & 1 & \cdots & 0 & 0 & 0 & -1 \\
		\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots \\
		0 & 0 & 0 & 0 & \cdots & 0 & 0 & 0 & 0 \\
		0 & -1 & 0 & 0 & \cdots & 0 & 1 & 0 & 0 \\
		0 & 0 & -1 & 0 & \cdots & 0 & 0 & 1 & 0 \\
		0 & 0 & 0 & -1 & \cdots & 0 & 0 & 0 & 1
		\end{pmatrix} \in \R^{(2d + 2) \times (2d + 2)}\\
	\BIq_1 := & \Bigl(1, (\BIX_j- \BIX_i)^\TRANSP, -1, \veczero_{d}^\TRANSP\Bigr)^\TRANSP \in \R^{2d + 2}
\end{align*}

With the dual variable $\gamma \geq 0$, the Lagrange dual function associated to \eqref{eqn: edge_update_standard} is 

\begin{align}
	\begin{split}
		\phi(\gamma) :=& \inf_{\Bxi} \BIL(\Bxi, \gamma) = \min_{\Bxi}\big\{\Bxi^\TRANSP \Bxi + \BIq_0^\TRANSP \Bxi + \gamma (\Bxi^\TRANSP \BIQ \Bxi + \BIq_1^\TRANSP \Bxi)\big\} \\
		=& -\frac{1}{4}(\BIq_0 + \gamma \BIq_1)^\TRANSP(\BII + \gamma \BIQ)^{-1}(\BIq_0 + \gamma \BIq_1). 
	\end{split}
	\label{prob: dual_local_qcqp}
\end{align}
Defining $\gamma^\star := \argmax_{\gamma \geq 0} \phi(\gamma)$, one can retrieve $\Bxi_{i \rightarrow j}^{k} = - \frac{1}{2}(\BII + \gamma^\star \BIQ)^{-1} (\BIq_0 + \gamma^\star \BIq_1)$.

We apply Newton's method to solve $\gamma^\star$. The gradient and the hessian of $\phi$ in \eqref{prob: dual_local_qcqp} with respect to $\gamma$ is evaluated as follows:
\begin{align}
	\nabla \phi(\gamma) &= \textstyle -\frac{1}{2} \BIq_1^\TRANSP (\BII + \gamma \BIQ)^{-1}(\BIq_0 + \gamma \BIq_1) + \frac{1}{4}(\BIq_0 + \gamma \BIq_1)^\TRANSP(\BII + \gamma \BIQ)^{-1} \BIQ (\BII + \gamma \BIQ)^{-1} (\BIq_0 + \gamma \BIq_1)\\
	\begin{split}
		\nabla^2 \phi(\gamma) &= \textstyle -\frac{1}{2} \BIq_1^\TRANSP(\BII + \gamma \BIQ)^{-1} \BIq_1 + \BIq_1^\TRANSP(\BII + \gamma \BIQ)^{-1}\BIQ (\BII + \gamma \BIQ)^{-1}(\BIq_0 + \gamma \BIq_1)\\
		& \hskip5em- \textstyle\frac{1}{2}(\BIq_0 + \gamma \BIq_1)^\TRANSP(\BII + \gamma \BIQ)^{-1}\BIQ(\BII + \gamma \BIQ)^{-1}\BIQ (\BII + \gamma \BIQ)^{-1} (\BIq_0 + \gamma \BIq_1) \\
	\end{split}
\end{align}
Specifically, we consider the eigen-decomposition $\BIQ = \BIU^\TRANSP \BID \BIU$ where $\BID$ is the diagonal matrix containing eigenvalues. Then, we have 
\begin{align}
	\begin{split}
		(\BII + \gamma \BIQ)^{-1} &= \gamma^{-1} \BIU^\TRANSP (\gamma^{-1}\BII + \BID)^{-1} \BIU,\\
		(\BII + \gamma \BIQ)^{-1} \BIQ (\BII + \gamma \BIQ)^{-1} &= \gamma^{-2} \BIU^\TRANSP (\gamma^{-1}\BII + \BID)^{-2} \BID \BIU,\\
		(\BII + \gamma \BIQ)^{-1} \BIQ (\BII + \gamma \BIQ)^{-1}\BIQ (\BII + \gamma \BIQ)^{-1} &= \gamma^{-3} \BIU^\TRANSP (\gamma^{-1}\BII + \BID)^{-3} \BID^2 \BIU.
	\end{split}
\end{align}

\section{Generation details of synthetic input measures}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{abbrvnat} 
\bibliography{reference.bib}

\end{document}

