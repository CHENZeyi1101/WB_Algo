{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Method for Barycenter Computation\n",
    "**GPU-only implementation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "from src.icnn import DenseICNN_U\n",
    "from src.plotters import plot_training_phase\n",
    "from src.tools import ewma, score_gen, freeze, unfreeze\n",
    "from src.fid_score import calculate_frechet_distance\n",
    "from src import distributions\n",
    "from src import bar_benchmark\n",
    "import itertools\n",
    "\n",
    "import gc\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the parent folder path (folder K)\n",
    "parent_folder = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "print(parent_folder)\n",
    "\n",
    "folder_classes_path = os.path.join(parent_folder, 'classes')\n",
    "sys.path.append(folder_classes_path)\n",
    "\n",
    "from input_generate_entropy import *\n",
    "from true_WB import *\n",
    "from entropic_iterative_scheme import *\n",
    "from sample_plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "assert dim > 1\n",
    "\n",
    "GPU_DEVICE = 0 # GPU index starting from 0\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "LAMBDA = 10\n",
    "G_LR = 1e-4\n",
    "D_LR = 1e-3\n",
    "MAX_ITER = 10001\n",
    "\n",
    "D_ITERS = 50\n",
    "T_ITERS = 10\n",
    "G_ITERS = 50\n",
    "\n",
    "PLOT_FREQ = 499\n",
    "SCORE_FREQ = 499\n",
    "\n",
    "# Parameters for input distributions\n",
    "NUM = 5 # we have 5 input measures\n",
    "ALPHAS = np.array([1. / NUM for _ in range(NUM)])\n",
    "\n",
    "CASE = {\n",
    "    'type' : 'EigWarp', \n",
    "    'sampler' : 'Rectangles', #'Gaussians', #'SwissRoll',# , #\n",
    "    'params' : {'num' : NUM, 'alphas' : ALPHAS, 'min_eig' : .5, 'max_eig' : 2}\n",
    "}\n",
    "\n",
    "\n",
    "OUTPUT_SEED = 0xBADBEEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert torch.cuda.is_available()\n",
    "torch.cuda.set_device(GPU_DEVICE)\n",
    "\n",
    "np.random.seed(OUTPUT_SEED)\n",
    "torch.manual_seed(OUTPUT_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if CASE['type'] == 'EigWarp':\n",
    "#     if CASE['sampler'] == 'Gaussians':\n",
    "#         sampler = distributions.StandardNormalSampler(dim=dim)\n",
    "#     elif CASE['sampler'] == 'SwissRoll':\n",
    "#         assert dim == 2\n",
    "#         sampler = distributions.SwissRollSampler()\n",
    "#     elif CASE['sampler'] == 'Rectangles':\n",
    "#         sampler = distributions.CubeUniformSampler(dim=dim, normalized=True, centered=True)\n",
    "    \n",
    "#     benchmark = bar_benchmark.EigenWarpBenchmark(sampler, **CASE['params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminators Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = nn.Sequential(\n",
    "    nn.Linear(dim, max(100, 2*dim)),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(max(100, 2*dim), max(100, 2*dim)),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(max(100, 2*dim), max(100, 2*dim)),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(max(100, 2*dim), 1)\n",
    ").cuda()\n",
    "\n",
    "T = nn.Sequential(\n",
    "    nn.Linear(dim, max(100, 2*dim)),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(max(100, 2*dim), max(100, 2*dim)),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(max(100, 2*dim), max(100, 2*dim)),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(max(100, 2*dim), dim)\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_measures = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ds = nn.ModuleList([deepcopy(D) for _ in range(num_measures)]).cuda()\n",
    "Ts = nn.ModuleList([deepcopy(T) for _ in range(num_measures)]).cuda()\n",
    "\n",
    "Ds_inv = nn.ModuleList([deepcopy(D) for _ in range(num_measures)]).cuda()\n",
    "Ts_inv = nn.ModuleList([deepcopy(T) for _ in range(num_measures)]).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now dim_Z = dim_X\n",
    "Z_sampler = distributions.StandardNormalSampler(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nn.Sequential(\n",
    "    nn.Linear(dim, max(100, 2*dim)),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(0.005),\n",
    "    nn.Linear(max(100, 2*dim), max(100, 2*dim)),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(0.005),\n",
    "    nn.Linear(max(100, 2*dim), max(100, 2*dim)),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(max(100, 2*dim), dim)\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_opt = torch.optim.Adam(G.parameters(), lr=1e-4, weight_decay=1e-8)\n",
    "loss = np.inf\n",
    "\n",
    "G.train(True)\n",
    "\n",
    "for iteration in tqdm_notebook(range(10000)):\n",
    "    Z = Z_sampler.sample(BATCH_SIZE).detach() * 3\n",
    "    loss = F.mse_loss(Z, G(Z))\n",
    "    loss.backward()\n",
    "    G_opt.step(); G_opt.zero_grad()\n",
    "    if loss.item() < 1e-2:\n",
    "        break\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "class Identity:\n",
    "    pass\n",
    "\n",
    "# if benchmark.bar_sampler is not None:\n",
    "#     pca.fit(benchmark.bar_sampler.sample(100000).cpu().detach().numpy())\n",
    "# elif benchmark.gauss_bar_sampler is not None:\n",
    "#     pca.fit(benchmark.gauss_bar_sampler.sample(100000).cpu().detach().numpy())\n",
    "# else:\n",
    "#     pca = Identity()\n",
    "#     pca.transform = lambda x: x\n",
    "    \n",
    "# No PCA for dim=2\n",
    "if dim == 2:\n",
    "    pca = Identity()\n",
    "    pca.transform = lambda x: x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_opt = torch.optim.Adam(G.parameters(), lr=G_LR, weight_decay=1e-10)\n",
    "Ts_opt, Ds_opt = [], []\n",
    "Ts_inv_opt, Ds_inv_opt = [], []\n",
    "for k in range(num_measures):\n",
    "    Ts_opt.append(torch.optim.Adam(Ts[k].parameters(), lr=D_LR, weight_decay=1e-10))\n",
    "    Ds_opt.append(torch.optim.Adam(Ds[k].parameters(), lr=D_LR, weight_decay=1e-10))\n",
    "    Ts_inv_opt.append(torch.optim.Adam(Ts_inv[k].parameters(), lr=D_LR, weight_decay=1e-10))\n",
    "    Ds_inv_opt.append(torch.optim.Adam(Ds_inv[k].parameters(), lr=D_LR, weight_decay=1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G_loss_history = []\n",
    "\n",
    "G_UVP_history = []\n",
    "\n",
    "# if hasattr(benchmark, 'gauss_bar_cost'):\n",
    "#     print('Gaussian Barycenter Cost:', benchmark.gauss_bar_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 0\n",
    "last_plot_it = -1\n",
    "last_score_it = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "# dim = 10\n",
    "num_samples = 5000\n",
    "num_measures = 5\n",
    "iter = 0\n",
    "truncated_radius = 100\n",
    "seed = 1009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sampler = MixtureOfGaussians(dim)\n",
    "source_sampler.random_components(num_components=5, uniform_weights = True, seed = seed) # seed from the measure selection\n",
    "source_sampler.set_truncation(truncated_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_gmm_pdf(source_sampler, truncated_radius, save_path = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_measure_sampler_set = []\n",
    "for auxiliary_seed in [1010, 1018, 1014, 1016, 1003]:\n",
    "    auxiliary_measure_sampler = MixtureOfGaussians(dim)\n",
    "    auxiliary_measure_sampler.random_components(num_components=5, uniform_weights = True, seed = auxiliary_seed)\n",
    "    auxiliary_measure_sampler.set_truncation(truncated_radius)\n",
    "    auxiliary_measure_sampler_set.append(auxiliary_measure_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct entropic_sampler\n",
    "entropic_sampler = entropic_input_sampler(dim, num_measures, auxiliary_measure_sampler_set, source_sampler = source_sampler, n_k = 1000, seed = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate strong convexity parameters of the mappings.\n",
    "entropic_sampler.generate_strong_convexity_param()\n",
    "print(\"strong convexity parameters all set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign theta\n",
    "entropic_sampler.assign_theta()\n",
    "print(\"theta all set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Y matrices\n",
    "entropic_sampler.generate_Y_matrices()\n",
    "print(\"Y matrices all set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# generate g vectors\n",
    "entropic_sampler.generate_g_vectors()\n",
    "print(\"g vectors all set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate smoothness parameters; this involves solving max eigen for each tilde_k\n",
    "entropic_sampler.generate_smoothness_param()\n",
    "print(\"smoothness parameters all set.\")\n",
    "print(entropic_sampler.smoothness_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a surjective mapping to map component maps to their respective OT maps for generating input measures.\n",
    "entropic_sampler.construct_surjective_mapping()\n",
    "print(\"surjective mapping all set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate A matrices\n",
    "entropic_sampler.generate_A_matrices()\n",
    "print(\"A matrices all set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in entropic_sampler.surjective_mapping.items():\n",
    "    print(key, ' : ', value)\n",
    "\n",
    "A_matrices_dict = entropic_sampler.A_matrices_dict\n",
    "sum_matrix = np.zeros((dim, dim))\n",
    "for i in range(num_measures):\n",
    "    sum_matrix += A_matrices_dict[i]\n",
    "print(f\"The sum of the matrices is {sum_matrix}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_sample_save(G_samples_dict, accepted_G_samples, iter, save_pathname = None):\n",
    "\n",
    "        # Save the generated samples from the G-mapping at each iteration;\n",
    "        # \"accepted_G_samples\" is the accepted samples generated from the G-mapping at the current iteration;\n",
    "        # \"G_samples\" is a dictinoary with keys corresponding to the iteration number and values corresponding to the generated samples at that iteration.\n",
    "\n",
    "        G_samples_dict[f\"iteration_{iter}\"] = accepted_G_samples\n",
    "        G_samples_json = {str(k): v.tolist() for k, v in G_samples_dict.items()}\n",
    "        G_sample_dir = f\"{save_pathname}/G_samples\"\n",
    "        os.makedirs(G_sample_dir, exist_ok=True)\n",
    "        save_data(G_samples_json, G_sample_dir, f\"G_samples.json\")\n",
    "\n",
    "def V_value_compute(V_values_dict, bary_samples, input_sample_collection, iter = None, save_pathname = None):\n",
    "    # Compute the V-value (i.e.,\\@ the weighted sum of the Wasserstein distances between the input measures and the generated samples)\n",
    "    # Notice that when iter = None, this returns the true V_value given by the ground-truth barycenter;\n",
    "    # Otherwise, it is the V_value returned by an approximated barycenter.\n",
    "    # The input_sample_collection is a dictionary with k keys, each key corresponds to the samples from the k-th input measure.\n",
    "\n",
    "    V_value = 0\n",
    "    for measure_index in range(num_measures):\n",
    "        input_samples = np.array(input_sample_collection[measure_index])\n",
    "        V_value += W2_pot(input_samples, bary_samples)\n",
    "    \n",
    "    # normalize the V_value by the number of input measures\n",
    "    V_value /= num_measures\n",
    "\n",
    "    if iter is None:\n",
    "        V_values_dict[\"true_V_value\"] = V_value\n",
    "    else:\n",
    "        V_values_dict[f\"iteration_{iter}\"] = V_value\n",
    "        \n",
    "    if save_pathname != None:\n",
    "        V_values_json = V_values_dict\n",
    "        V_value_dir = f\"{save_pathname}/V_values\"\n",
    "        os.makedirs(V_value_dir, exist_ok=True)\n",
    "        save_data(V_values_json, V_value_dir, f\"V_values.json\")\n",
    "    else:\n",
    "        return V_value\n",
    "\n",
    "\n",
    "def W2_to_true_bary_compute(W2_to_true_bary_dict, accepted_G_samples, bary_samples, iter, save_pathname = None):\n",
    "\n",
    "    # Compute the Wasserstein distance between the generated samples from the G-mapping\n",
    "    # and the barycenter samples at each iteration;\n",
    "    # \"accepted_G_samples\" is the accepted samples generated from the G-mapping at the current iteration;\n",
    "    # \"bary_samples\" is the barycenter samples generated from the input measure at the current iteration;\n",
    "\n",
    "    W2_sq = W2_pot(accepted_G_samples, bary_samples)\n",
    "    W2_to_true_bary_dict[f\"iteration_{iter}\"] = W2_sq\n",
    "    W2_to_true_bary_json = W2_to_true_bary_dict\n",
    "    W2_to_true_bary_dir = f\"{save_pathname}/W2_to_true_bary\"\n",
    "    os.makedirs(W2_to_true_bary_dir, exist_ok=True)\n",
    "    save_data(W2_to_true_bary_json, W2_to_true_bary_dir, f\"W2_to_true_bary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5000\n",
    "result_dir = \"results\"\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "save_pathname = f\"{result_dir}/WIN_measures_{num_measures}_seed_{seed}_samples_{num_samples}_dim_{dim}\"\n",
    "os.makedirs(save_pathname, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source and input measures visualization\n",
    "plot_dirc = f\"{save_pathname}/plots\"\n",
    "os.makedirs(plot_dirc, exist_ok=True)\n",
    "\n",
    "visualize_num_samples = 1000\n",
    "\n",
    "source_measure_samples = source_sampler.sample(visualize_num_samples, multiplication_factor=1)\n",
    "print(\"Start to plot the source measure\")\n",
    "plot_2d_source_measures_kde(source_measure_samples, plot_dirc = plot_dirc, scatter = False)\n",
    "print(\"Finish plotting the input measures\")\n",
    "\n",
    "input_measure_samples = entropic_sampler.sample(visualize_num_samples, gamma = 0.5, manual = True)\n",
    "for measure_index in range(num_measures):\n",
    "    measure_samples = np.array(input_measure_samples[measure_index])\n",
    "    print(f\"Start to plot the input measure {measure_index}\")\n",
    "    plot_2d_input_measure_kde(measure_samples, measure_index, plot_dirc = plot_dirc, scatter = False)\n",
    "    print(f\"Finish plotting the input measure {measure_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_samples_dict = {}\n",
    "V_values_dict = {}\n",
    "W2_to_true_bary_dict = {}\n",
    "V_value_compute(V_values_dict, source_measure_samples, input_measure_samples, iter = None, save_pathname = save_pathname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while it < MAX_ITER:\n",
    "    freeze(G)\n",
    "    input_measure_samples_for_D = entropic_sampler.sample(BATCH_SIZE * D_ITERS)\n",
    "    input_measure_samples_for_T_inv = entropic_sampler.sample(BATCH_SIZE * D_ITERS * T_ITERS)\n",
    "    input_measure_samples_for_D_inv = entropic_sampler.sample(BATCH_SIZE * D_ITERS)\n",
    "    # this is a dictionary with k keys, pointing to the samples collected from each measure\n",
    "    for k in range(num_measures):\n",
    "        # D and T optimization cycle\n",
    "        for d_iter in tqdm(range(D_ITERS)):\n",
    "            it += 1\n",
    "\n",
    "            # T optimization\n",
    "            unfreeze(Ts[k]); freeze(Ds[k])\n",
    "            for t_iter in range(T_ITERS): \n",
    "                with torch.no_grad():\n",
    "                    X = G(Z_sampler.sample(BATCH_SIZE))\n",
    "                Ts_opt[k].zero_grad()\n",
    "                T_X = Ts[k](X)\n",
    "                T_loss = F.mse_loss(X, T_X).mean() - Ds[k](T_X).mean()\n",
    "                T_loss.backward(); Ts_opt[k].step()\n",
    "            del T_loss, T_X, X\n",
    "            gc.collect()\n",
    "\n",
    "            # D optimization\n",
    "            with torch.no_grad():\n",
    "                X = G(Z_sampler.sample(BATCH_SIZE))\n",
    "            # Y = benchmark.samplers[k].sample(BATCH_SIZE)\n",
    "            Y = torch.tensor(input_measure_samples_for_D[k][d_iter * BATCH_SIZE : (d_iter + 1) * BATCH_SIZE]).float()\n",
    "            \n",
    "            unfreeze(Ds[k]); freeze(Ts[k])\n",
    "            T_X = Ts[k](X).detach()\n",
    "            Ds_opt[k].zero_grad()\n",
    "            D_loss = Ds[k](T_X).mean() - Ds[k](Y).mean()\n",
    "            D_loss.backward(); Ds_opt[k].step()\n",
    "            del D_loss, Y, X, T_X; gc.collect(); torch.cuda.empty_cache()\n",
    "            \n",
    "            # T inv optimization\n",
    "            unfreeze(Ts_inv[k]); freeze(Ds_inv[k])\n",
    "            for t_iter in range(T_ITERS): \n",
    "                Y = torch.tensor(input_measure_samples_for_T_inv[k][d_iter * T_ITERS * BATCH_SIZE + t_iter * BATCH_SIZE : d_iter * T_ITERS * BATCH_SIZE + (t_iter + 1) * BATCH_SIZE]).float()\n",
    "                Ts_inv_opt[k].zero_grad()\n",
    "                T_inv_Y = Ts_inv[k](Y)\n",
    "                T_inv_loss = F.mse_loss(Y, T_inv_Y).mean() - Ds_inv[k](T_inv_Y).mean()\n",
    "                T_inv_loss.backward(); Ts_inv_opt[k].step()\n",
    "            del T_inv_loss, T_inv_Y, Y; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "            # D inv optimization\n",
    "            Y = torch.tensor(input_measure_samples_for_D_inv[k][d_iter * BATCH_SIZE : (d_iter + 1) * BATCH_SIZE]).float()\n",
    "            with torch.no_grad():\n",
    "                X = G(Z_sampler.sample(BATCH_SIZE))\n",
    "            \n",
    "            unfreeze(Ds_inv[k]); freeze(Ts_inv[k])\n",
    "            T_inv_Y = Ts_inv[k](Y).detach()\n",
    "            Ds_inv_opt[k].zero_grad()\n",
    "            D_inv_loss = Ds_inv[k](T_inv_Y).mean() - Ds_inv[k](X).mean()\n",
    "            D_inv_loss.backward(); Ds_inv_opt[k].step()\n",
    "            del D_inv_loss, Y, X, T_inv_Y; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "    # G optimization\n",
    "    if G_ITERS > 0:\n",
    "        for k in range(num_measures):\n",
    "            freeze(Ts[k])\n",
    "        G_old = deepcopy(G); freeze(G_old)\n",
    "        unfreeze(G)\n",
    "        for g_iter in range(G_ITERS):\n",
    "            it += 1\n",
    "            Z = Z_sampler.sample(BATCH_SIZE)\n",
    "            with torch.no_grad():\n",
    "                G_old_Z = G_old(Z)\n",
    "                T_G_old_Z = torch.zeros_like(G_old(Z))\n",
    "            G_old_Z.requires_grad_(True)\n",
    "            for k in range(num_measures):\n",
    "                T_G_old_Z += ALPHAS[k] * Ts[k](G_old_Z)\n",
    "\n",
    "            G_opt.zero_grad()\n",
    "            G_loss = .5 * F.mse_loss(G(Z), T_G_old_Z)\n",
    "            G_loss.backward(); G_opt.step() \n",
    "\n",
    "            G_loss_history.append(G_loss.item())\n",
    "\n",
    "        save_path = f\"{save_pathname}/trained_models_iter_{it}.pth\"\n",
    "        # os.makedirs(save_path, exist_ok=True)\n",
    "        models_to_save = {\n",
    "            \"G\": G.state_dict(),\n",
    "            \"Ts\": {k: Ts[k].state_dict() for k in range(num_measures)},\n",
    "            \"Ds\": {k: Ds[k].state_dict() for k in range(num_measures)},\n",
    "            # Save the inverse transforms if needed\n",
    "            \"Ts_inv\": {k: Ts_inv[k].state_dict() for k in range(num_measures)},\n",
    "            \"Ds_inv\": {k: Ds_inv[k].state_dict() for k in range(num_measures)},\n",
    "        }\n",
    "\n",
    "        # Save the dictionary\n",
    "        torch.save(models_to_save, save_path)\n",
    "        print(f\"Models saved to {save_path}\")\n",
    "\n",
    "            # Log G_loss_history to a local file\n",
    "        with open(\"G_loss_history.log\", \"a\") as f:\n",
    "            f.write(f\"Iteration {it}, G_loss: {G_loss.item()}\\n\")\n",
    "\n",
    "        del G_old, G_loss, T_G_old_Z, Z\n",
    "        gc.collect()\n",
    "\n",
    "        # Save the generated samples from the G-mapping at each iteration\n",
    "        accepted_G_samples = G(Z_sampler.sample(num_samples)).cpu().detach().numpy()\n",
    "        G_sample_save(G_samples_dict, accepted_G_samples, it, save_pathname = save_pathname)\n",
    "\n",
    "        # Compute the V-value (i.e.,\\@ the weighted sum of the Wasserstein distances between the input measures and the generated samples)\n",
    "        # Notice that when iter = None, this returns the true V_value given by the ground-truth barycenter;\n",
    "        V_value_compute(V_values_dict, accepted_G_samples, input_measure_samples, iter = it, save_pathname = save_pathname)\n",
    "\n",
    "        # Compute the Wasserstein distance between the generated samples from the G-mapping\n",
    "        # and the barycenter samples at each iteration;\n",
    "        W2_to_true_bary_compute(W2_to_true_bary_dict, accepted_G_samples, source_measure_samples, it, save_pathname = save_pathname)\n",
    "\n",
    "        # Plot the generated samples from the G-mapping at each iteration\n",
    "        plot_2d_compare_with_source_kde(source_measure_samples, accepted_G_samples, it, plot_dirc = plot_dirc, scatter = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
