{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zeyichen/GitHub/Repo/WB_Algo/ICNN_Fan\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import logging\n",
    "# import GPUtil\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Get the parent folder path (folder K)\n",
    "parent_folder_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "print(parent_folder_path)\n",
    "\n",
    "sys.path.append(parent_folder_path)\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optimal_transport_modules.log_utils as LLU\n",
    "import optimal_transport_modules.generate_data as g_data\n",
    "import optimal_transport_modules.generate_NN as g_NN\n",
    "import optimal_transport_modules.pytorch_utils as PTU\n",
    "from optimal_transport_modules.record_mean_cov import select_mean_and_cov\n",
    "from CNX.cfg import CNXCfgCustom as Cfg_class\n",
    "import CNX.compare_dist_results as CDR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 10\n",
    "num_samples = 5000\n",
    "num_measures = 10\n",
    "seed = 1009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### For computing the constraint loss of negtive weights ######\n",
    "def compute_constraint_loss(list_of_params):\n",
    "    loss_val = 0\n",
    "    for p in list_of_params:\n",
    "        loss_val += torch.relu(-p).pow(2).sum()\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path './results/ICNN_10_seed_1009_samples_5000_dim_10/input_and_source_samples/csv_files' exists.\n"
     ]
    }
   ],
   "source": [
    "folder_name = f\"ICNN_{num_measures}_seed_{seed}_samples_{num_samples}_dim_{dim}\"\n",
    "csv_path = f\"./results/{folder_name}/input_and_source_samples/csv_files\"\n",
    "\n",
    "if os.path.exists(csv_path):\n",
    "    print(f\"The path '{csv_path}' exists.\")\n",
    "else:\n",
    "    print(f\"The path '{csv_path}' does not exist.\")\n",
    "\n",
    "# evaluation_dirc = \"./Evaluate\"\n",
    "# os.makedirs(evaluation_dirc, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                    Training function definition\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "############## For each function here, it's an epoch ##################\n",
    "\n",
    "def train(epoch, csv_path):\n",
    "    convex_f.train()\n",
    "    convex_g.train()\n",
    "    generator_h.train()\n",
    "\n",
    "    # These values are just for saving data\n",
    "    w2_loss_value_epoch = 0\n",
    "    g_OT_loss_value_epoch = [0] * cfg.NUM_DISTRIBUTION\n",
    "    g_constraints_loss_value_epoch = 0\n",
    "    remaining_f_loss_value_epoch = [0] * cfg.NUM_DISTRIBUTION\n",
    "    mu_2moment_loss_value_epoch = 0\n",
    "    miu_mean_value_epoch = 0\n",
    "    miu_var_value_epoch = 0\n",
    "\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                            Data\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    total_data = torch.empty(cfg.N_TRAIN_SAMPLES, cfg.INPUT_DIM, cfg.NUM_DISTRIBUTION + 1)\n",
    "\n",
    "    for marg_id in range(cfg.NUM_DISTRIBUTION):\n",
    "        df = pd.read_csv(f\"{csv_path}/input_measure_samples_{marg_id}.csv\", header=None)\n",
    "        total_data[:, :, marg_id] = torch.from_numpy(df.to_numpy())\n",
    "\n",
    "    total_data[:, :, -1] = torch.randn(cfg.N_TRAIN_SAMPLES, cfg.INPUT_DIM)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        total_data, batch_size=cfg.BATCH_SIZE, shuffle=True, **kwargs)\n",
    "\n",
    "    for batch_idx, real_data in enumerate(train_loader):\n",
    "\n",
    "        # real_data = real_data.cuda(PTU.device)\n",
    "        real_data = real_data.cpu()\n",
    "\n",
    "        miu_i = real_data[:, :, 0:cfg.NUM_DISTRIBUTION]\n",
    "        epsilon = real_data[:, :, cfg.NUM_DISTRIBUTION]\n",
    "        miu_i = Variable(miu_i, requires_grad=True)\n",
    "        epsilon = Variable(epsilon)\n",
    "\n",
    "        # containing four distribution\n",
    "        g_OT_loss_value_batch = [0] * cfg.NUM_DISTRIBUTION\n",
    "        g_constraints_loss_value_batch = 0  # containing four g networks\n",
    "        remaining_f_loss_value_batch = [0] * cfg.NUM_DISTRIBUTION\n",
    "        mu_2moment_loss_value_batch = 0\n",
    "        miu_mean_value_batch = torch.zeros([cfg.INPUT_DIM])\n",
    "        miu_var_value_batch = np.zeros(\n",
    "            [cfg.INPUT_DIM, cfg.INPUT_DIM])\n",
    "\n",
    "        ######################################################\n",
    "        #                Medium Loop Begin                   #\n",
    "        ######################################################\n",
    "        ######### Here iterate over a given number: cfg.N_Fnet_ITERS=4 ##\n",
    "        for medium_iter in range(1, cfg.N_Fnet_ITERS + 1):\n",
    "\n",
    "            ######################################################\n",
    "            #                Inner Loop Begin                   #\n",
    "            ######################################################\n",
    "            ######### Here iterate over a given number: cfg.N_Gnet_ITERS=16 ##\n",
    "            for inner_iter in range(1, cfg.N_Gnet_ITERS + 1):\n",
    "\n",
    "                loss_g = torch.ones(cfg.NUM_DISTRIBUTION)\n",
    "                for i in range(cfg.NUM_DISTRIBUTION):\n",
    "                    optimizer_g[i].zero_grad()\n",
    "\n",
    "                    # Get the gradient of g(y):=g(miu_i_data)\n",
    "                    tmp_miu_i = miu_i[:, :, i]\n",
    "                    g_of_y = convex_g[i](tmp_miu_i).sum()\n",
    "                    grad_g_of_y = torch.autograd.grad(\n",
    "                        g_of_y, tmp_miu_i, create_graph=True)[0]\n",
    "\n",
    "                    # For each distribution you need to calculate a f(gradient of y)\n",
    "                    # it's the mean of the batch\n",
    "                    f_grad_g_y = convex_f[i](grad_g_of_y).mean()\n",
    "                    # The 1st loss part useful for f/g parameters\n",
    "                    loss_g[i] = f_grad_g_y - torch.dot(\n",
    "                        grad_g_of_y.reshape(-1), miu_i[:, :, i].reshape(-1)) / cfg.BATCH_SIZE\n",
    "                    g_OT_loss_value_batch[i] += loss_g[i].item()\n",
    "\n",
    "                total_loss_g = loss_g.sum()\n",
    "                total_loss_g.backward()\n",
    "                # The 2nd loss part useful for g parameters:\n",
    "                g_positive_constraints_loss = cfg.LAMBDA_CVX * \\\n",
    "                    compute_constraint_loss(\n",
    "                        g_positive_params)\n",
    "                g_constraints_loss_value_batch += g_positive_constraints_loss.item()\n",
    "                g_positive_constraints_loss.backward()\n",
    "\n",
    "                # ! update g\n",
    "                for i in range(cfg.NUM_DISTRIBUTION):\n",
    "                    optimizer_g[i].step()\n",
    "\n",
    "                # Just for the last iteration keep the gradient on f intact\n",
    "                if inner_iter != cfg.N_Gnet_ITERS:\n",
    "                    for i in range(cfg.NUM_DISTRIBUTION):\n",
    "                        optimizer_f[i].zero_grad()\n",
    "\n",
    "            ######################################################\n",
    "            #                Inner Loop Ends                     #\n",
    "            ######################################################\n",
    "            miu = generator_h(epsilon)\n",
    "\n",
    "            # miu_mean = miu.mean(dim=0).cuda(PTU.device)\n",
    "            miu_mean = miu.mean(dim = 0).cpu()\n",
    "\n",
    "            # miu_var = np.cov(miu.cuda(PTU.device).detach().numpy().T)\n",
    "            miu_var = np.cov(miu.cpu().detach().numpy().T)\n",
    "            \n",
    "            miu_mean_value_batch += miu_mean\n",
    "            miu_var_value_batch += miu_var\n",
    "\n",
    "            remaining_f_loss = torch.ones(cfg.NUM_DISTRIBUTION)\n",
    "            # The 3rd loss part useful for f/h parameters\n",
    "            for i in range(cfg.NUM_DISTRIBUTION):\n",
    "                remaining_f_loss[i] = - convex_f[i](miu).mean()\n",
    "                remaining_f_loss_value_batch[i] += remaining_f_loss[i].item()\n",
    "            total_remaining_f_loss = remaining_f_loss.sum()\n",
    "            total_remaining_f_loss.backward(retain_graph=True)\n",
    "\n",
    "            # Flip the gradient sign for parameters in convex f\n",
    "            # Because we need to solve \"sup\" of the loss for f\n",
    "            for p in list(convex_f.parameters()):\n",
    "                p.grad.copy_(-p.grad)\n",
    "            # ! update f\n",
    "            for i in range(cfg.NUM_DISTRIBUTION):\n",
    "                optimizer_f[i].step()\n",
    "\n",
    "            # Clamp the positive constraints on the convex_f_params\n",
    "            for p in f_positive_params:\n",
    "                p.data.copy_(torch.relu(p.data))\n",
    "\n",
    "            if medium_iter != cfg.N_Fnet_ITERS:\n",
    "                optimizer_h.zero_grad()\n",
    "\n",
    "        ######################################################\n",
    "        #               Medium Loop Ends                     #\n",
    "        ######################################################\n",
    "        # The 4th loss part useful for h parameters:\n",
    "        # ? keep untouched for all weights\n",
    "        # mu_2moment_loss_value_batch is total 4 distributions combined F\n",
    "        mu_2moment_loss = 0.5 * \\\n",
    "            miu.pow(2).sum(dim=1).mean() * cfg.NUM_DISTRIBUTION\n",
    "        mu_2moment_loss_value_batch += mu_2moment_loss.item() / cfg.NUM_DISTRIBUTION\n",
    "\n",
    "        # ! update h\n",
    "        mu_2moment_loss.backward()\n",
    "        optimizer_h.step()\n",
    "        # The four parts loss gradients are accumulated\n",
    "\n",
    "        miu_mean_value_batch = miu_mean_value_batch / cfg.N_Fnet_ITERS\n",
    "        miu_var_value_batch = miu_var_value_batch / cfg.N_Fnet_ITERS\n",
    "\n",
    "        g_OT_loss_value_batch[:] = [\n",
    "            item / (cfg.N_Gnet_ITERS * cfg.N_Fnet_ITERS) for item in g_OT_loss_value_batch]\n",
    "        remaining_f_loss_value_batch[:] = [\n",
    "            item / cfg.N_Fnet_ITERS for item in remaining_f_loss_value_batch]\n",
    "        g_constraints_loss_value_batch /= (cfg.N_Gnet_ITERS *\n",
    "                                           cfg.N_Fnet_ITERS)\n",
    "\n",
    "        ##### Calculate W2 batch loss ###############\n",
    "        w2_loss_value_batch = (sum(g_OT_loss_value_batch) + sum(remaining_f_loss_value_batch)) / cfg.NUM_DISTRIBUTION + \\\n",
    "            mu_2moment_loss_value_batch + 0.5 * \\\n",
    "            miu_i.pow(2).sum(dim=1).mean().item()\n",
    "        w2_loss_value_batch *= 2\n",
    "        # miu_i.pow(2).sum(dim=1).mean().item() is already the mean of all distributions\n",
    "\n",
    "        ##### Calculate all epoch loss ###############\n",
    "        w2_loss_value_epoch += w2_loss_value_batch\n",
    "        miu_mean_value_epoch += miu_mean_value_batch\n",
    "        miu_var_value_epoch += miu_var_value_batch\n",
    "\n",
    "        g_OT_loss_value_epoch = [\n",
    "            a + b for a,\n",
    "            b in zip(\n",
    "                g_OT_loss_value_epoch,\n",
    "                g_OT_loss_value_batch)]\n",
    "        g_constraints_loss_value_epoch += g_constraints_loss_value_batch\n",
    "        remaining_f_loss_value_epoch = [\n",
    "            a + b for a,\n",
    "            b in zip(\n",
    "                remaining_f_loss_value_epoch,\n",
    "                remaining_f_loss_value_batch)]\n",
    "        mu_2moment_loss_value_epoch += mu_2moment_loss_value_batch\n",
    "\n",
    "        if batch_idx % cfg.log_interval == 0:\n",
    "            logging.info('Train_Epoch: {} [{}/{} ({:.0f}%)] avg_dstb_g_OT_loss: {:.4f} avg_dstb_remaining_f_loss: {:.4f} mu_2moment_loss: {:.4f} g_constraint_loss: {:.4f} W2_loss: {:.4f} '.format(\n",
    "                epoch,\n",
    "                batch_idx * len(real_data),\n",
    "                len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                sum(g_OT_loss_value_batch) / cfg.NUM_DISTRIBUTION,\n",
    "                sum(remaining_f_loss_value_batch) / cfg.NUM_DISTRIBUTION,\n",
    "                mu_2moment_loss_value_batch,\n",
    "                miu_mean_value_batch.mean().tolist(),\n",
    "                miu_var_value_batch.mean().tolist(),\n",
    "                g_constraints_loss_value_batch,\n",
    "                w2_loss_value_batch\n",
    "            ))\n",
    "\n",
    "    w2_loss_value_epoch /= len(train_loader)\n",
    "    g_OT_loss_value_epoch[:] = [\n",
    "        item / len(train_loader) for item in g_OT_loss_value_epoch]\n",
    "    g_constraints_loss_value_epoch /= len(train_loader)\n",
    "    remaining_f_loss_value_epoch[:] = [\n",
    "        item / len(train_loader) for item in remaining_f_loss_value_epoch]\n",
    "    mu_2moment_loss_value_epoch /= len(train_loader)\n",
    "    miu_mean_value_epoch /= len(train_loader)\n",
    "    miu_var_value_epoch /= len(train_loader)\n",
    "    if cfg.high_dim_flag:\n",
    "        results.add(epoch=epoch,\n",
    "                    w2_loss_train_samples=w2_loss_value_epoch,\n",
    "                    g_OT_train_loss=g_OT_loss_value_epoch,\n",
    "                    g_constraints_train_loss=g_constraints_loss_value_epoch,\n",
    "                    remaining_f_train_loss=remaining_f_loss_value_epoch,\n",
    "                    mu_2moment_train_loss=mu_2moment_loss_value_epoch\n",
    "                    )\n",
    "    else:\n",
    "        results.add(epoch=epoch,\n",
    "                    w2_loss_train_samples=w2_loss_value_epoch,\n",
    "                    g_OT_train_loss=g_OT_loss_value_epoch,\n",
    "                    g_constraints_train_loss=g_constraints_loss_value_epoch,\n",
    "                    remaining_f_train_loss=remaining_f_loss_value_epoch,\n",
    "                    mu_2moment_train_loss=mu_2moment_loss_value_epoch,\n",
    "                    miu_mean_train=miu_mean_value_epoch.tolist(),\n",
    "                    miu_var_train=miu_var_value_epoch.tolist()\n",
    "                    )\n",
    "    results.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_evaluate = 5000\n",
    "input_measure_samples_for_evaluation = {}\n",
    "for marg_id in range(num_measures):\n",
    "    df = pd.read_csv(f\"{csv_path}/input_measure_samples_{marg_id}.csv\", header=None)\n",
    "    sampled_df = df.sample(n=1000, random_state=42)\n",
    "    samples = sampled_df.to_numpy()\n",
    "    input_measure_samples_for_evaluation[marg_id] = samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/var/folders/ty/glwljgt93kv4cddc7b1kk26c0000gn/T/ipykernel_65541/1305659235.py:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if cfg.optimizer is 'Adam':\n",
      "Train_Epoch: 1 [0/60000 (0%)] avg_dstb_g_OT_loss: -2118.7013 avg_dstb_remaining_f_loss: -0.2066 mu_2moment_loss: 0.4509 g_constraint_loss: 0.1223 W2_loss: 0.0088 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 60\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124m                    Real Training Process\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, cfg\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mschedule_learning_rate:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m cfg\u001b[38;5;241m.\u001b[39mlr_schedule_per_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[5], line 84\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, csv_path)\u001b[0m\n\u001b[1;32m     81\u001b[0m     g_OT_loss_value_batch[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_g[i]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     83\u001b[0m total_loss_g \u001b[38;5;241m=\u001b[39m loss_g\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m---> 84\u001b[0m \u001b[43mtotal_loss_g\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# The 2nd loss part useful for g parameters:\u001b[39;00m\n\u001b[1;32m     86\u001b[0m g_positive_constraints_loss \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mLAMBDA_CVX \u001b[38;5;241m*\u001b[39m \\\n\u001b[1;32m     87\u001b[0m     compute_constraint_loss(\n\u001b[1;32m     88\u001b[0m         g_positive_params)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/WB/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/WB/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    cfg = Cfg_class(DIM = dim, NUM_DISTRIBUTION=num_measures)\n",
    "\n",
    "    # gpus_choice = GPUtil.getFirstAvailable(\n",
    "    #     order='random', maxLoad=0.5, maxMemory=0.5, attempts=5, interval=900, verbose=False)\n",
    "    # PTU.set_gpu_mode(True, gpus_choice[0])\n",
    "    PTU.set_gpu_mode(False, 0)\n",
    "\n",
    "    cfg.INPUT_DIM = dim\n",
    "    cfg.OUTPUT_DIM = cfg.INPUT_DIM\n",
    "    cfg.NUM_DISTRIBUTION = num_measures\n",
    "    cfg.high_dim_flag = False\n",
    "    cfg.epochs = 500\n",
    "    results_save_path, model_save_path, results, testresults = LLU.init_path(cfg)\n",
    "    # kwargs = {'num_workers': 4, 'pin_memory': True}\n",
    "    kwargs = {'pin_memory': True}\n",
    "\n",
    "    \n",
    "    convex_f, convex_g, generator_h = g_NN.generate_FixedWeight_NN(cfg)\n",
    "\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            Initialization with some positive parameters\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    f_positive_params = []\n",
    "    g_positive_params = []\n",
    "\n",
    "    for i in range(cfg.NUM_DISTRIBUTION):\n",
    "        for p in list(convex_f[i].parameters()):\n",
    "            if hasattr(p, 'be_positive'):\n",
    "                f_positive_params.append(p)\n",
    "\n",
    "        for p in list(convex_g[i].parameters()):\n",
    "            if hasattr(p, 'be_positive'):\n",
    "                g_positive_params.append(p)\n",
    "\n",
    "        # convex_f[i].cuda(PTU.device)\n",
    "        # convex_g[i].cuda(PTU.device)\n",
    "        convex_f[i].cpu()\n",
    "        convex_g[i].cpu()\n",
    "    # generator_h.cuda(PTU.device)\n",
    "    generator_h.cpu()\n",
    "\n",
    "    optimizer_f = []\n",
    "    optimizer_g = []\n",
    "    if cfg.optimizer is 'Adam':\n",
    "        for i in range(cfg.NUM_DISTRIBUTION):\n",
    "            optimizer_f.append(optim.Adam(convex_f[i].parameters(), lr=cfg.LR_f))\n",
    "            optimizer_g.append(\n",
    "                optim.Adam(convex_g[i].parameters(), lr=cfg.LR_g))\n",
    "        optimizer_h = optim.Adam(\n",
    "            generator_h.parameters(),\n",
    "            lr=cfg.LR_h)\n",
    "\n",
    "\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                        Real Training Process\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        train(epoch, csv_path)\n",
    "        if cfg.schedule_learning_rate:\n",
    "            if epoch % cfg.lr_schedule_per_epoch == 0:\n",
    "                for i in range(cfg.NUM_DISTRIBUTION):\n",
    "                    optimizer_f[i].param_groups[0]['lr'] *= cfg.lr_schedule_scale\n",
    "                    optimizer_g[i].param_groups[0]['lr'] *= cfg.lr_schedule_scale\n",
    "                optimizer_h.param_groups[0]['lr'] *= cfg.lr_schedule_scale\n",
    "\n",
    "        LLU.dump_nn(generator_h, convex_f, convex_g, epoch,\n",
    "                    model_save_path, num_distribution=cfg.NUM_DISTRIBUTION, save_f=cfg.save_f)\n",
    "            \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
